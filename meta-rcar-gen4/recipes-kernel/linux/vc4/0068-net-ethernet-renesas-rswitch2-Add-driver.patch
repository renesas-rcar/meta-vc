From fe09b6de19a922ce56c87688dca78e082f199e37 Mon Sep 17 00:00:00 2001
From: Dennis Ostermann <dennis.ostermann@renesass.com>
Date: Tue, 15 Feb 2022 09:30:19 +0100
Subject: [PATCH 68/85] net: ethernet: renesas: rswitch2: Add driver

---
 drivers/net/ethernet/renesas/Kconfig          |    2 +
 drivers/net/ethernet/renesas/Makefile         |    1 +
 drivers/net/ethernet/renesas/rswitch2/Kconfig |   29 +
 .../net/ethernet/renesas/rswitch2/Makefile    |   22 +
 .../net/ethernet/renesas/rswitch2/rswitch2.h  |  180 +
 .../ethernet/renesas/rswitch2/rswitch2_coma.h |  177 +
 .../ethernet/renesas/rswitch2/rswitch2_eth.c  | 3839 +++++++++++++++++
 .../ethernet/renesas/rswitch2/rswitch2_eth.h  |  491 +++
 .../ethernet/renesas/rswitch2/rswitch2_fwd.c  |  515 +++
 .../ethernet/renesas/rswitch2/rswitch2_fwd.h  |  602 +++
 .../ethernet/renesas/rswitch2/rswitch2_gwca.h |  513 +++
 .../ethernet/renesas/rswitch2/rswitch2_main.c |  143 +
 .../renesas/rswitch2/rswitch2_platf.c         |  700 +++
 .../renesas/rswitch2/rswitch2_platf.h         |   99 +
 .../ethernet/renesas/rswitch2/rswitch2_rmac.h |  441 ++
 .../renesas/rswitch2/rswitch2_serdes.h        |   64 +
 16 files changed, 7818 insertions(+)
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/Kconfig
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/Makefile
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2.h
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2_coma.h
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2_eth.c
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2_eth.h
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2_fwd.c
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2_fwd.h
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2_gwca.h
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2_main.c
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2_platf.c
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2_platf.h
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2_rmac.h
 create mode 100644 drivers/net/ethernet/renesas/rswitch2/rswitch2_serdes.h

diff --git a/drivers/net/ethernet/renesas/Kconfig b/drivers/net/ethernet/renesas/Kconfig
index 04acc0333c44..061714d96ccf 100644
--- a/drivers/net/ethernet/renesas/Kconfig
+++ b/drivers/net/ethernet/renesas/Kconfig
@@ -58,6 +58,8 @@ config RENESAS_ETHER_SWITCH
 	  This driver supports the following SoCs:
 		- R8A779Fx.
 
+source "drivers/net/ethernet/renesas/rswitch2/Kconfig"
+
 config RTSN
         tristate "Renesas Ethernet-TSN support"
         depends on ARCH_RENESAS || COMPILE_TEST
diff --git a/drivers/net/ethernet/renesas/Makefile b/drivers/net/ethernet/renesas/Makefile
index 1848c370a555..fa6345a5638b 100644
--- a/drivers/net/ethernet/renesas/Makefile
+++ b/drivers/net/ethernet/renesas/Makefile
@@ -12,3 +12,4 @@ obj-$(CONFIG_RAVB) += ravb.o
 obj-$(CONFIG_RTSN_PTP) += rtsn_ptp.o
 obj-$(CONFIG_RTSN) += rtsn.o
 obj-$(CONFIG_RENESAS_ETHER_SWITCH) += rswitch.o
+obj-$(CONFIG_RENESAS_RSWITCH2) += rswitch2/
diff --git a/drivers/net/ethernet/renesas/rswitch2/Kconfig b/drivers/net/ethernet/renesas/rswitch2/Kconfig
new file mode 100644
index 000000000000..89628eb40e18
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/Kconfig
@@ -0,0 +1,29 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for the Renesas RSwitch2 device drivers.
+#
+
+config RENESAS_RSWITCH2
+	tristate "Renesas RSwitch2 TSN switch"
+	default n
+	help
+		Device driver for Renesas RSwitch2 TSN switch.
+
+if RENESAS_RSWITCH2
+
+config RENESAS_RSWITCH2_PLATF
+	bool "Platform driver for Renesas RSwitch2 TSN switch"
+	default n
+	depends on RENESAS_RSWITCH2
+	help
+		Enable access to Renesas RSwitch2 device platform bus (SoC).
+		Select if RSwitch2 is part of your SoC.
+
+config RENESAS_RSWITCH2_GWCA
+	bool "RSwitch2 TSN ethernet device (GWCA)"
+	default n
+	help
+		Driver for RSwitch2 Gateway Common Agent. This enables an internal ethernet device,
+		directly connected to the switch.
+
+endif # RENESAS_RSWITCH2
diff --git a/drivers/net/ethernet/renesas/rswitch2/Makefile b/drivers/net/ethernet/renesas/rswitch2/Makefile
new file mode 100644
index 000000000000..71da102a3de8
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/Makefile
@@ -0,0 +1,22 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for the RSwitch2 ethernet switch device driver.
+#
+
+#
+#  Build Options
+#
+
+obj-$(CONFIG_RENESAS_RSWITCH2) += rswitch2.o
+
+ifneq ($(CONFIG_RENESAS_RSWITCH2_PLATF), )
+	rswitch2-y += rswitch2_platf.o
+endif
+
+rswitch2-y += rswitch2_main.o
+ifneq ($(CONFIG_RENESAS_RSWITCH2_GWCA), )
+	rswitch2-y += rswitch2_eth.o
+endif
+
+rswitch2-y += rswitch2_fwd.o
+rswitch2-y += ../rtsn_ptp.o
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2.h b/drivers/net/ethernet/renesas/rswitch2/rswitch2.h
new file mode 100644
index 000000000000..1e57a0a5c3e8
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2.h
@@ -0,0 +1,180 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Renesas RSwitch2 common header
+ *
+ * Copyright (C) 2021, 2022 Renesas Electronics Corporation
+ */
+
+#ifndef _RSWITCH2_RSWITCH2_H
+#define _RSWITCH2_RSWITCH2_H
+
+#include <linux/if_ether.h>
+#include <linux/phy.h>
+#include <linux/reset.h>
+
+
+
+#define RSWITCH2_NAME 	"rswitch2"
+#define RSW2_GWCA0_NAME	"gwca0"
+
+
+#define RSWITCH2_AXI_CHAIN_N		128
+#define RSWITCH2_BITS_PER_REG		32
+#define RSWITCH2_CHAIN_REG_NUM		(RSWITCH2_AXI_CHAIN_N / RSWITCH2_BITS_PER_REG)
+
+#define RSWITCH2_PTP_Q_NUM			2
+#define RSWITCH2_MAC_AGING_TIME		300
+
+#define RSWITCH2_MAX_RXTX_IRQS		32
+#define RSWITCH2_MAX_STATUS_IRQS	8
+
+#define RSWITCH2_MAX_IRQS			(RSWITCH2_MAX_RXTX_IRQS + RSWITCH2_MAX_STATUS_IRQS)
+
+
+#define RSWITCH2_MAX_VLAN_ENTRIES	8192
+
+/* Wait max. 100ms as default */
+#define RSWITCH2_REG_POLL_TIMEOUT	100000
+
+/* Poll every 100us */
+#define RSWITCH2_REG_POLL_DELAY		100
+
+
+static const u8 rsw2_base_mac[ETH_ALEN] __aligned(2) = {
+		0x74, 0x90, 0x50, 0x00, 0x00, 0x00 };
+
+#define RSWITCH2_BASE_ADDR_MAC { 0x74, 0x90, 0x50, 0x00, 0x00, 0x00 }
+
+static const u8 rsw2_ptp_multicast_mac[ETH_ALEN] __aligned(2) = {
+		0x01, 0x80, 0xc2, 0x00, 0x00, 0x0E };
+
+static const u8 rsw2_own_multicast_mac[ETH_ALEN] __aligned(2) = {
+		0x74, 0x90, 0x50, 0x00, 0xcc, 0x00};
+
+
+enum rsw2_msg_section {
+	MSG_GEN = 0,
+	MSG_DESC,			/* Descriptors, ring layout */
+	MSG_RXTX,			/* Reception/Transmission */
+	MSG_FWD,			/* Forwarding engine */
+	MSG_SERDES,			/* SerDes */
+	MSG_LAST_ENTRY
+};
+
+
+
+
+
+struct rswitch2_port_data {
+	char phy_id[MII_BUS_ID_SIZE + 3];
+	phy_interface_t phy_iface;
+	u32 reg;
+	u8 mac_addr[ETH_ALEN] __aligned(2);
+};
+
+struct rsw2_q_port_backref {
+	uint port_num;
+	uint port_q;
+};
+
+struct rswitch2_drv {
+	void __iomem *base_addr;
+	void __iomem *fwd_base_addr;
+	void __iomem *fab_base_addr;
+	void __iomem *coma_base_addr;
+	void __iomem **etha_base_addrs;
+	void __iomem **gwca_base_addrs;
+	void __iomem *serdes_base_addr;
+	void __iomem *ptp_base_addr;
+	void __iomem *sram_base_addr;  /* for sram debug and pps output control */
+	struct device *dev;
+	struct reset_control *sd_rst;
+	u32 num_of_cpu_ports;
+	u32 num_of_tsn_ports;
+	bool serdes_common_init_done;
+	struct rswitch2_eth_port **ports;
+
+	/* Move these to eth generic struct */
+
+	struct rswitch2_dma_desc **bat_addr;
+	dma_addr_t bat_dma_addr;
+
+	struct rswitch2_dma_desc *bat_ts_addr;
+	dma_addr_t bat_ts_dma_addr;
+	struct rswitch2_dma_ts_desc *ts_desc_ring;
+	dma_addr_t ts_desc_dma;
+	u32 ts_cur_desc;	/* Consumer ring indices */
+	u32 ts_dirty_desc;	/* Producer ring indices */
+
+#ifdef RSW2_DEPRECATED
+	struct rswitch2_port_data *port_data;
+#endif /* RSW2_DEPRECATED */
+	int rxtx_irqs[RSWITCH2_MAX_RXTX_IRQS];
+	unsigned int num_of_rxtx_irqs;
+	int status_irqs[RSWITCH2_MAX_STATUS_IRQS];
+	unsigned int num_of_status_irqs;
+	struct rtsn_ptp_private *ptp_drv;
+	struct rsw2_q_port_backref port_backref[RSWITCH2_AXI_CHAIN_N];
+	int msg_enable;
+	int sec_log_lvl[MSG_LAST_ENTRY];
+	spinlock_t lock;
+
+};
+
+#define __THIS_FILE__   ((strrchr(__FILE__, '/') == NULL) ?  ( __FILE__ ) :  (strrchr(__FILE__, '/') + 1))
+
+static inline const char *_rsw2_get_msg_section(enum rsw2_msg_section msg_sec) {
+	const char *section_str;
+
+		switch(msg_sec) {
+		case MSG_GEN:
+			section_str = "";
+			break;
+
+		case MSG_DESC:
+			section_str = "[DESC]";
+			break;
+
+		case MSG_RXTX:
+			section_str = "[RX/TX]";
+			break;
+
+		case MSG_FWD:
+			section_str = "[FWD]";
+			break;
+
+		case MSG_SERDES:
+			section_str = "[SerDes]";
+			break;
+
+
+		default:
+			pr_warn("RSwitch2[%s:%d]: Illegal message section.\n", __THIS_FILE__,  __LINE__);
+			section_str = "";
+		}
+		return section_str;
+}
+
+#define rsw2_dbg(MSG_SEC, ARG, ...)  _rsw2_pr(rsw2, LOGLEVEL_DEBUG, MSG_SEC, "RSwitch2[%s:%d]%s: " ARG , (__THIS_FILE__), (__LINE__), _rsw2_get_msg_section(MSG_SEC),  ##__VA_ARGS__)
+#define rsw2_info(MSG_SEC, ARG, ...)  _rsw2_pr(rsw2, LOGLEVEL_INFO, MSG_SEC, "RSwitch2[%s:%d]%s: " ARG , (__THIS_FILE__), (__LINE__), _rsw2_get_msg_section(MSG_SEC),  ##__VA_ARGS__)
+#define rsw2_notice(MSG_SEC, ARG, ...)  _rsw2_pr(rsw2, LOGLEVEL_NOTICE, MSG_SEC, "RSwitch2[%s:%d]%s: " ARG , (__THIS_FILE__), (__LINE__), _rsw2_get_msg_section(MSG_SEC),  ##__VA_ARGS__)
+#define rsw2_warn(MSG_SEC, ARG, ...)  _rsw2_pr(rsw2, LOGLEVEL_WARNING, MSG_SEC, "RSwitch2[%s:%d]%s: " ARG , (__THIS_FILE__), (__LINE__), _rsw2_get_msg_section(MSG_SEC),  ##__VA_ARGS__)
+#define rsw2_err(MSG_SEC, ARG, ...)  _rsw2_pr(rsw2, LOGLEVEL_ERR, MSG_SEC, "RSwitch2[%s:%d]%s: " ARG , (__THIS_FILE__), (__LINE__), _rsw2_get_msg_section(MSG_SEC),  ##__VA_ARGS__)
+#define rsw2_crit(MSG_SEC, ARG, ...)  _rsw2_pr(rsw2, LOGLEVEL_CRIT, MSG_SEC, "RSwitch2[%s:%d]%s: " ARG , (__THIS_FILE__), (__LINE__), _rsw2_get_msg_section(MSG_SEC),  ##__VA_ARGS__)
+
+static inline int _rsw2_pr(struct rswitch2_drv *rsw2,  int level, enum rsw2_msg_section msg_sec, const char *fmt, ...) {
+	int ret;
+	va_list args;
+
+	if(level <= rsw2->sec_log_lvl[msg_sec]) {
+		va_start(args, fmt);
+		ret = vprintk(fmt, args);
+		va_end(args);
+	}
+	return ret;
+}
+
+
+int rswitch2_init(struct rswitch2_drv *rsw2);
+void rswitch2_exit(struct rswitch2_drv *rsw2);
+
+#endif /* _RSWITCH2_RSWITCH2_H */
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2_coma.h b/drivers/net/ethernet/renesas/rswitch2/rswitch2_coma.h
new file mode 100644
index 000000000000..cc0e62dfc085
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2_coma.h
@@ -0,0 +1,177 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Renesas RSwitch2 Common Agent device driver
+ *
+ * Copyright (C) 2021 Renesas Electronics Corporation
+ *
+ */
+#ifndef _RSWITCH2_COMA_H
+#define _RSWITCH2_COMA_H
+
+#include <linux/bits.h>
+
+#define RSW2_COMA_RIPV				0x0000 /* R-Switch IP Version */
+#define RIPV_CAIPV					GENMASK(23, 21)
+#define RIPV_FBIPV					GENMASK(19, 16)
+#define RIPV_EAIPV					GENMASK(15, 12)
+#define RIPV_FWIPV					GENMASK(11, 8)
+#define RIPV_GWIPV					GENMASK(7, 4)
+#define RIPV_TIPV					GENMASK(3, 0)
+
+#define RSW2_COMA_RRC				0x0004 /* R-Switch Reset Configuration */
+#define RRC_RR						BIT_MASK(0)
+
+#define RSW2_COMA_RCEC				0x0008 /* R-Switch Clock Enable Configuration */
+#define RCEC_RCE					BIT_MASK(16)
+#define RCEC_ACE					GENMASK(6, 0)
+
+#define RSW2_COMA_RCDC				0x000C /* R-Switch Clock Disable Configuration */
+#define RCDC_RCD					BIT_MASK(16)
+#define RCDC_ACD					GENMASK(6, 0)
+
+#define RSW2_COMA_RSSIS				0x0010 /* R-Switch Software Synchronization Interrupt Status */
+#define RSSIS_NSSSIS(i)				BIT_MASK(((i) & 0xFF) << 8)
+#define RSSIS_SNSSIS(i)				BIT_MASK((i) & 0xFF)
+
+#define RSW2_COMA_RSSIE				0x0014 /* R-Switch Software Synchronization Interrupt Enable */
+#define RSSIE_NSSSIE(i)				BIT_MASK(((i) & 0xFF) << 8)
+#define RSSIE_SNSSIE(i)				BIT_MASK((i) & 0xFF)
+
+#define RSW2_COMA_RSSID				0x0018 /* R-Switch Software Synchronization Interrupt Disable */
+#define RSSID_NSSSID(i)				BIT_MASK(((i) & 0xFF) << 8)
+#define RSSID_SNSSID(i)				BIT_MASK((i) & 0xFF)
+
+/* Common Agent Buffer Pool IPV Based Watermark Config (i = 0..7) */
+#define RSW2_COMA_CABPIBWMC(i)		(0x0020 + (0x4 * (i)))
+#define CABPIBWMC_IBSWMPN			GENMASK(28, 16)
+
+
+#define RSW2_COMA_CABPWMLC			0x0040 /* Common Agent Buffer Pool Watermark Level Configuration */
+#define CABPWMLC_WMCL				GENMASK(28, 16)
+#define CABPWMLC_WMFL				GENMASK(12, 0)
+
+/* Common Agent Buffer Pointer Pause Frame Level Configuration i (i = 0..PAS_LVL_N -1) */
+#define RSW2_COMA_CABPPFLC(i)		(0x0050 + (0x4 * (i)))
+#define CABPPFLC_PAL				GENMASK(28, 16)
+#define CABPPFLC_PDL				GENMASK(12, 0)
+
+/* Common Agent Buffer Pool Watermark Level Configuration i (i = 0..PORT_N-1) */
+#define RSW2_COMA_CABPPWMLC(i)		(0x0060 + (0x4 * (i)))
+#define CABPPWMLC_PWMCL				GENMASK(28, 16)
+#define CABPPWMLC_PWMFK				GENMASK(12, 0)
+
+/* Common Agent Buffer Pointer per Port Pause Frame Level Configuration */
+#define RSW2_COMA_CABPPPFLC(i, j)	(0x00A0 + (0x4 * ((2 * i) + j)))
+#define CABPPPFLC_PPAL				GENMASK(28, 16)
+#define CABPPPFLC_PPDL				GENMASK(12, 0)
+
+/* Common Agent Buffer Pointer Utilization Level Configuration i (i = 0..PORT_N-1) */
+#define RSW2_COMA_CABPULC(i)		(0x0100 + (0x4 * (i)))
+#define CABPULC_MNNPN				GENMASK(28, 16)
+#define CABPULC_MXNPN				GENMASK(12, 0)
+
+#define RSW2_COMA_CABPIRM			0x0140 /* Common Agent Buffer Pool Initialization Register Monitoring */
+#define CABPIRM_BPR					BIT_MASK(1)
+#define CABPIRM_BPIOG				BIT_MASK(0)
+
+#define RSW2_COMA_CABPPCM			0x0144 /* Common Agent Buffer Pool Pointer Count Monitoring */
+#define CABPPCM_TPC					GENMASK(28, 16)
+#define CABPPCM_RPC					GENMASK(12, 0)
+
+#define RSW2_COMA_CABPLCM			0x0148 /* Common Agent Buffer Pool Pointer Least Count Monitoring */
+#define CABPLCM_LRC					GENMASK(12, 0)
+
+#define RSW2_COMA_CABPLCME			0x014C /* Common Agent Buffer Pool Pointer Least Count Monitoring Emu */
+#define CABPLCME_RPCP				GENMASK(12, 0)
+
+ /* Common Agent Buffer Pointer Count per Port Monitoring i (i = 0..PORT_N-1) */
+#define RSW2_COMA_CABPCPM(i)		(0x0180 + (0x4 * (i)))
+#define CABPCPM_RPMP				GENMASK(12, 0)
+
+/* Common Agent Buffer Pointer Maximum Count per Port Monitoring I (i = 0..PORT_N-1) */
+#define RSW2_COMA_CABPMCPM(i)		(0x0200 + (0x4 * (i)))
+/* Common Agent Buffer Pointer Maximum Count per Port Monitoring I Emu (i = 0..PORT_N-1) */
+#define RSW2_COMA_CABPMCPME(i)		(0x0280 + (0x4 * (i)))
+#define CABPMCPM_RPMCP				GENMASK(12, 0)
+
+#define RSW2_COMA_CARDNM			0x0300 /* Common Agent Rejected Descriptor Number Monitoring */
+#define CARDNM_RDNRR				GENMASK(12, 0)
+
+/* Common Agent Rejected Descriptor Maximum Number Monitoring */
+#define RSW2_COMA_CARDMNM			0x0304
+/* Common Agent Rejected Descriptor Maximum Number Monitoring Emu */
+#define RSW2_COMA_CARDMNME			0x0308
+#define CARDMNM_RDNRR				GENMASK(12, 0)
+
+#define RSW2_COMA_CARDCN			0x0310 /* Common Agent Rejected Descriptor Counter */
+#define RSW2_COMA_CARDCNE			0x0314 /* Common Agent Rejected Descriptor Counter Emu */
+#define CARDCN_RDN					GENMASK(31, 0)
+
+#define RSW2_COMA_CAEIS0			0x0400 /* Common Agent Error Interrupt Status 0 */
+#define CAEIS0_EEIPLN				GENMASK(19, 16)
+#define CAEIS0_WMFLOS				BIT_MASK(10)
+#define CAEIS0_WMCLOS				BIT_MASK(9)
+#define CAEIS0_BPOPS				BIT_MASK(8)
+#define CAEIS0_BPECCES				BIT_MASK(2)
+#define CAEIS0_DSECCES				BIT_MASK(1)
+#define CAEIS0_PECCES				BIT_MASK(0)
+
+#define RSW2_COMA_CAEIE0			0x0404 /* Common Agent Error Interrupt Enable 0 */
+#define CAEIE0_WMFLOE				BIT_MASK(10)
+#define CAEIE0_WMCLOE				BIT_MASK(9)
+#define CAEIE0_BPOPE				BIT_MASK(8)
+#define CAEIE0_BPECCEE				BIT_MASK(2)
+#define CAEIE0_DSECCEE				BIT_MASK(1)
+#define CAEIE0_PECCEE				BIT_MASK(0)
+
+#define RSW2_COMA_CAEID0			0x0408 /* Common Agent Error Interrupt Disable 0 */
+#define CAEID0_WMFLOD				BIT_MASK(10)
+#define CAEID0_WMCLOD				BIT_MASK(9)
+#define CAEID0_BPOPD				BIT_MASK(8)
+#define CAEID0_BPECCED				BIT_MASK(2)
+#define CAEID0_DSECCED				BIT_MASK(1)
+#define CAEID0_PECCED				BIT_MASK(0)
+
+#define RSW2_COMA_CAEIS1			0x0410 /* Common Agent Error Interrupt Status 1 */
+#define CAEIS1_PWMFLOS				GENMASK(22, 16)
+#define CAEIS1_PWMCLOS				GENMASK(6, 0)
+
+#define RSW2_COMA_CAEIE1			0x0414 /* Common Agent Error Interrupt Enable 1 */
+#define CAEIE1_PWMFLOE				GENMASK(22, 16)
+#define CAEIE1_PWMCLOE				GENMASK(6, 0)
+
+#define RSW2_COMA_CAEID1			0x0418 /* Common Agent Error Interrupt Disable 1 */
+#define CAEID1_PWMFLOD				GENMASK(22, 16)
+#define CAEID1_PWMCLOD				GENMASK(6, 0)
+
+#define RSW2_COMA_CAMIS0			0x0440 /* Common Agent Monitoring Interrupt Status 0 */
+#define CAMIS0_PFS					GENMASK(1, 0)
+
+#define RSW2_COMA_CAMIE0			0x0444 /* Common Agent Monitoring Interrupt Enable 0 */
+#define CAMIE0_PFE					GENMASK(1, 0)
+
+#define RSW2_COMA_CAMID0			0x0448 /* Common Agent Monitoring Interrupt Disable 0 */
+#define CAMID0_PFD					GENMASK(1, 0)
+
+#define RSW2_COMA_CAMIS1			0x0450 /* Common Agent Monitoring Interrupt Status 1 */
+#define CAMIS1_PPFS					GENMASK(13, 0)
+
+#define RSW2_COMA_CAMIE1			0x0454 /* Common Agent Monitoring Interrupt Enable 1 */
+#define CAMIE1_PPFE					GENMASK(13, 0)
+
+#define RSW2_COMA_CAMID1			0x0458 /* Common Agent Monitoring Interrupt Disable 1 */
+#define CAMID1_PPFD					GENMASK(13, 0)
+
+#define RSW2_COMA_CASCR				0x0480 /* Common Agent Security configuration register. */
+#define CASCR_MIRSL					BIT_MASK(5)
+#define CASCR_EIRSL					BIT_MASK(4)
+#define CASCR_CRSL					BIT_MASK(3)
+#define CASCR_RRRSL					BIT_MASK(2)
+#define CASCR_BPRSL					BIT_MASK(1)
+#define CASCR_VRSL					BIT_MASK(0)
+
+#define RSW2_RSW0PPS0R0			0x0604 /* Common PPS output select */
+#define RSW2_RSW0PPS0R1			0x0608 /* Common PPS output select */
+#define RSW2_RSW0PPS1R0			0x060c /* Common PPS output select */
+#define RSW2_RSW0PPS1R1			0x0610 /* Common PPS output select */
+
+#endif /* _RSWITCH2_COMA_H */
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2_eth.c b/drivers/net/ethernet/renesas/rswitch2/rswitch2_eth.c
new file mode 100644
index 000000000000..abab9fdb0a2f
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2_eth.c
@@ -0,0 +1,3839 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Renesas RSwitch2 Ethernet device driver
+ *
+ * Copyright (C) 2019-2021 Renesas Electronics Corporation
+ *
+ */
+//#define DEBUG_SERDES
+
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/dma-mapping.h>
+#include <linux/err.h>
+#include <linux/errno.h>
+#include <linux/etherdevice.h>
+#include <linux/netdevice.h>
+#include <linux/ethtool.h>
+#include <linux/phy.h>
+#include <linux/if_vlan.h>
+#include <linux/kernel.h>
+#include <linux/irq.h>
+#include <linux/list.h>
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
+#include <linux/net_tstamp.h>
+
+#include <linux/of_platform.h>
+
+#include "rswitch2.h"
+#include "rswitch2_eth.h"
+#include "rswitch2_gwca.h"
+#include "rswitch2_fwd.h"
+
+#include "rswitch2_rmac.h"
+#include "rswitch2_serdes.h"
+#include "../rtsn_ptp.h"
+
+static int rswitch2_gwca_set_state(struct rswitch2_drv *rsw2, enum gwmc_op state)
+{
+	int ret;
+	u32 reg_val;
+
+	reg_val = FIELD_PREP(GWMC_OPC, state);
+	iowrite32(reg_val, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWMC);
+
+	ret = readl_poll_timeout(rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWMS, reg_val,
+						reg_val == FIELD_PREP(GWMS_OPS, state),
+						RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		rsw2_err(MSG_GEN, "Setting GWCA state timed out\n");
+		return ret;
+	}
+	return 0;
+}
+
+static int rswitch2_emac_set_state(struct net_device *ndev, enum emac_op state)
+{
+	int ret;
+	u32 reg_val;
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	void __iomem *etha_base_addr;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+	etha_base_addr = (void __iomem *)ndev->base_addr;
+
+	reg_val = FIELD_PREP(EAMC_OPC, state);
+	iowrite32(reg_val, etha_base_addr + RSW2_ETHA_EAMC);
+
+	ret = readl_poll_timeout_atomic(etha_base_addr + RSW2_ETHA_EAMS, reg_val,
+						(reg_val == FIELD_PREP(EAMS_OPS, state)),
+						RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		rsw2_err(MSG_GEN, "Setting EMAC state from %d to %d timed out\n", reg_val, state);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int rswitch2_gwca_init(struct rswitch2_drv *rsw2)
+{
+	u32 reg_val;
+	int ret;
+
+	/* TODO: Only GWCA0 supported in FPGA implementation */
+
+	ret = rswitch2_gwca_set_state(rsw2, gwmc_disable);
+	if (ret != 0) {
+		rsw2_err(MSG_GEN, "Failed to set GWCA disable state\n");
+		goto err_out;
+	}
+
+
+	ret = rswitch2_gwca_set_state(rsw2, gwmc_reset);
+		if (ret != 0) {
+			rsw2_err(MSG_GEN, "Failed to set GWCA reset state\n");
+			goto err_out;
+		}
+
+
+	ret = rswitch2_gwca_set_state(rsw2, gwmc_disable);
+		if (ret != 0) {
+			rsw2_err(MSG_GEN, "Failed to set GWCA disable state\n");
+			goto err_out;
+		}
+
+
+	ret = rswitch2_gwca_set_state(rsw2, gwmc_config);
+	if (ret != 0) {
+		rsw2_err(MSG_GEN, "Failed to set GWCA config state\n");
+		goto err_out;
+	}
+
+	/* Reset multicast table */
+	iowrite32(GWMTIRM_MTIOG, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWMTIRM);
+
+	/* Wait for operation to complete */
+	ret = readl_poll_timeout(rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWMTIRM, reg_val,
+							(reg_val == GWMTIRM_MTR),
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		rsw2_err(MSG_GEN, "Failed to reset multicast table\n");
+		goto err_out;
+	}
+
+	/* Reset AXI RAM */
+	iowrite32(GWARIRM_ARIOG, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWARIRM);
+
+	/* Wait for operation to complete */
+	ret = readl_poll_timeout(rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWARIRM, reg_val,
+							(reg_val == GWARIRM_ARR),
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		rsw2_err(MSG_GEN, "Failed to reset AXI RAM table\n");
+		goto err_out;
+	}
+
+	/* Set VLAN egress mode for GWCA to SCTAG*/
+	iowrite32(FIELD_PREP(GWVCC_VEM, sc_ctag), rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWVCC);
+
+	reg_val = FIELD_PREP(GWVTC_CTV, RSWITCH2_DEF_CTAG_VLAN_ID);
+	reg_val |= FIELD_PREP(GWVTC_CTP, RSWITCH2_DEF_CTAG_PCP);
+	reg_val |= FIELD_PREP(GWVTC_CTD, RSWITCH2_DEF_CTAG_DEI);
+	reg_val |= FIELD_PREP(GWVTC_STV, RSWITCH2_DEF_STAG_VLAN_ID);
+	reg_val |= FIELD_PREP(GWVTC_STP, RSWITCH2_DEF_STAG_PCP);
+	reg_val |= FIELD_PREP(GWVTC_STD, RSWITCH2_DEF_STAG_DEI);
+	iowrite32(reg_val, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWVTC);
+
+	/* Allow split frames with up to 15 descriptors */
+	reg_val = ioread32(rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWMDNC);
+	reg_val |= FIELD_PREP(GWMDNC_TXDMN, TX_MAX_DESC_PER_FRAME - 1);
+	reg_val |= FIELD_PREP(GWMDNC_RXDMN, RX_MAX_DESC_PER_FRAME - 1);
+	iowrite32(reg_val, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWMDNC);
+
+	return 0;
+
+err_out:
+	return ret;
+}
+
+
+/* SerDes functionality */
+void rswitch2_serdes_write32(void __iomem *addr, u32 offs,  u32 bank, u32 data)
+{
+	iowrite32(bank, addr + RSWITCH_SERDES_BANK_SELECT);
+	iowrite32(data, addr + offs);
+#ifdef DEBUG_SERDES
+	printk("SERDES WR %d: %03x - %04x  <= %04x\n", (int)((u64)addr&0xc00)/0x400, bank, offs, data);
+#endif
+}
+
+u32 rswitch2_serdes_read32(void __iomem *addr, u32 offs,  u32 bank)
+{
+	u32 ret;
+
+	iowrite32(bank, addr + RSWITCH_SERDES_BANK_SELECT);
+	ret = ioread32(addr + offs);
+
+#ifdef DEBUG_SERDES
+	printk("SERDES RD %d: %03x - %04x  is %04x\n", (int)((u64)addr&0xc00)/0x400, bank, offs, ret);
+#endif
+	return ret;
+}
+
+static int rswitch2_serdes_reg_wait(void __iomem *addr, u32 offs, u32 bank, u32 mask, u32 expected)
+{
+	u32 ret;
+	int i;
+
+	iowrite32(bank, addr + RSWITCH_SERDES_BANK_SELECT);
+	//udelay(100);
+
+	for (i = 0; i < 1000 /*RSWITCH2_REG_POLL_TIMEOUT*/; i++) {
+		ret = ioread32(addr + offs);
+		if ((ret & mask) == expected) {
+#ifdef DEBUG_SERDES
+			printk("SERDES po %d: %03x - %04x  is %04x after %d iterations\n", (int)((u64)addr&0xc00)/0x400, bank, offs, ret, i);
+#endif
+			return 0;
+		}
+		mdelay(1);
+	}
+#ifdef DEBUG_SERDES
+	printk("(SERDES po %d: %03x - %04x  is %04x but %04x==%04x expected\n", (int)((u64)addr&0xc00)/0x400, bank, offs, ret, (ret&mask), expected);
+#endif
+	return -ETIMEDOUT;
+}
+
+static int rswitch2_serdes_set_speed(struct rswitch2_eth_port *eth_port, int phy_speed)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_physical_port *phy_port;
+	void __iomem *addr = eth_port->phy_port->serdes_chan_addr;
+	int ret;
+
+	phy_port = eth_port->phy_port;
+	rsw2 = eth_port->rsw2;
+
+	rsw2_notice(MSG_SERDES, "Set serdes speed: %d\n", phy_speed);
+
+	switch (phy_port->phy_iface) {
+	case PHY_INTERFACE_MODE_SGMII:
+		if (phy_speed == SPEED_1000)
+			rswitch2_serdes_write32(addr, SR_MII_CTRL, BANK_1F00, 0x0140);
+		else if (phy_speed == SPEED_100)
+			rswitch2_serdes_write32(addr, SR_MII_CTRL, BANK_1F00, 0x2100);
+		else if (phy_speed == SPEED_10)
+			rswitch2_serdes_write32(addr, SR_MII_CTRL, BANK_1F00, 0x0100);
+		else
+			return -EOPNOTSUPP;
+#if 0
+		//Autoneg for 1G
+		ret = rswitch2_serdes_reg_wait(addr, 0x008, BANK_1F80, BIT(0), 1);
+		if (ret)
+			return ret;
+		rswitch2_serdes_write32(addr, 0x008, BANK_1F80, 0);
+#endif
+		break;
+
+	case PHY_INTERFACE_MODE_USXGMII:
+		if (phy_speed == SPEED_2500)
+			rswitch2_serdes_write32(addr, SR_MII_CTRL, BANK_1F00, 0x120);
+		else
+			return -EOPNOTSUPP;
+		udelay(50);
+		rswitch2_serdes_write32(addr, VR_XS_PCS_DIG_CTRL1, BANK_380, 0x2600);
+		ret = rswitch2_serdes_reg_wait(addr, VR_XS_PCS_DIG_CTRL1, BANK_380, BIT(10), 0);
+		if (ret) {
+			rsw2_err(MSG_SERDES, "Speed update failed  err=%d\n", ret);
+			return ret;
+		}
+		break;
+
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return 0;
+}
+
+
+
+
+static int rswitch2_serdes_init(struct net_device *ndev, bool check_op);
+static void rswitch2_phy_state_change(struct net_device *ndev)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+	struct phy_device *phydev;
+	u32 reg_val;
+	int ret;
+	unsigned long flags;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+	phy_port = eth_port->phy_port;
+	phydev = ndev->phydev;
+
+	rsw2_info(MSG_GEN, "Link change(%d): %s uses %s at %d Mbps\n", phydev->link, ndev->name, phy_modes(phydev->interface), phydev->speed);
+	if(phydev->speed == SPEED_10000)
+		return;
+
+	if(!phydev->link) {
+
+		void __iomem *etha_base_addr;
+		phy_interface_t phy_iface;
+		int speed;
+
+		etha_base_addr = (void __iomem *)ndev->base_addr;
+		spin_lock_irqsave(&rsw2->lock, flags);
+		rsw2_notice(MSG_GEN, "================= Link Down start (%s) ===================\n", ndev->name);
+
+		/* ETHA state machine will lock up due to SerDes connection in S4 ES 1.0
+		 * Lockup can be release by switching SerDes to USXGMII */
+		if(phy_port->phy_iface != PHY_INTERFACE_MODE_USXGMII) {
+			reg_val = FIELD_PREP(EAMC_OPC, emac_disable);
+			iowrite32(reg_val, etha_base_addr + RSW2_ETHA_EAMC);
+
+			phy_iface = phy_port->phy_iface;
+			speed = phy_port->phy->speed;
+
+			phy_port->phy_iface = PHY_INTERFACE_MODE_USXGMII;
+			phy_port->phy->speed = SPEED_2500;
+			rswitch2_serdes_init(ndev, false);
+
+			rswitch2_emac_set_state(ndev, emac_disable);
+
+			phy_port->phy_iface = phy_iface;
+			phy_port->phy->speed = speed;
+		}
+
+		rsw2_notice(MSG_GEN, "===================== Link Down end =======================\n");
+		spin_unlock_irqrestore(&rsw2->lock, flags);
+
+	} else {
+		uint phy_port_num;
+		void __iomem *etha_base_addr;
+		etha_base_addr = (void __iomem *)ndev->base_addr;
+		spin_lock_irqsave(&rsw2->lock, flags);
+
+		rsw2_notice(MSG_GEN, "================= Link Up start (%s) ==================\n", ndev->name);
+
+		if(phy_port->phy_iface != PHY_INTERFACE_MODE_USXGMII) {
+			phy_port_num = eth_port->port_num - eth_port->rsw2->num_of_cpu_ports;
+
+
+		ret = rswitch2_emac_set_state(ndev, emac_config);
+		if (ret < 0) {
+			rsw2_err(MSG_GEN, "Port set state 'config' failed\n");
+		}
+		else {
+			rsw2_dbg(MSG_GEN, "Port set state 'config' SUCCEEDED\n");
+		}
+
+		reg_val = ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MPIC);
+		reg_val = reg_val & ~0x1Fu;
+
+		switch (phydev->speed) {
+			case SPEED_100:
+				reg_val |= FIELD_PREP(MPIC_LSC, rsw2_rmac_100mbps);
+				reg_val |= FIELD_PREP(MPIC_PIS, rsw2_rmac_gmii);
+				phy_port->phy_iface = PHY_INTERFACE_MODE_SGMII;
+				phy_port->phy->speed = SPEED_100;
+
+				break;
+
+			case SPEED_1000:
+				reg_val |= FIELD_PREP(MPIC_LSC, rsw2_rmac_1000mbps);
+				reg_val |= FIELD_PREP(MPIC_PIS, rsw2_rmac_gmii);
+				phy_port->phy_iface = PHY_INTERFACE_MODE_SGMII;
+				phy_port->phy->speed = SPEED_1000;
+
+				break;
+
+			case SPEED_2500:
+				reg_val |= FIELD_PREP(MPIC_LSC, rsw2_rmac_2500mbps);
+				reg_val |= FIELD_PREP(MPIC_PIS, rsw2_rmac_xgmii);
+				phy_port->phy_iface = PHY_INTERFACE_MODE_USXGMII;
+				phy_port->phy->speed = SPEED_2500;
+
+				break;
+
+			default:
+				rsw2_err(MSG_GEN, "Unsupported Speed\n");
+				spin_unlock_irqrestore(&rsw2->lock, flags);
+				return;
+		}
+
+		rsw2_notice(MSG_GEN, "Link change: %s uses %s at %d Mbps\n", ndev->name, phy_modes(phydev->interface), phydev->speed);
+
+		iowrite32(reg_val, phy_port->rmac_base_addr + RSW2_RMAC_MPIC);
+		rsw2_dbg(MSG_GEN, "reg_val=0x%.8x (expected): 0x%.8x\n", ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MPIC), reg_val);
+
+		ret = rswitch2_emac_set_state(ndev, emac_disable);
+		if (ret < 0) {
+			rsw2_err(MSG_GEN, "Port set state 'disable' failed\n");
+		}
+		else {
+			rsw2_dbg(MSG_GEN, "Port set state 'disable' SUCCEEDED\n");
+		}
+
+		ret = rswitch2_emac_set_state(ndev, emac_operation);
+		if (ret < 0) {
+			rsw2_err(MSG_GEN, "Port set state 'operation' failed\n");
+		}
+		else {
+			rsw2_dbg(MSG_GEN, "Port set state 'operation' SUCCEEDED\n");
+		}
+		}
+
+		rswitch2_serdes_init(ndev, true);
+
+		rsw2_notice(MSG_GEN, "===================== Link Up end =======================\n");
+		spin_unlock_irqrestore(&rsw2->lock, flags);
+
+	}
+
+	if (netif_msg_link(rsw2)) {
+		int phy_speed = 0;
+
+		if(phydev->link)
+			phy_speed =phydev->speed;
+
+		phy_print_status(phydev);
+
+		rsw2_info(MSG_GEN, "Link status changed. PHY %d UID 0x%08x Link = %d Speed = %d\n",
+				  phydev->mdio.addr, phydev->phy_id, phydev->link, phy_speed);
+	}
+	else {
+		rsw2_dbg(MSG_GEN,"PHY state change but no msg link!\n");
+	}
+}
+
+static struct device_node *rswitch2_get_port_node(struct rswitch2_drv *rsw2, unsigned int port_num)
+{
+	struct device_node *ports, *port;
+	int err = 0;
+	u32 index;
+
+	ports = of_get_child_by_name(rsw2->dev->of_node, "ports");
+	if (!ports)
+		return NULL;
+
+	for_each_child_of_node(ports, port) {
+		err = of_property_read_u32(port, "reg", &index);
+		if (err < 0)
+			return NULL;
+		if (index == port_num) {
+			of_node_get(port);
+			break;
+		}
+
+	}
+	of_node_put(ports);
+
+	return port;
+}
+
+
+
+
+
+static int rswitch2_serdes_init_sram(struct rswitch2_drv *rsw2)
+{
+	int ret;
+	int lane;
+	void __iomem *addr;
+
+	//printk("%s()\n", __FUNCTION__);
+	for (lane = 0; lane < rsw2->num_of_tsn_ports; lane++) {
+		addr = rsw2->serdes_base_addr + (RSW2_SERDES_CHANNEL_OFFSET * lane);
+		ret = rswitch2_serdes_reg_wait(addr,
+				VR_XS_PMA_MP_12G_16G_25G_SRAM, BANK_180, BIT(0), 0x01);
+		if (ret) {
+			rsw2_err(MSG_SERDES, "SERDES SRAM init on lane %d failed (step 1)\n", lane);
+			return ret;
+		}
+	}
+
+	//Step 2 only on lane 0
+	rswitch2_serdes_write32(rsw2->serdes_base_addr, VR_XS_PMA_MP_12G_16G_25G_SRAM,
+			       BANK_180, 0x3);
+
+	for (lane = 0; lane < rsw2->num_of_tsn_ports; lane++) {
+		addr = rsw2->serdes_base_addr + (RSW2_SERDES_CHANNEL_OFFSET * lane);
+		ret = rswitch2_serdes_reg_wait(addr,
+				SR_XS_PCS_CTRL1, BANK_300, BIT(15), 0);
+		if (ret) {
+			rsw2_err(MSG_SERDES, "SERDES on lane %d does not finish reset (step 3)\n", lane);
+			return ret;
+		}
+	}
+	return 0;
+}
+
+static int rswitch2_serdes_common_setting(struct rswitch2_drv *rsw2)
+{
+	int ret;
+	int lane;
+	void __iomem *addr;
+
+	enum {UNDEF, SGMII_ONLY, USXGMII_ONLY, MIXED, ERROR} mode = UNDEF;
+
+	/* Check if common init is already done */
+	if (!rsw2->serdes_common_init_done)
+		rsw2->serdes_common_init_done = true;
+	else
+		return 0;
+
+	/* Check the SERDES configuration of all ports to decide which
+	 * PLLs to be activated */
+	for (lane = 0; lane < rsw2->num_of_tsn_ports; lane++) {
+		phy_interface_t phy_iface;
+		uint phy_port_num = lane + rsw2->num_of_cpu_ports;
+		struct rswitch2_eth_port *cur_port = rsw2->ports[phy_port_num];
+
+		if(!cur_port)
+			continue;
+
+		phy_iface = cur_port->phy_port->phy_iface;
+		rsw2_dbg(MSG_SERDES, "PHYiface[%d] is %s (%d)\n", lane, phy_modes(phy_iface), phy_iface);
+
+		switch (phy_iface) {
+		case PHY_INTERFACE_MODE_SGMII :
+			if (mode == UNDEF || mode == SGMII_ONLY) mode = SGMII_ONLY;
+			else if (mode == USXGMII_ONLY || mode == MIXED) mode = MIXED;
+			else mode = ERROR;
+			break;
+		case PHY_INTERFACE_MODE_USXGMII :
+			if (mode == UNDEF || mode == USXGMII_ONLY) mode = USXGMII_ONLY;
+			else if (mode == SGMII_ONLY || mode == MIXED) mode = MIXED;
+			else mode = ERROR;
+			break;
+		default:
+			mode = ERROR;
+		}
+	}
+
+	/* Disable FUSE_OVERRIDE_EN */
+	for (lane = 0; lane < rsw2->num_of_tsn_ports; lane++) {
+		addr = rsw2->serdes_base_addr + RSWITCH_SERDES_FUSE_OVERRIDE(lane);
+		if (ioread32(addr))
+			iowrite32(0, addr);
+	}
+
+	/* Initialize SRAM */
+	ret = rswitch2_serdes_init_sram(rsw2);
+	if (ret)
+		return ret;
+
+	for (lane = 0; lane < rsw2->num_of_tsn_ports; lane++) {
+		addr = rsw2->serdes_base_addr + (RSW2_SERDES_CHANNEL_OFFSET * lane);
+		rswitch2_serdes_write32(addr, VR_XS_PCS_SFTY_MR_CTRL, BANK_380, 0x443);
+	}
+
+	addr = rsw2->serdes_base_addr;
+	switch (mode) {
+	case SGMII_ONLY:
+		rsw2_notice(MSG_SERDES, "Configure SERDES PLLs for SGMII mode\n");
+		//S4.1~S4.5
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_REF_CLK_CTRL, BANK_180, 0x97);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_MPLLB_CTRL0, BANK_180, 0x60);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_MPLLB_CTRL2, BANK_180, 0x2200);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_MPLLB_CTRL1, BANK_180, 0);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_MPLLB_CTRL3, BANK_180, 0x3d);
+		break;
+
+	case USXGMII_ONLY:
+		rsw2_notice(MSG_SERDES, "Configure SERDES PLLs for USXGMII mode\n");
+		//U4.1 ~ U4.5
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_REF_CLK_CTRL, BANK_180, 0x57);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_10G_MPLLA_CTRL2, BANK_180, 0xc200);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_MPLLA_CTRL0, BANK_180, 0x42);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_MPLLA_CTRL1, BANK_180, 0);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_MPLLA_CTRL3, BANK_180, 0x2f);
+		break;
+
+	case MIXED:
+		rsw2_notice(MSG_SERDES, "Configure SERDES PLLs for mixed SGMII and USXGMII mode\n");
+		//to allow mix operation SGMII and USXGMII PLLs configured both
+		//C4.1 == U4.1 | S4.1
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_REF_CLK_CTRL, BANK_180, 0x57 | 0x97);
+
+		//C4.2 == U4.2
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_10G_MPLLA_CTRL2, BANK_180, 0xc200);
+		//C4.3 == U4.3
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_MPLLA_CTRL0, BANK_180, 0x42);
+		//C4.4 == U4.4
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_MPLLA_CTRL1, BANK_180, 0);
+		//C4.5 == U4.5
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_MPLLA_CTRL3, BANK_180, 0x2f);
+
+		//C4.6 == S4.2
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_MPLLB_CTRL0, BANK_180, 0x60);
+		//C4.7 == S4.2
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_MPLLB_CTRL2, BANK_180, 0x2200);
+		//C4.8 == S4.2
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_MPLLB_CTRL1, BANK_180, 0);
+		//C4.9 == S4.2
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_MPLLB_CTRL3, BANK_180, 0x3d);
+		break;
+
+	default:
+		rsw2_err(MSG_SERDES, "Unsupported combination of MAC xMII formats\n");
+		return -EOPNOTSUPP;
+	}
+
+	/* Assert soft reset for SERDES PHY (step 5) */
+	for (lane = 0; lane < rsw2->num_of_tsn_ports; lane++) {
+		addr = rsw2->serdes_base_addr + (RSW2_SERDES_CHANNEL_OFFSET * lane);
+		rswitch2_serdes_write32(addr, 0x03d0, BANK_380, 1);
+	}
+	rswitch2_serdes_write32(rsw2->serdes_base_addr, VR_XS_PCS_DIG_CTRL1, BANK_380, 0x8000);
+
+	/* Re-Initialize SRAM */
+	ret = rswitch2_serdes_init_sram(rsw2);
+	if (ret)
+		return ret;
+
+	/* Check for soft reset done */
+	ret = rswitch2_serdes_reg_wait(rsw2->serdes_base_addr,
+			VR_XS_PCS_DIG_CTRL1, BANK_380, BIT(15), 0);
+	if (ret) {
+		rsw2_err(MSG_SERDES, "SERDES does not finished soft reset (step 8)\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static int rswitch2_serdes_chan_setting(struct rswitch2_eth_port *eth_port)
+{
+	void __iomem *addr;
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_physical_port *phy_port;
+	int ret;
+
+	phy_port = eth_port->phy_port;
+	rsw2 = eth_port->rsw2;
+	addr = phy_port->serdes_chan_addr;
+
+	//printk("%s(port=%d, mode=%d)\n", __FUNCTION__, eth_port->port_num, mode);
+	switch (phy_port->phy_iface) {
+	case PHY_INTERFACE_MODE_SGMII:
+		rswitch2_serdes_write32(addr, VR_XS_PCS_DIG_CTRL1, BANK_380, 0x2000);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_MPLL_CMN_CTRL, BANK_180, 0x11);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_VCO_CAL_LD0, BANK_180, 0x540);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_VCO_CAL_REF0, BANK_180, 0x15);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_RX_GENCTRL1, BANK_180, 0x100);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_CONSUMER_10G_RX_GENCTRL4, BANK_180, 0);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_TX_RATE_CTRL, BANK_180, 0x02);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_RX_RATE_CTRL, BANK_180, 0x03);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_TX_GENCTRL2, BANK_180, 0x100);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_RX_GENCTRL2, BANK_180, 0x100);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_AFE_DFE_EN_CTRL, BANK_180, 0);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_RX_EQ_CTRL0, BANK_180, 0x07);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_10G_RX_IQ_CTRL0, BANK_180, 0);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_TX_GENCTRL1, BANK_180, 0x310);
+
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_TX_GENCTRL2, BANK_180, 0x0101);
+		ret = rswitch2_serdes_reg_wait(addr, VR_XS_PMA_MP_12G_16G_TX_GENCTRL2, BANK_180, BIT(0), 0);
+		if (ret) {
+			rsw2_err(MSG_SERDES, "SerDes req wait failed\n");
+			return ret;
+		}
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_RX_GENCTRL2, BANK_180, 0x101);
+		ret = rswitch2_serdes_reg_wait(addr, VR_XS_PMA_MP_12G_16G_RX_GENCTRL2, BANK_180, BIT(0), 0);
+		if (ret) {
+			rsw2_err(MSG_SERDES, "SerDes req wait failed\n");
+			return ret;
+		}
+
+		//Accept AutoNeg
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_TX_GENCTRL1, BANK_180, 0x1310);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_TX_EQ_CTRL0, BANK_180, 0x1800);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_TX_EQ_CTRL1, BANK_180, 0);
+
+		rswitch2_serdes_write32(addr, SR_XS_PCS_CTRL2, BANK_300, 0x01);
+
+		rswitch2_serdes_write32(addr, VR_XS_PCS_DIG_CTRL1, BANK_380, 0x2100);
+		ret = rswitch2_serdes_reg_wait(addr, VR_XS_PCS_DIG_CTRL1, BANK_380, BIT(8), 0);
+		if (ret) {
+			rsw2_err(MSG_SERDES, "SerDes req wait failed\n");
+			return ret;
+		}
+#if 0
+		//Enable AutoNeg
+		rswitch2_serdes_write32(addr, SR_MII_CTRL, BANK_1F00, 0x0140);  //start from 1000
+		//rswitch2_serdes_write32(addr, SR_MII_CTRL, BANK_1F00, 0x2100);  //start from 100
+		rswitch2_serdes_write32(addr, 0x004, BANK_1F80, 5);
+		rswitch2_serdes_write32(addr, 0x028, BANK_1F80, 0x7a1);
+		rswitch2_serdes_write32(addr, 0x000, BANK_1F80, 0x208);
+#endif
+		break;
+
+	case PHY_INTERFACE_MODE_USXGMII:
+		rswitch2_serdes_write32(addr, SR_XS_PCS_CTRL2, BANK_300, 0x0);
+		rswitch2_serdes_write32(addr, VR_XS_PCS_DEBUG_CTRL, BANK_380, 0x50);
+		rswitch2_serdes_write32(addr, VR_XS_PCS_DIG_CTRL1, BANK_380, 0x2200);
+
+		rswitch2_serdes_write32(addr, VR_XS_PCS_KR_CTRL, BANK_380, 0x400);
+
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_MPLL_CMN_CTRL,
+				       BANK_180, 0x1);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_VCO_CAL_LD0, BANK_180, 0x56a);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_VCO_CAL_REF0, BANK_180, 0x15);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_RX_GENCTRL1, BANK_180, 0x1100);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_CONSUMER_10G_RX_GENCTRL4, BANK_180, 1);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_TX_RATE_CTRL, BANK_180, 0x01);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_RX_RATE_CTRL, BANK_180, 0x01);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_TX_GENCTRL2, BANK_180, 0x300);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_RX_GENCTRL2, BANK_180, 0x300);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_AFE_DFE_EN_CTRL, BANK_180, 0);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_RX_EQ_CTRL0, BANK_180, 0x0);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_10G_RX_IQ_CTRL0, BANK_180, 0);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_TX_GENCTRL1, BANK_180, 0x310);
+
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_TX_GENCTRL2, BANK_180, 0x0301);
+		ret = rswitch2_serdes_reg_wait(addr, VR_XS_PMA_MP_12G_16G_TX_GENCTRL2, BANK_180, BIT(0), 0);
+		if (ret)
+			return ret;
+
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_RX_GENCTRL2, BANK_180, 0x301);
+		ret = rswitch2_serdes_reg_wait(addr, VR_XS_PMA_MP_12G_16G_RX_GENCTRL2, BANK_180, BIT(0), 0);
+		if (ret)
+			return ret;
+
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_TX_GENCTRL1, BANK_180, 0x1310);
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_TX_EQ_CTRL0, BANK_180, 0x1800);
+
+		rswitch2_serdes_write32(addr, VR_XS_PMA_MP_12G_16G_25G_TX_EQ_CTRL1, BANK_180, 0);
+
+		rswitch2_serdes_write32(addr, VR_XS_PCS_DIG_CTRL1, BANK_380, 0x2300);
+		ret = rswitch2_serdes_reg_wait(addr, VR_XS_PCS_DIG_CTRL1, BANK_380, BIT(8), 0);
+		if (ret)
+			return ret;
+
+		//TODO: Some phys does not support auto neg on USXGMII (as the one on VC4)
+		//This code shall be finally enabled/disabled by devicetree
+#if 0
+		//Enter AN_ON
+		rswitch2_serdes_write32(addr, VR_MII_AN_CTRL, BANK_1F80, 1);
+		rswitch2_serdes_write32(addr, SR_MII_CTRL, BANK_1F00, 0x1000);
+		ret = rswitch2_serdes_reg_wait(addr, 8, BANK_1F80, BIT(0), 1);
+		rswitch2_serdes_write32(addr, 8, BANK_1F80, 0);
+		if (ret) {
+				printk("Enter AN_ON failed\n");
+				return ret;
+		}
+#endif
+		break;
+
+	default:
+		return -EOPNOTSUPP;
+	}
+
+	return 0;
+}
+
+#if 0
+static int dummy_config_aneg(struct phy_device *phydev)
+{
+	struct rswitch2_drv *rsw2;
+	struct net_device *ndev = phydev->attached_dev;
+	struct rswitch2_eth_port *eth_port;
+
+	eth_port = netdev_priv(ndev);
+
+	rsw2 = eth_port->rsw2;
+	rsw2_err(MSG_GEN, "dummy_config_aneg(): Link change(%d): %s uses %s at %d Mbps\n", phydev->link, ndev->name, phy_modes(phydev->interface), phydev->speed);
+	phydev->speed = 2500;
+	phydev->link = 1;
+	linkmode_empty(phydev->supported);
+	linkmode_set_bit(ETHTOOL_LINK_MODE_2500baseT_Full_BIT, phydev->supported);
+	//ETHTOOL_LINK_MODE_2500baseT_Full_BIT
+	return 0;
+}
+#endif
+static void rswitch2_serdes_check_operation(struct timer_list *t)
+{
+	struct rswitch2_physical_port *phy_port = from_timer(phy_port, t, serdes_usxgmii_op_timer);
+	unsigned long flags;
+	u32 reg_val;
+
+
+	/* FIXME: May there more elegant way to get eth_port /rsw2 data? */
+	struct rswitch2_drv *rsw2;
+	struct net_device *ndev = phy_port->phy->attached_dev;
+	struct rswitch2_eth_port *eth_port;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	spin_lock_irqsave(&rsw2->lock, flags);
+
+
+	if (phy_port->serdes_usxgmii_op_cnt < RSW2_SERDES_OP_RETRIES) {
+		//Link status is latched, for current status read twice
+		rswitch2_serdes_read32(phy_port->serdes_chan_addr, SR_XS_PCS_STS1, BANK_300);
+		reg_val = rswitch2_serdes_read32(phy_port->serdes_chan_addr, SR_XS_PCS_STS1, BANK_300);
+		if (reg_val & 0x04) {
+			rsw2_notice(MSG_SERDES, "SerDes USXGMII is operational on port %s %s (retries = %d)\n", phy_port->mii_bus->name, phy_port->mii_bus->id, phy_port->serdes_usxgmii_op_cnt);
+		}
+		else {
+			rsw2_notice(MSG_SERDES, "Resetting SerDes USXGMII on port %s %s (retries = %d)\n", phy_port->mii_bus->name, phy_port->mii_bus->id, phy_port->serdes_usxgmii_op_cnt);
+
+
+			reg_val = rswitch2_serdes_read32(phy_port->serdes_chan_addr, VR_XS_PMA_MP_12G_16G_25G_RX_GENCTRL1, BANK_180);
+			rswitch2_serdes_write32(phy_port->serdes_chan_addr, VR_XS_PMA_MP_12G_16G_25G_RX_GENCTRL1, BANK_180, reg_val | 0x10);
+			udelay(10);
+			rswitch2_serdes_write32(phy_port->serdes_chan_addr, VR_XS_PMA_MP_12G_16G_25G_RX_GENCTRL1, BANK_180, reg_val);
+
+			/* Try again */
+			phy_port->serdes_usxgmii_op_cnt++;
+			mod_timer(&phy_port->serdes_usxgmii_op_timer, jiffies + msecs_to_jiffies(RSW2_SERDES_OP_TIMER_INTERVALL));
+		}
+	}
+	else {
+		rsw2_err(MSG_SERDES, "Could not bring SerDes USXGMII on port %s %s into operational state. Giving up!\n", phy_port->mii_bus->name, phy_port->mii_bus->id);
+	}
+
+	spin_unlock_irqrestore(&rsw2->lock, flags);
+}
+
+static int rswitch2_serdes_init(struct net_device *ndev, bool check_op)
+{
+	int ret;
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+	int phy_speed;
+	uint port_num;
+
+	eth_port = netdev_priv(ndev);
+	phy_port = eth_port->phy_port;
+
+	/* Only physical ports have PHYs attached */
+	BUG_ON(phy_port == NULL);
+	rsw2 = eth_port->rsw2;
+
+	ret = rswitch2_serdes_common_setting(rsw2);
+	if (ret)
+		return ret;
+
+	port_num = eth_port->port_num - eth_port->rsw2->num_of_cpu_ports;
+
+	/* TODO: Support more modes and speed selection */
+	//printk("%s: sw0p%d  phy_mode=%d\n", __FUNCTION__, port_num, ndev->phydev->interface );
+	switch (phy_port->phy_iface) {
+	case PHY_INTERFACE_MODE_SGMII:
+		if((phy_port->phy->speed != SPEED_100) && (phy_port->phy->speed != SPEED_1000)) {
+			phy_speed = SPEED_100;
+			rsw2_notice(MSG_SERDES, "No valid default speed. Setting to %d Mbit/s\n", phy_speed);
+		} else
+			phy_speed = phy_port->phy->speed;
+
+		rsw2_notice(MSG_SERDES, "port %d uses SGMII at %d Mbit/s\n", port_num, phy_speed);
+		break;
+
+	case PHY_INTERFACE_MODE_USXGMII:
+		if(phy_port->phy->speed != SPEED_2500) {
+			phy_speed = SPEED_2500;
+			rsw2_notice(MSG_SERDES, "No valid default speed. Setting to %d Mbit/s\n", phy_speed);
+		} else
+			phy_speed = phy_port->phy->speed
+			;
+		rsw2_notice(MSG_SERDES, "port %d uses USXGMII at %d Mbit/s\n", port_num, phy_speed);
+		break;
+
+	default:
+		rsw2_err(MSG_SERDES, "%s: Don't support this interface %d on port %d", __func__,
+				phy_port->phy_iface, port_num);
+		return -EOPNOTSUPP;
+	}
+
+
+	/* Set channel settings*/
+	ret = rswitch2_serdes_chan_setting(eth_port);
+	if (ret) {
+		rsw2_err(MSG_SERDES, "channel specific SERDES configuration for port %d failed. ret=%d\n", port_num, ret);
+		return ret;
+	}
+
+	/* Set speed (bps) */
+	ret = rswitch2_serdes_set_speed(eth_port, phy_speed);
+	if (ret) {
+		rsw2_err(MSG_SERDES, "set initial SERDES speed for port %d failed\n", port_num);
+		return ret;
+	}
+
+#if 0
+	/* The serdes connection to PHYs takes quite long */
+	ret = rswitch2_serdes_reg_wait(phy_port->serdes_chan_addr, SR_XS_PCS_STS1, BANK_300, BIT(2), BIT(2));
+	printk("%s:%d link-up ret=%d\n", __FUNCTION__, __LINE__, ret);
+	if (ret) {
+		//reset RX side and retry
+		u32 value = rswitch2_serdes_read32(phy_port->serdes_chan_addr, VR_XS_PMA_MP_12G_16G_25G_RX_GENCTRL1, BANK_180);
+		rswitch2_serdes_write32(phy_port->serdes_chan_addr, VR_XS_PMA_MP_12G_16G_25G_RX_GENCTRL1, BANK_180, value | 0x10);
+		udelay(10);
+		rswitch2_serdes_write32(phy_port->serdes_chan_addr, VR_XS_PMA_MP_12G_16G_25G_RX_GENCTRL1, BANK_180, value);
+
+		ret = rswitch2_serdes_reg_wait(phy_port->serdes_chan_addr, SR_XS_PCS_STS1, BANK_300, BIT(2), BIT(2));
+		printk("%s:%d link-up ret=%d\n", __FUNCTION__, __LINE__, ret);
+	}
+	if (ret) {
+		netdev_err(ndev, "SerDes on port %d has not reached link up\n", port_num);
+		//return ret;
+	}
+	else
+		dev_info(rsw2->dev, "SerDes on port %d connected.\n", port_num);
+#endif
+
+	//printk("Step 11,12\n");
+	rswitch2_serdes_write32(phy_port->serdes_chan_addr, 0x03c0, BANK_380, 0);
+	rswitch2_serdes_write32(phy_port->serdes_chan_addr, 0x03d0, BANK_380, 0);
+
+	/* USXGMII needs deferred check to proof operation */
+	if(check_op && (phy_port->phy_iface == PHY_INTERFACE_MODE_USXGMII)) {
+		rsw2_info(MSG_SERDES, "Starting SerDes serdes_usxgmii_op_timer for '%s'\n", ndev->name);
+		timer_setup(&phy_port->serdes_usxgmii_op_timer, rswitch2_serdes_check_operation, 0);
+		mod_timer(&phy_port->serdes_usxgmii_op_timer, jiffies + msecs_to_jiffies(RSW2_SERDES_OP_TIMER_INTERVALL_FIRST));
+	}
+
+	return 0;
+}
+
+static void rswitch2_phy_port_update_stats(struct rswitch2_physical_port *phy_port) {
+
+	u64 bytes_upper;
+	u64 bytes_lower;
+
+	phy_port->rx_pkt_cnt += ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MRGFCE);
+	phy_port->rx_pkt_cnt += ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MRGFCP);
+
+	bytes_upper = ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MRXBCEU);
+	bytes_lower = ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MRXBCEL);
+
+	phy_port->rx_byte_cnt += (bytes_upper << 32) | bytes_lower;
+
+
+	phy_port->tx_pkt_cnt += ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MTGFCE);
+	phy_port->tx_pkt_cnt += ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MTGFCP);
+
+	bytes_upper = ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MTXBCEU);
+	bytes_lower = ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MTXBCEL);
+
+	phy_port->tx_byte_cnt += (bytes_upper << 32) | bytes_lower;
+
+}
+
+static void rswitch2_stat_timer(struct timer_list *t)
+{
+	struct rswitch2_eth_port *eth_port  = from_timer(eth_port, t, stat_timer);
+	struct rswitch2_physical_port *phy_port = eth_port->phy_port;
+	unsigned long flags;
+
+	spin_lock_irqsave(&eth_port->rsw2->lock, flags);
+	rswitch2_phy_port_update_stats(phy_port);
+
+	mod_timer(&eth_port->stat_timer, jiffies + msecs_to_jiffies(RSW2_STAT_TIMER_INTERVALL));
+	spin_unlock_irqrestore(&eth_port->rsw2->lock, flags);
+
+
+}
+
+/*static int rswitch2_mac_set_speed(struct rswitch2_eth_port *eth_port)
+{
+	u32 oldval, newval, mask;
+	int ret;
+	struct rswitch2_physical_port *phy_port;
+
+	phy_port = eth_port->phy_port;
+
+	//TODO code the correct speeds
+
+	newval = 0;
+	switch (phy_port->phy_iface) {
+		case PHY_INTERFACE_MODE_SGMII :
+			//speed may chage during link up. MII is fixed
+			newval |= FIELD_PREP(MPIC_LSC, rsw2_rmac_1000mbps);
+			newval |= FIELD_PREP(MPIC_PIS, rsw2_rmac_gmii);
+			break;
+		case PHY_INTERFACE_MODE_USXGMII :
+			//speed may chage during link up. MII is fixed
+			newval |= FIELD_PREP(MPIC_LSC, rsw2_rmac_2500mbps);
+			newval |= FIELD_PREP(MPIC_PIS, rsw2_rmac_xgmii);
+			break;
+		default:
+			netdev_err(eth_port->ndev, "Unsupported MAC xMII format %s (%d)\n",phy_modes(phy_port->phy_iface), phy_port->phy_iface );
+			return -EINVAL;
+	}
+
+	oldval = ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MPIC);
+	mask = MPIC_LSC | MPIC_PIS;
+	if ((oldval & mask) != (newval & mask)) {
+		printk("old mode is %x\n", ioread32(eth_port->ndev->base_addr + RSW2_ETHA_EAMC));
+
+		ret = rswitch2_emac_set_state(eth_port->ndev, emac_disable);
+		if (ret < 0)
+			return ret;
+		ret = rswitch2_emac_set_state(eth_port->ndev, emac_config);
+		if (ret < 0)
+			return ret;
+
+		newval |= oldval & ~mask;
+		printk("[%x] = %x\n", (u32)(phy_port->rmac_base_addr), newval);
+		iowrite32(newval, phy_port->rmac_base_addr + RSW2_RMAC_MPIC);
+
+		oldval = ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MPIC);
+		netdev_info(eth_port->ndev, "Configure MAC to %x\n", oldval);
+
+		ret = rswitch2_emac_set_state(eth_port->ndev, emac_disable);
+		if (ret < 0)
+			return ret;
+		ret = rswitch2_emac_set_state(eth_port->ndev, emac_operation);
+		if (ret < 0)
+			return ret;
+	}
+	return 0;
+}*/
+
+static int rswitch2_eth_open(struct net_device *ndev)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+	struct rswitch2_internal_port *intern_port;
+	int ret;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+	phy_port = eth_port->phy_port;
+	intern_port = eth_port->intern_port;
+
+	/* Gateway port */
+	if (intern_port != NULL) {
+		int cur_q;
+		const uint 	num_of_rx_q = ARRAY_SIZE(intern_port->rx_q);
+
+		ret = rswitch2_gwca_set_state(rsw2, gwmc_operation);
+		if (ret != 0) {
+			rsw2_err(MSG_GEN, "Failed to set GWCA operation state\n");
+		}
+
+		rsw2_notice(MSG_GEN, "internal port open(): '%s'\n", ndev->name);
+		for (cur_q = 0; cur_q < num_of_rx_q; cur_q++) {
+			u32 reg_queue = (cur_q + intern_port->rx_q[cur_q].offset) / 32;
+			u32 bit_queue = (cur_q + intern_port->rx_q[cur_q].offset) % 32;
+
+
+			/* Enable RX interrupt */
+			iowrite32(BIT(bit_queue), rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDIE(reg_queue));
+
+			napi_enable(&intern_port->rx_q[cur_q].napi);
+		}
+		// FIXME:
+		netif_tx_start_all_queues(ndev);
+		//netif_start_queue(ndev);
+	}
+	else {
+		int cur_q;
+		const uint 	num_of_rx_q = ARRAY_SIZE(phy_port->rx_q);
+
+
+		rsw2_notice(MSG_GEN, "physical port open(): '%s'\n", ndev->name);
+		for (cur_q = 0; cur_q < num_of_rx_q; cur_q++) {
+			napi_enable(&phy_port->rx_q[cur_q].napi);
+		}
+
+		netif_tx_start_all_queues(ndev);
+
+
+		ret = rswitch2_serdes_init(ndev, false);
+		if (ret != 0) {
+			rsw2_err(MSG_SERDES, "%s: rswitch2_serdes_init failed: %d\n", ndev->name, ret);
+			return ret;
+		}
+		else {
+			rsw2_info(MSG_SERDES, "%s: rswitch2_serdes_init SUCCESS\n", ndev->name);
+		}
+
+		timer_setup(&eth_port->stat_timer, rswitch2_stat_timer, 0);
+		mod_timer(&eth_port->stat_timer, jiffies + msecs_to_jiffies(RSW2_STAT_TIMER_INTERVALL));
+		if (phy_port != NULL) {
+			rsw2_info(MSG_SERDES, "physical port open(): '%s'\n", ndev->name);
+			phy_start(ndev->phydev);
+
+		}
+
+		phy_attached_info(ndev->phydev);
+	}
+	return 0;
+}
+
+static inline struct rswitch2_eth_port *
+rswitch2_netdev_get_rx_q(struct net_device *ndev, uint q, struct rsw2_rx_q_data **rx_q) {
+
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_internal_port *intern_port;
+	struct rswitch2_physical_port *phy_port;
+
+	eth_port = netdev_priv(ndev);
+	intern_port = eth_port->intern_port;
+	phy_port = eth_port->phy_port;
+
+	if(intern_port)
+		*rx_q = &intern_port->rx_q[q];
+	else
+		*rx_q = &phy_port->rx_q[q];
+
+	return eth_port;
+}
+
+static inline struct rswitch2_eth_port *
+rswitch2_netdev_get_tx_q(struct net_device *ndev, uint q, struct rsw2_tx_q_data **tx_q) {
+
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_internal_port *intern_port;
+	struct rswitch2_physical_port *phy_port;
+
+	eth_port = netdev_priv(ndev);
+	intern_port = eth_port->intern_port;
+	phy_port = eth_port->phy_port;
+
+	if(intern_port)
+		*tx_q = &intern_port->tx_q[q];
+	else
+		*tx_q = &phy_port->tx_q[q];
+
+	return eth_port;
+}
+
+
+static void rswitch2_disable_rx(struct net_device *ndev)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_internal_port *intern_port;
+	struct rswitch2_physical_port *phy_port;
+	uint num_of_rx_queues;
+	uint q;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	intern_port = eth_port->intern_port;
+	phy_port = eth_port->phy_port;
+
+	if(intern_port) {
+		num_of_rx_queues = ARRAY_SIZE(intern_port->rx_q);
+	} else {
+		num_of_rx_queues = ARRAY_SIZE(phy_port->rx_q);
+	}
+
+	for (q = 0; q < num_of_rx_queues; q++) {
+		u32 reg_queue;
+		u32 bit_queue;
+		struct rsw2_rx_q_data *rx_q;
+
+		(void)rswitch2_netdev_get_rx_q(ndev, q, &rx_q);
+
+		reg_queue = (q + rx_q->offset) / RSWITCH2_BITS_PER_REG;
+		bit_queue = (q + rx_q->offset) % RSWITCH2_BITS_PER_REG;
+
+		iowrite32(BIT(bit_queue), rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDID(reg_queue));
+		iowrite32(BIT(bit_queue), rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDIS(reg_queue));
+
+		napi_disable(&intern_port->rx_q[q].napi);
+	}
+}
+
+
+
+static int rswitch2_tx_free(struct net_device *ndev, int q)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+	struct rswitch2_internal_port *intern_port;
+	struct rsw2_tx_q_data *tx_q;
+	struct rswitch2_dma_ext_desc *tx_desc;
+	int free_num = 0;
+	int entry;
+	u32 data_ptr = 0;
+	unsigned int data_len = 0;
+
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	intern_port = eth_port->intern_port;
+	phy_port = eth_port->phy_port;
+	if(intern_port) {
+		tx_q = &intern_port->tx_q[q];
+	} else {
+		tx_q = &phy_port->tx_q[q];
+	}
+
+	while (tx_q->cur_desc - tx_q->dirty_desc > 0) {
+		entry = tx_q->dirty_desc % tx_q->entries;
+		tx_desc = &tx_q->desc_ring[entry];
+
+		if (FIELD_GET(RSW2_DESC_DT, tx_desc->die_dt) != DT_FEMPTY)
+			break;
+
+		/* Descriptor type must be checked before all other reads */
+		dma_rmb();
+
+
+		/* Free the original skb */
+		if (tx_q->skb[entry]) {
+			bool txc = false;
+			uint desc_data_len;
+
+			/* FIXME: Avoid using last descriptor - Add TS pending to tx_q */
+			txc = FIELD_GET(RSW2_DESC_INFO1_TXC, tx_desc->info1);
+
+			desc_data_len = FIELD_GET(RSW2_DESC_DS, le16_to_cpu(tx_desc->info_ds));
+			data_ptr = le32_to_cpu(tx_desc->dptrl - (tx_q->skb[entry]->len - desc_data_len));
+
+			//printk("DMA unmap(): Q: %d    Entry %d   TX desc: 0x%px    0x%.8x  skb_len: %d   len: %d\n", (tx_q->offset + q), entry, tx_desc, data_ptr, tx_q->skb[entry]->len, desc_data_len);
+			dma_unmap_single(ndev->dev.parent, data_ptr, tx_q->skb[entry]->len, DMA_TO_DEVICE);
+
+			/* Timestamped TX skbs are free once the timestamp is fetched */
+			if(!txc)
+				dev_kfree_skb_any(tx_q->skb[entry]);
+
+			tx_q->skb[entry] = NULL;
+			data_ptr = 0;
+			/* TODO: stats */
+			//stats->tx_packets++;
+
+			free_num++;
+		} else {
+			tx_q->skb[entry] = NULL;
+
+			rsw2_dbg(MSG_GEN, "%s: Entry: %d Sum up length of multi frame: len=%u\n",
+					 ndev->name, entry, data_len);
+			free_num++;
+		}
+
+		/* TODO: update stats */
+		//	stats->tx_bytes += size;
+
+		tx_desc->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_EEMPTY);
+		tx_q->dirty_desc++;
+		dma_wmb();
+	}
+
+	return free_num;
+}
+
+
+static int rswitch2_eth_close(struct net_device *ndev)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+	struct rswitch2_internal_port *intern_port;
+	int ret;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+	phy_port = eth_port->phy_port;
+	intern_port = eth_port->intern_port;
+
+	if (intern_port != NULL) {
+		uint cur_q;
+		const uint num_of_tx_q = ARRAY_SIZE(intern_port->tx_q);
+
+		rsw2_info(MSG_GEN, "internal port close(): '%s'\n", ndev->name);
+
+		netif_tx_stop_all_queues(ndev);
+
+		rswitch2_disable_rx(ndev);
+
+		ret = rswitch2_gwca_set_state(rsw2, gwmc_disable);
+		if (ret != 0) {
+			rsw2_err(MSG_GEN,  "Failed to set GWCA disable state\n");
+		}
+
+		for (cur_q = 0; cur_q < num_of_tx_q; cur_q++) {
+			rswitch2_tx_free(ndev, cur_q);
+		}
+
+	} else if (phy_port != NULL) {
+		uint cur_q;
+		const uint num_of_rx_q = ARRAY_SIZE(phy_port->rx_q);
+		const uint num_of_tx_q = ARRAY_SIZE(phy_port->tx_q);
+
+		rsw2_info(MSG_GEN,  "physical port close(): '%s'\n", ndev->name);
+
+		del_timer(&eth_port->stat_timer);
+		del_timer(&phy_port->serdes_usxgmii_op_timer);
+
+		netif_carrier_off(ndev);
+		netif_tx_stop_all_queues(ndev);
+
+		for (cur_q = 0; cur_q < num_of_rx_q; cur_q++) {
+			napi_disable(&phy_port->rx_q[cur_q].napi);
+		}
+		for (cur_q = 0; cur_q < num_of_tx_q; cur_q++) {
+			rswitch2_tx_free(ndev, cur_q);
+		}
+		phy_stop(ndev->phydev);
+
+	}
+
+	return 0;
+}
+
+
+/* Packet receive function for RSwitch2 */
+static bool rswitch2_rx(struct net_device *ndev, int budget, int q)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+	struct rswitch2_internal_port *intern_port;
+	struct rsw2_rx_q_data *rx_q;
+#ifdef RSW2_RX_TS_DESC
+	struct rswitch2_dma_ext_ts_desc *rx_desc;
+#else
+	struct rswitch2_dma_ext_desc *rx_desc;
+#endif
+	struct sk_buff *skb;
+	dma_addr_t dma_addr;
+	int entry;
+	u16 pkt_len;
+	unsigned int count = 0;
+
+	/* TODO: Move to get_rx_q_from_nedv() */
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	intern_port = eth_port->intern_port;
+	phy_port = eth_port->phy_port;
+	if(intern_port) {
+		rx_q = &intern_port->rx_q[q];
+	} else {
+		rx_q = &phy_port->rx_q[q];
+	}
+	rsw2 = eth_port->rsw2;
+
+
+	while (count < budget) {
+		struct skb_shared_hwtstamps *shhwtstamps;
+		struct timespec64 ts;
+
+		entry = rx_q->cur_desc % rx_q->entries;
+		rx_desc = &rx_q->desc_ring[entry];
+		//printk("Abs. Q: %d: RX desc. entry %d of %ld\n", q + rx_q->offset, entry, rx_q->entries);
+
+		if(FIELD_GET(RSW2_DESC_DT, rx_desc->die_dt) == DT_FEMPTY)
+			break;
+
+		/* Descriptor type must be checked before all other reads */
+		dma_rmb();
+
+		count++;
+
+		pkt_len = FIELD_GET(RSW2_DESC_DS, le16_to_cpu(rx_desc->info_ds));
+
+		//printk("RX: pkt_len: %d DMA: 0x%.8x\n", pkt_len, rx_desc->dptrl);
+		skb = rx_q->skb[entry];
+		rx_q->skb[entry] = NULL;
+		dma_unmap_single(ndev->dev.parent, le32_to_cpu(rx_desc->dptrl),
+						RSW2_PKT_BUF_SZ, DMA_FROM_DEVICE);
+
+		shhwtstamps = skb_hwtstamps(skb);
+		memset(shhwtstamps, 0, sizeof(*shhwtstamps));
+		ts.tv_sec = (u64)le32_to_cpu(rx_desc->ts_sec);
+		ts.tv_nsec = le32_to_cpu(rx_desc->ts_nsec & 0x3FFFFFFF);
+		shhwtstamps->hwtstamp = timespec64_to_ktime(ts);
+
+		skb_put(skb, pkt_len);
+		skb->protocol = eth_type_trans(skb, ndev);
+		napi_gro_receive(&rx_q->napi, skb);
+
+		// TODO
+		if(intern_port) {
+			intern_port->rx_pkt_cnt++;
+			intern_port->rx_byte_cnt += pkt_len;
+		}
+		rx_q->cur_desc++;
+	}
+
+	/* Refill the RX ring buffers. */
+	for (; rx_q->cur_desc - rx_q->dirty_desc > 0; rx_q->dirty_desc++) {
+		entry = rx_q->dirty_desc % rx_q->entries;
+
+		rsw2_dbg(MSG_RXTX, "RX Refill Q: %d: RX desc. entry %d of %ld\n", q + rx_q->offset, entry, rx_q->entries);
+
+		rx_desc = &rx_q->desc_ring[entry];
+		rx_desc->info_ds = cpu_to_le16(RSW2_PKT_BUF_SZ);
+		rx_desc->info1 = 0;
+
+		if (!rx_q->skb[entry]) {
+			skb = dev_alloc_skb(RSW2_PKT_BUF_SZ + RSW2_BUF_ALIGN - 1);
+			if (!skb)
+				break;
+
+			skb_reserve(skb, NET_IP_ALIGN);
+
+			dma_addr = dma_map_single(rsw2->dev, skb->data,
+						  RSW2_PKT_BUF_SZ, DMA_FROM_DEVICE);
+
+			/* We just set the data size to 0 for a failed mapping which
+			 * should prevent DMA from happening...
+			 */
+			if (dma_mapping_error(rsw2->dev, dma_addr)) {
+				rsw2_err(MSG_RXTX, "Descriptor Mapping error\n");
+				rx_desc->info_ds = cpu_to_le16(0);
+			}
+			rx_desc->dptrl = cpu_to_le32(dma_addr);
+			skb_checksum_none_assert(skb);
+			rx_q->skb[entry] = skb;
+		} else {
+			rsw2_warn(MSG_RXTX, "%s: SKB already set: rx_desc->dptrl=0x%.8x\n",
+					ndev->name, rx_desc->dptrl);
+		}
+		/* Descriptor type must be set after all the above writes */
+		dma_wmb();
+		rx_desc->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_FEMPTY) | RSW2_DESC_DIE;
+	}
+
+	return count;
+}
+
+
+static int rswitch2_poll(struct napi_struct *napi, int budget)
+{
+	struct net_device *ndev = napi->dev;
+
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+	struct rswitch2_internal_port *intern_port;
+	struct rsw2_rx_q_data *rx_q;
+	unsigned long flags;
+	int q;
+	u32 reg_queue;
+	u32 bit_queue;
+	int work_done = 0;
+	bool rearm_irq;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	rx_q = container_of(napi, struct rsw2_rx_q_data, napi);
+
+	intern_port = eth_port->intern_port;
+	phy_port = eth_port->phy_port;
+	if(intern_port) {
+		q = (rx_q - &intern_port->rx_q[0]);
+	} else {
+		q = (rx_q - &phy_port->rx_q[0]);
+	}
+
+	reg_queue = (q + rx_q->offset) / 32;
+	bit_queue = (q + rx_q->offset) % 32;
+
+	rsw2_dbg(MSG_RXTX, "RX poll. q: %d abs_q: %d   budget: %d reg_queue: %d bit_queue: %d\n", q, (q + rx_q->offset), budget, reg_queue, bit_queue);
+
+	work_done = rswitch2_rx(ndev, budget, q);
+	rearm_irq = napi_complete_done(napi, work_done);
+	if(rearm_irq) {
+		/* Re-enable RX interrupts*/
+		spin_lock_irqsave(&rsw2->lock, flags);
+		iowrite32((1 << bit_queue), rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDIS(reg_queue));
+		iowrite32((1 << bit_queue), rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDIE(reg_queue));
+		spin_unlock_irqrestore(&rsw2->lock, flags);
+	}
+	else {
+		/* FIXME: Remove this. Just left for debugging purpose */
+		rsw2_notice(MSG_RXTX, "NAPI says: don't rearm\n");
+	}
+
+	return work_done;
+}
+
+
+static netdev_tx_t rswitch2_eth_start_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rsw2_tx_q_data *tx_q;
+	struct rswitch2_dma_ext_desc *tx_desc;
+	u32 reg_val;
+	u32 reg_queue;
+	u32 bit_queue;
+	u32 dma_addr;
+	u32 entry;
+	u16 q;
+	unsigned long flags;
+	char *data_ptr;
+	unsigned int data_len;
+
+
+	/* Get current q */
+	q = skb_get_queue_mapping(skb);
+
+	eth_port = rswitch2_netdev_get_tx_q(ndev, q, &tx_q);
+	rsw2 = eth_port->rsw2;
+	rsw2_dbg(MSG_RXTX, "start_xmit() on abs Q: %d  Q: %d\n", (q + tx_q->offset), q);
+
+	/* Free unused descriptors */
+	rswitch2_tx_free(ndev, q);
+
+	if (skb_put_padto(skb, ETH_ZLEN))
+		goto exit;
+
+	// TODO: Handle more data if there are more descriptors
+	// while (netdev_xmit_more() && descr free)
+
+	dma_addr = dma_map_single(rsw2->dev, skb->data, skb->len, DMA_TO_DEVICE);
+	if (dma_mapping_error(rsw2->dev, dma_addr)) {
+		rsw2_err(MSG_RXTX, "DMA mapping failed\n");
+		goto unmap;
+	}
+
+	data_ptr = skb->data;
+	data_len = skb->len;
+
+	if (data_len <= RSWITCH2_MAX_DESC_SIZE) {
+		entry = tx_q->cur_desc % tx_q->entries;
+
+		/* FIXME: atomic  ?? */
+		tx_q->cur_desc++;
+		tx_desc = &tx_q->desc_ring[entry];
+		rsw2_dbg(MSG_RXTX, "Using TX desc. 0x%px\n", tx_desc);
+
+
+		//tx_desc->info_ds = FIELD_PREP(RSW2_DESC_DS, cpu_to_le16(data_len));
+		tx_desc->info_ds = cpu_to_le16(data_len);
+		tx_desc->dptrl = cpu_to_le32(dma_addr);
+		tx_desc->info1 = 0;
+
+		if (eth_port->phy_port) {
+			uint ts_tag_entry = eth_port->phy_port->ts_tag % MAX_TS_Q_ENTRIES_PER_PORT;
+			uint port_num = eth_port->port_num - rsw2->num_of_cpu_ports;
+
+			if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) {
+				rsw2_dbg(MSG_RXTX, "HW TS xmit(): Tag-Entry: %d port_num: %d skb: 0x%px\n", ts_tag_entry, port_num, skb);
+				skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+
+
+				tx_desc->info1 |= FIELD_PREP(RSW2_DESC_INFO1_TXC, 1);
+				tx_desc->info1 |= FIELD_PREP(RSW2_DESC_INFO1_TSUN, ts_tag_entry);
+
+				eth_port->phy_port->ts_skb[ts_tag_entry] = skb;
+				eth_port->phy_port->ts_tag++;
+
+			}
+			tx_desc->info1 |= FIELD_PREP(RSW2_DESC_INFO1_FMT, direct_desc);
+			tx_desc->info1 |= FIELD_PREP(RSW2_DESC_INFO1_DV, 1 << port_num);
+
+			skb_tx_timestamp(skb);
+		}
+
+
+		/* HW won't process descriptor until type is set,
+		 * ensure all other items have been written
+		 */
+		dma_wmb();
+
+		tx_desc->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_FSINGLE);
+	} else {
+		struct rswitch2_dma_ext_desc *tx_start_desc;
+		do {
+			u8 dt_type;
+
+			entry = tx_q->cur_desc % tx_q->entries;
+
+			// FIXME: atomic  ??
+			tx_q->cur_desc++;
+			tx_desc = &tx_q->desc_ring[entry];
+			tx_q->skb[entry] = NULL;
+
+			rsw2_dbg(MSG_RXTX, "Using entry: %d    TX desc. 0x%px\n", entry, tx_desc);
+			tx_desc->dptrl = cpu_to_le32(dma_addr + (skb->len - data_len));
+			tx_desc->info1 = 0;
+			rsw2_dbg(MSG_RXTX, "data_len: %d  skb_len: %d\n", data_len, skb->len);
+
+			if (data_len == skb->len) {
+				rsw2_dbg(MSG_RXTX, "FSTART\n");
+				tx_start_desc = tx_desc;
+				dt_type = FIELD_PREP(RSW2_DESC_DT, DT_FSTART);
+				tx_desc->info_ds = cpu_to_le16(RSWITCH2_MAX_DESC_SIZE);
+				data_len -= RSWITCH2_MAX_DESC_SIZE;
+				if (eth_port->phy_port) {
+					uint ts_tag_entry = eth_port->phy_port->ts_tag % MAX_TS_Q_ENTRIES_PER_PORT;
+
+					if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) {
+						rsw2_dbg(MSG_RXTX, "HW TS xmit() MULTI: Tag-Entry: %d port_num: %d skb: 0x%px\n", ts_tag_entry, (eth_port->port_num - rsw2->num_of_cpu_ports), skb);
+
+						skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+						tx_desc->info1 |= FIELD_PREP(RSW2_DESC_INFO1_TXC, 1);
+						tx_desc->info1 |= FIELD_PREP(RSW2_DESC_INFO1_TSUN, ts_tag_entry);
+						eth_port->phy_port->ts_skb[ts_tag_entry] = skb;
+						eth_port->phy_port->ts_tag++;
+					}
+					skb_tx_timestamp(skb);
+				}
+			} else if (data_len <= RSWITCH2_MAX_DESC_SIZE) {
+				rsw2_dbg(MSG_RXTX, "FEND\n");
+				rsw2_dbg(MSG_RXTX, "Multi TX: Q: %d Entry: %d    TX desc. 0x%px  DMA addr: 0x%.8x  skb_len: %d\n", (q + tx_q->offset), entry, tx_desc, dma_addr, skb->len);
+
+				dt_type = FIELD_PREP(RSW2_DESC_DT, DT_FEND);
+
+				/* FIXME: Hack to pass information to tx_free() not to free skb */
+				tx_desc->info_ds = cpu_to_le16(data_len);
+				tx_desc->info1 |= FIELD_PREP(RSW2_DESC_INFO1_TXC, 1);
+				tx_desc->info1 |= FIELD_PREP(RSW2_DESC_INFO1_TSUN, 0);
+				data_len = 0;
+			} else {
+				rsw2_dbg(MSG_RXTX, "FMID\n");
+				dt_type = FIELD_PREP(RSW2_DESC_DT, DT_FMID);
+				tx_desc->info_ds = cpu_to_le16(RSWITCH2_MAX_DESC_SIZE);
+				data_len -= RSWITCH2_MAX_DESC_SIZE;
+				tx_q->skb[entry] = NULL;
+
+			}
+
+			if (eth_port->phy_port) {
+				uint port_num = eth_port->port_num - rsw2->num_of_cpu_ports;
+
+				tx_desc->info1 |= FIELD_PREP(RSW2_DESC_INFO1_FMT, direct_desc);
+				tx_desc->info1 |= FIELD_PREP(RSW2_DESC_INFO1_DV, 1 << port_num);
+			}
+			if(FIELD_GET(RSW2_DESC_DT, dt_type) != DT_FSTART) {
+				//printk("dt_type: %d\n", dt_type);
+			tx_desc->die_dt = dt_type;
+			}
+		} while (data_len > 0);
+		dma_wmb();
+		tx_start_desc->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_FSTART);
+
+	}
+
+	/* On multi descriptor transmit, the last entry holds the skb data */
+	tx_q->skb[entry] = skb;
+
+	/* Get register offset and bit offset for the used queue */
+	reg_queue = (q + tx_q->offset) / 32;
+	bit_queue = (q + tx_q->offset) % 32;
+
+	rsw2_dbg(MSG_RXTX, "Write kick to RSW2_GCWA_GWTRC(%d) bit %d\n", reg_queue, bit_queue);
+	spin_lock_irqsave(&rsw2->lock, flags);
+	reg_val = GWTRC_TSR(bit_queue);
+	iowrite32(reg_val, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWTRC(reg_queue));
+	spin_unlock_irqrestore(&rsw2->lock, flags);
+
+	/* TODO:: Make generic eth_port counter and use RMAC counters on phy_port */
+	if(eth_port->intern_port) {
+		eth_port->intern_port->tx_pkt_cnt++;
+		eth_port->intern_port->tx_byte_cnt += skb->len;
+	}
+unmap:
+
+exit:
+	return NETDEV_TX_OK;
+}
+
+static void rswitch2_get_ts(struct rswitch2_drv *rsw2) {
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+	struct rswitch2_dma_ts_desc *ts_desc;
+	struct skb_shared_hwtstamps shhwtstamps;
+	struct timespec64 ts;
+	uint ring_entries;
+	u16 tsun;
+	uint src_port_num;
+	uint dest_port_num;
+	uint entry;
+
+
+	ring_entries = rsw2->num_of_tsn_ports * MAX_TS_Q_ENTRIES_PER_PORT;
+	entry = (rsw2->ts_cur_desc) % ring_entries;
+	ts_desc = &rsw2->ts_desc_ring[entry];
+
+	while (FIELD_GET(RSW2_DESC_DT, ts_desc->die_dt) != DT_FEMPTY_ND) {
+		ts.tv_nsec = ts_desc->ts_nsec & 0x3FFFFFFF;
+		ts.tv_sec = ts_desc->ts_sec;
+		tsun = ((ts_desc->tsun) & 0xFF);
+
+		src_port_num = ts_desc->src_port_num;
+		dest_port_num = ts_desc->dest_port_num;
+
+		memset(&shhwtstamps, 0, sizeof(shhwtstamps));
+		shhwtstamps.hwtstamp = timespec64_to_ktime(ts);
+
+		eth_port = rsw2->ports[dest_port_num + rsw2->num_of_cpu_ports];
+		phy_port = eth_port->phy_port;
+		BUG_ON(!phy_port);
+
+		rsw2_dbg(MSG_RXTX, "Got TS for skb: 0x%px\n", phy_port->ts_skb[tsun]);
+		rsw2_dbg(MSG_RXTX, "Entry %d TX timestamp (%.lld:%.ld) for src port %d, dest port %d, tag; %d\n", entry, ts.tv_sec, ts.tv_nsec, src_port_num, dest_port_num, tsun);
+
+		skb_tstamp_tx(phy_port->ts_skb[tsun], &shhwtstamps);
+
+		dev_kfree_skb_any(phy_port->ts_skb[tsun]);
+		phy_port->ts_skb[tsun] = NULL;
+
+		ts_desc->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_FEMPTY_ND);
+		ts_desc->die_dt |= FIELD_PREP(RSW2_DESC_DIE, 1);
+		dma_wmb();
+
+		rsw2->ts_cur_desc++;
+		entry = (rsw2->ts_cur_desc) % ring_entries;
+		ts_desc = &rsw2->ts_desc_ring[entry];
+	}
+
+}
+
+static irqreturn_t rswitch2_status_interrupt(int irq, void *dev_id)
+{
+	struct rswitch2_drv *rsw2 = dev_id;
+	u32 reg_val;
+	irqreturn_t ret = IRQ_HANDLED;
+	unsigned long flags;
+
+	/* Read Timestamp IRQ status GWTSDIS */
+	spin_lock_irqsave(&rsw2->lock, flags);
+	reg_val = ioread32(rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWTSDIS);
+
+	rsw2_dbg(MSG_GEN, "GWTSDIS: 0x%.8x\n", reg_val);
+	if((reg_val & BIT(0)) == BIT(0)) {
+		/* Mask IRQ */
+		iowrite32(BIT(0), rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWTSDID);
+		spin_unlock_irqrestore(&rsw2->lock, flags);
+
+		rswitch2_get_ts(rsw2);
+
+		spin_lock_irqsave(&rsw2->lock, flags);
+		/* Ack IRQ */
+		iowrite32(BIT(0), rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWTSDIS);
+		/* Re-enable IRQ */
+		iowrite32(BIT(0), rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWTSDIE);
+	}
+	spin_unlock_irqrestore(&rsw2->lock, flags);
+
+	return ret;
+}
+
+static irqreturn_t rswitch2_eth_interrupt(int irq, void *dev_id)
+{
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_drv *rsw2 = dev_id;
+	struct napi_struct *cur_napi;
+	irqreturn_t ret = IRQ_HANDLED;
+	u32 reg_val;
+	u32 reg_num;
+	u32 reg_bit;
+	u32 cur_queue;
+	u32 irq_status[RSWITCH2_CHAIN_REG_NUM];
+	u32 irq_active[RSWITCH2_CHAIN_REG_NUM];
+	unsigned long flags;
+
+	spin_lock_irqsave(&rsw2->lock, flags);
+	for (reg_num = 0; reg_num < RSWITCH2_CHAIN_REG_NUM; reg_num++) {
+		irq_status[reg_num] = ioread32(rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDIS(reg_num));
+		irq_active[reg_num] = ioread32(rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDIE(reg_num));
+		iowrite32(irq_status[reg_num], rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDID(reg_num));
+	}
+	spin_unlock_irqrestore(&rsw2->lock, flags);
+
+
+	for (reg_num = 0; reg_num < RSWITCH2_CHAIN_REG_NUM; reg_num++) {
+		reg_val = irq_status[reg_num] & irq_active[reg_num];
+
+		for (reg_bit = 0; reg_bit < RSWITCH2_BITS_PER_REG; reg_bit++) {
+			if ((BIT(reg_bit) & reg_val) == BIT(reg_bit)) {
+				uint port_num;
+				uint port_q;
+				cur_queue = (reg_num * RSWITCH2_BITS_PER_REG) + reg_bit;
+
+				port_num = rsw2->port_backref[cur_queue].port_num;
+				eth_port = rsw2->ports[port_num];
+
+				port_q = rsw2->port_backref[cur_queue].port_q;
+				if(eth_port->intern_port)
+					cur_napi = &eth_port->intern_port->rx_q[port_q].napi;
+				else
+					cur_napi = &eth_port->phy_port->rx_q[port_q].napi;
+
+				if (napi_schedule_prep(cur_napi))
+					__napi_schedule(cur_napi);
+				else
+					/* Although this is no real problem, it shouldn't happen.
+					 * It wastes CPU time with unnecessary IRQ handling
+					 */
+					rsw2_warn(MSG_RXTX, "NAPI is already running Q: %d\n", cur_queue);
+			}
+		}
+	}
+
+	return ret;
+}
+
+
+
+
+
+
+static u16 rswitch2_eth_select_queue(struct net_device *ndev, struct sk_buff *skb,
+							struct net_device *sb_dev)
+{
+	/* TODO: Select queue as needed. E.g. TX timestamp, it is handled by NC queue */
+	return 0;
+}
+
+static struct net_device_stats *rswitch2_eth_get_stats(struct net_device *ndev)
+{
+	struct net_device_stats *nstats;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_internal_port *intern_port;
+	struct rswitch2_physical_port *phy_port;
+	unsigned long flags;
+
+	nstats = &ndev->stats;
+
+	eth_port = netdev_priv(ndev);
+	intern_port = eth_port->intern_port;
+	phy_port = eth_port->phy_port;
+
+	spin_lock_irqsave(&eth_port->rsw2->lock, flags);
+
+	/* Only internal ports can TX */
+	if(intern_port) {
+		nstats->rx_packets = intern_port->rx_pkt_cnt;
+		nstats->tx_packets = intern_port->tx_pkt_cnt;
+		nstats->rx_bytes = intern_port->rx_byte_cnt;
+		nstats->tx_bytes = intern_port->tx_byte_cnt;
+		nstats->multicast = 0;
+		nstats->rx_errors = 0;
+		nstats->rx_crc_errors = 0;
+		nstats->rx_frame_errors = 0;
+		nstats->rx_length_errors = 0;
+		nstats->rx_missed_errors = 0;
+		nstats->rx_over_errors = 0;
+
+	} else if(phy_port) {
+		rswitch2_phy_port_update_stats(phy_port);
+		nstats->rx_packets = phy_port->rx_pkt_cnt;
+		nstats->tx_packets = phy_port->tx_pkt_cnt;
+		nstats->rx_bytes = phy_port->rx_byte_cnt;
+		nstats->tx_bytes = phy_port->tx_byte_cnt;
+		nstats->multicast = 0;
+		nstats->rx_errors = 0;
+		nstats->rx_crc_errors = 0;
+		nstats->rx_frame_errors = 0;
+		nstats->rx_length_errors = 0;
+		nstats->rx_missed_errors = 0;
+		nstats->rx_over_errors = 0;
+
+	}
+	spin_unlock_irqrestore(&eth_port->rsw2->lock, flags);
+
+	return nstats;
+}
+
+/* TODO: Update promiscuous bit */
+static void rswitch2_eth_set_rx_mode(struct net_device *ndev)
+{
+}
+
+/* TODO: Reset on TX Timeout */
+static void rswitch2_eth_tx_timeout(struct net_device *ndev, unsigned int txqueue)
+{
+}
+
+static int rswitch2_hwstamp_get(struct net_device *ndev, struct ifreq *req)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_internal_port *intern_port;
+	struct rswitch2_physical_port *phy_port;
+	struct rtsn_ptp_private *ptp_priv;
+	struct hwtstamp_config config;
+
+	eth_port = netdev_priv(ndev);
+	intern_port = eth_port->intern_port;
+	phy_port = eth_port->phy_port;
+
+
+	if(intern_port) {
+		rsw2_err(MSG_GEN, "HW get intern port!?\n");
+		return -EINVAL;
+	}
+	rsw2 = eth_port->rsw2;
+	ptp_priv = rsw2->ptp_drv;
+
+
+	config.flags = 0;
+	config.tx_type = ptp_priv->tstamp_tx_ctrl ? HWTSTAMP_TX_ON :
+						    HWTSTAMP_TX_OFF;
+	switch (ptp_priv->tstamp_rx_ctrl & RTSN_RXTSTAMP_TYPE) {
+	case RTSN_RXTSTAMP_TYPE_V2_L2_EVENT:
+		config.rx_filter = HWTSTAMP_FILTER_PTP_V2_L2_EVENT;
+		break;
+	case RTSN_RXTSTAMP_TYPE_ALL:
+		config.rx_filter = HWTSTAMP_FILTER_ALL;
+		break;
+	default:
+		config.rx_filter = HWTSTAMP_FILTER_NONE;
+		break;
+	}
+
+	return copy_to_user(req->ifr_data, &config, sizeof(config)) ? -EFAULT : 0;
+}
+
+static int rswitch2_hwstamp_set(struct net_device *ndev, struct ifreq *req)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_internal_port *intern_port;
+	struct rswitch2_physical_port *phy_port;
+	struct rtsn_ptp_private *ptp_priv;
+	struct hwtstamp_config config;
+	u32 tstamp_rx_ctrl = RTSN_RXTSTAMP_ENABLED;
+	u32 tstamp_tx_ctrl;
+
+
+	eth_port = netdev_priv(ndev);
+	intern_port = eth_port->intern_port;
+	phy_port = eth_port->phy_port;
+
+	if(intern_port) {
+		rsw2_err(MSG_GEN, "HW set intern port!?\n");
+		return -EINVAL;
+	}
+
+	rsw2 = eth_port->rsw2;
+	ptp_priv = rsw2->ptp_drv;
+
+	if (copy_from_user(&config, req->ifr_data, sizeof(config)))
+		return -EFAULT;
+
+	if (config.flags)
+		return -EINVAL;
+
+	switch (config.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		tstamp_tx_ctrl = 0;
+		break;
+	case HWTSTAMP_TX_ON:
+		tstamp_tx_ctrl = RTSN_TXTSTAMP_ENABLED;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (config.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		tstamp_rx_ctrl = 0;
+		break;
+	case HWTSTAMP_FILTER_PTP_V2_L2_EVENT:
+		tstamp_rx_ctrl |= RTSN_RXTSTAMP_TYPE_V2_L2_EVENT;
+		break;
+	default:
+		config.rx_filter = HWTSTAMP_FILTER_ALL;
+		tstamp_rx_ctrl |= RTSN_RXTSTAMP_TYPE_ALL;
+		break;
+	}
+
+	ptp_priv->tstamp_tx_ctrl = tstamp_tx_ctrl;
+	ptp_priv->tstamp_rx_ctrl = tstamp_rx_ctrl;
+
+	return copy_to_user(req->ifr_data, &config, sizeof(config)) ? -EFAULT : 0;
+}
+
+
+static int rswitch2_eth_do_ioctl(struct net_device *ndev, struct ifreq *req, int cmd)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct phy_device *phydev = ndev->phydev;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	if (!netif_running(ndev)) {
+		rsw2_warn(MSG_GEN, "netif not running\n");
+
+		return -EINVAL;
+	}
+
+	if (!phydev) {
+		rsw2_warn(MSG_GEN, "no phydev\n");
+		return -ENODEV;
+	}
+	switch (cmd) {
+	case SIOCGHWTSTAMP:
+		/* HW get timestamp */
+		return rswitch2_hwstamp_get(ndev, req);
+		break;
+	case SIOCSHWTSTAMP:
+		/* HW set timestamp */
+		return rswitch2_hwstamp_set(ndev, req);
+		break;
+	}
+
+	return phy_mii_ioctl(phydev, req, cmd);
+}
+
+static int rswitch2_eth_change_mtu(struct net_device *ndev, int new_mtu)
+{
+	if (netif_running(ndev))
+		return -EBUSY;
+
+	ndev->mtu = new_mtu;
+	netdev_update_features(ndev);
+
+	return 0;
+}
+
+
+static void rswitch2_port_set_mac_addr(struct net_device *port_ndev)
+{
+	u32 reg_val;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+
+	eth_port = netdev_priv(port_ndev);
+	phy_port = eth_port->phy_port;
+
+	reg_val = FIELD_PREP(MRMAC0_MAUP,
+			((port_ndev->dev_addr[0] & 0xff) << 8) | port_ndev->dev_addr[1]);
+	iowrite32(reg_val, phy_port->rmac_base_addr + RSW2_RMAC_MRMAC0);
+
+	reg_val = ((port_ndev->dev_addr[2] & 0xff) << 24);
+	reg_val |= ((port_ndev->dev_addr[3] & 0xff) << 16);
+	reg_val |= ((port_ndev->dev_addr[4] & 0xff) << 8) | port_ndev->dev_addr[5];
+	iowrite32(reg_val, phy_port->rmac_base_addr + RSW2_RMAC_MRMAC1);
+}
+
+/* TODO: Currently unused. Maybe used if MAC is set by bootloader */
+#if 0
+static void rswitch2_port_get_mac_addr(struct net_device *port_ndev)
+{
+	u32 reg_val;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+
+	eth_port = netdev_priv(port_ndev);
+	phy_port = eth_port->phy_port;
+
+	reg_val = ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MRMAC0);
+	port_ndev->dev_addr[1] = FIELD_GET(MRMAC0_MAUP, reg_val) & 0xff;
+	port_ndev->dev_addr[0] = (FIELD_GET(MRMAC0_MAUP, reg_val) << 8) & 0xff;
+
+	reg_val = ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MRMAC1);
+	port_ndev->dev_addr[5] = (reg_val & 0xff);
+	port_ndev->dev_addr[4] = ((reg_val << 8) & 0xff);
+	port_ndev->dev_addr[3] = ((reg_val << 26) & 0xff);
+	port_ndev->dev_addr[2] = 8(reg_val << 24) & 0xff);
+
+}
+#endif
+
+
+static void rswitch2_intern_set_mac_addr(struct net_device *ndev)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	int num_of_ports;
+	u32 reg_val;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	num_of_ports = (rsw2->num_of_cpu_ports + rsw2->num_of_tsn_ports);
+	rsw2_fwd_add_l2_entry(rsw2, ndev->dev_addr,  ((1 << num_of_ports) - 1),  ((1 << num_of_ports) - 1), rsw2_be_rx_q_0);
+
+	reg_val = FIELD_PREP(GWMAC0_MAUP,
+			((ndev->dev_addr[0] & 0xff) << 8) | ndev->dev_addr[1]);
+	iowrite32(reg_val, ((volatile void __iomem *)ndev->base_addr) + RSW2_GCWA_GWMAC0);
+
+	reg_val = ((ndev->dev_addr[2] & 0xff) << 24);
+	reg_val |= ((ndev->dev_addr[3] & 0xff) << 16);
+	reg_val |= ((ndev->dev_addr[4] & 0xff) << 8) | ndev->dev_addr[5];
+	iowrite32(reg_val, ((volatile void __iomem *)ndev->base_addr) + RSW2_GCWA_GWMAC1);
+}
+
+
+static int rswitch2_eth_mac_addr(struct net_device *ndev, void *p)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct sockaddr *addr = p;
+	u8 old_macaddr[ETH_ALEN];
+	u8 *new_macaddr;
+
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	if (netif_running(ndev))
+		return -EBUSY;
+
+	new_macaddr = (u8 *)addr->sa_data;
+	memcpy(old_macaddr, ndev->dev_addr, ETH_ALEN);
+
+	rsw2_dbg(MSG_GEN, "Update MAC for '%s' from %.2X:%.2X:%.2X:%.2X:%.2X:%.2X "
+							                 "to %.2X:%.2X:%.2X:%.2X:%.2X:%.2X\n",
+			ndev->name,
+			old_macaddr[0], old_macaddr[1],
+			old_macaddr[2], old_macaddr[3],
+			old_macaddr[4], old_macaddr[5],
+			new_macaddr[0], new_macaddr[1],
+			new_macaddr[2], new_macaddr[3],
+			new_macaddr[4], new_macaddr[5]
+	);
+
+	memcpy(ndev->dev_addr, addr->sa_data, ETH_ALEN);
+
+
+	if(eth_port->intern_port) {
+		rswitch2_intern_set_mac_addr(ndev);
+	} else {
+		/* Update fwd engine */
+		rswitch2_port_set_mac_addr(ndev);
+	}
+	rsw2_fwd_del_l2_entry(rsw2, old_macaddr);
+
+
+	return 0;
+}
+
+static const struct net_device_ops rswitch2_netdev_ops = {
+	.ndo_open				= rswitch2_eth_open,
+	.ndo_stop				= rswitch2_eth_close,
+	.ndo_start_xmit			= rswitch2_eth_start_xmit,
+	.ndo_select_queue		= rswitch2_eth_select_queue,
+	.ndo_get_stats			= rswitch2_eth_get_stats,
+	.ndo_set_rx_mode		= rswitch2_eth_set_rx_mode,
+	.ndo_tx_timeout			= rswitch2_eth_tx_timeout,
+	.ndo_do_ioctl			= rswitch2_eth_do_ioctl,
+	.ndo_change_mtu			= rswitch2_eth_change_mtu,
+	.ndo_validate_addr		= eth_validate_addr,
+	.ndo_set_mac_address	= rswitch2_eth_mac_addr,
+};
+
+
+/* Allocate TS descriptor base address table */
+static int rswitch2_ts_ring_init(struct rswitch2_drv *rsw2)
+{
+	struct rswitch2_dma_desc *bat_entry;
+	struct rswitch2_dma_ts_desc *ts_desc;
+	struct rswitch2_dma_ext_desc *dma_desc;
+	size_t ts_ring_size;
+	uint ring_entries;
+	int i;
+	u32 reg_val;
+	int ret;
+
+
+	rsw2->ts_cur_desc = 0;
+	rsw2->ts_dirty_desc = 0;
+
+
+	/* Create BAT entry for TS descriptors */
+	bat_entry = dma_alloc_coherent(rsw2->dev, sizeof(*bat_entry), &rsw2->bat_ts_dma_addr, GFP_KERNEL);
+	if (!bat_entry) {
+		ret = -ENOMEM;
+		goto bat_alloc_err;
+	}
+	rsw2->bat_ts_addr = bat_entry;
+
+	rsw2_info(MSG_RXTX, "BAT TS: BAT entry is at 0x%px DMA: (DMA: 0x%llx)\n", bat_entry, rsw2->bat_ts_dma_addr);
+
+
+	ring_entries = rsw2->num_of_tsn_ports * MAX_TS_Q_ENTRIES_PER_PORT;
+	ts_ring_size = sizeof(*ts_desc) * (ring_entries + 1);
+
+	rsw2_dbg(MSG_RXTX, "rswitch2_ts_ring_init(): ring_size: %ld\n", ts_ring_size);
+
+	rsw2->ts_desc_ring = dma_alloc_coherent(rsw2->dev, ts_ring_size,
+											&rsw2->ts_desc_dma, GFP_KERNEL);
+
+	if (!rsw2->ts_desc_ring) {
+		rsw2_err(MSG_RXTX, "Failed to alloc memory for TS descriptor ring\n");
+		ret = -ENOMEM;
+		goto dma_alloc_err;
+	}
+
+	memset(rsw2->ts_desc_ring, 0, ts_ring_size);
+
+	/* Init descriptors  */
+	for (i = 0, ts_desc = rsw2->ts_desc_ring; i < ring_entries; i++, ts_desc++) {
+		ts_desc->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_FEMPTY_ND);
+		ts_desc->die_dt |= FIELD_PREP(RSW2_DESC_DIE, 1);
+	}
+
+	rsw2_dbg(MSG_RXTX, "TS Ring is at 0x%px (DMA: 0x%llx)\n", rsw2->ts_desc_ring, rsw2->ts_desc_dma);
+
+	/* Close the loop */
+	dma_desc = (struct rswitch2_dma_ext_desc *)ts_desc;
+	dma_desc->dptrl = cpu_to_le32((u32)rsw2->ts_desc_dma);
+	dma_desc->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_LINKFIX); /* type */
+
+	/* Enlist ring to BAT */
+	bat_entry->die_dt = DT_LINKFIX; /* type */
+	bat_entry->dptrl = cpu_to_le32((u32)rsw2->ts_desc_dma);
+
+	dma_wmb();
+
+	iowrite32(rsw2->bat_ts_dma_addr, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWTDCAC1(0));
+	iowrite32(0, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWTDCAC0(0));
+
+	reg_val = FIELD_PREP(GWTSDCC_TE, 1);
+	iowrite32(reg_val, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWTSDCC(0));
+
+	/* Enable timestamp interrupt */
+	reg_val = FIELD_PREP(GWTSDIE_TSDIE, 1);
+	iowrite32(reg_val, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWTSDIE);
+
+	return 0;
+
+dma_alloc_err:
+	dma_free_coherent(rsw2->dev, sizeof(*bat_entry), rsw2->bat_ts_addr, rsw2->bat_ts_dma_addr);
+
+bat_alloc_err:
+
+	return ret;
+}
+
+
+
+/* Allocate descriptor base address table */
+static int rswitch2_bat_init(struct rswitch2_drv *rsw2)
+{
+	int ret;
+	struct rswitch2_dma_desc *bat_entry;
+	size_t bat_size;
+	int i;
+
+	bat_size = sizeof(*bat_entry) * BAT_ENTRIES;
+
+	rsw2->bat_addr = kzalloc(sizeof(*rsw2->bat_addr) * BAT_ENTRIES, GFP_KERNEL);
+	if (!rsw2->bat_addr) {
+		ret = -ENOMEM;
+		goto bat_ptr_alloc_err;
+	}
+	bat_entry = dma_alloc_coherent(rsw2->dev, bat_size, &rsw2->bat_dma_addr, GFP_KERNEL);
+	if (!bat_entry) {
+		ret = -ENOMEM;
+		goto dma_alloc_err;
+	}
+	rsw2_info(MSG_RXTX, "BAT is at 0x%px (DMA: 0x%llx)\n", rsw2->bat_addr, rsw2->bat_dma_addr);
+
+	/* Init BE RX BAT */
+	for (i = rsw2_be_rx_q_0; i < rsw2_be_rx_q_max_entry; i++) {
+		//pr_info("Creating BAT entry for queue %d (RX queue %d)\n", i, (i - RSW2_BE_RX_Q_OFFSET));
+		bat_entry[i].die_dt = FIELD_PREP(RSW2_DESC_DT, DT_EOS);
+		rsw2->bat_addr[i] = &bat_entry[i];
+	}
+
+	/* Init LL RX BAT */
+	for (i = rsw2_ll_rx_q_port0; i < rsw2_ll_rx_q_max_entry; i++) {
+		//pr_info("Creating BAT entry for queue %d (RX queue %d)\n", i, (i - RSW2_LL_RX_Q_OFFSET));
+		bat_entry[i].die_dt = FIELD_PREP(RSW2_DESC_DT, DT_EOS);
+		rsw2->bat_addr[i] = &bat_entry[i];
+	}
+
+
+	for (i = rsw2_be_tx_q_0; i < rsw2_be_tx_q_max_entry; i++) {
+		//pr_info("Creating BAT entry for queue %d (TX queue %d)\n", i, (i - RSW2_BE_TX_Q_OFFSET));
+		bat_entry[i].die_dt = FIELD_PREP(RSW2_DESC_DT, DT_EOS);
+		rsw2->bat_addr[i] = &bat_entry[i];
+	}
+
+	/* Init LL TX BAT */
+	for (i = rsw2_ll_tx_q_port0; i < rsw2_ll_tx_q_max_entry; i++) {
+		//pr_info("Creating BAT entry for queue %d (TX queue %d)\n", i, (i - RSW2_LL_TX_Q_OFFSET));
+		bat_entry[i].die_dt = FIELD_PREP(RSW2_DESC_DT, DT_EOS);
+		rsw2->bat_addr[i] = &bat_entry[i];
+	}
+
+	iowrite32(rsw2->bat_dma_addr, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDCBAC1);
+	iowrite32(0, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDCBAC0);
+
+	return 0;
+
+dma_alloc_err:
+	kfree(rsw2->bat_addr);
+
+bat_ptr_alloc_err:
+
+	return ret;
+}
+
+static int rswitch2_rx_ring_format(struct net_device *ndev, int q)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rsw2_rx_q_data *rx_q;
+	struct rswitch2_dma_desc *bat_entry;
+#ifdef RSW2_RX_TS_DESC
+	struct rswitch2_dma_ext_ts_desc *rx_desc;
+#else
+	struct rswitch2_dma_ext_desc *rx_desc;
+#endif
+	int ring_size;
+	int i;
+	dma_addr_t dma_addr;
+	u32 reg_queue;
+	u32 bit_queue;
+	u32 reg_val;
+
+	eth_port = rswitch2_netdev_get_rx_q(ndev, q, &rx_q);
+	BUG_ON(!eth_port);
+
+	rsw2 = eth_port->rsw2;
+
+	ring_size = sizeof(*rx_q->desc_ring) * (rx_q->entries + 1);
+	memset(rx_q->desc_ring, 0, ring_size);
+
+	for (i = 0, rx_desc = rx_q->desc_ring; i < rx_q->entries; i++, rx_desc++) {
+		rx_desc->info_ds = cpu_to_le16(RSW2_PKT_BUF_SZ);
+
+		dma_addr = dma_map_single(rsw2->dev, (void *)rx_q->skb[i]->data,
+					  	  	  	  RSW2_PKT_BUF_SZ, DMA_FROM_DEVICE);
+
+		/* We just set the data size to 0 for a failed mapping which
+		 * should prevent DMA from happening...
+		 */
+		if (dma_mapping_error(rsw2->dev, dma_addr)) {
+			dev_err(rsw2->dev, "Descriptor mapping error\n");
+			rx_desc->info_ds = cpu_to_le16(0);
+
+			return -ENOMEM;
+		}
+		rx_desc->dptrl = cpu_to_le32(dma_addr);
+		rx_desc->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_FEMPTY) | RSW2_DESC_DIE;
+	}
+	/* Close the loop */
+	rx_desc->dptrl = cpu_to_le32((u32)rx_q->desc_dma);
+	rx_desc->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_LINKFIX); /* type */
+
+	bat_entry = rsw2->bat_addr[q + rx_q->offset];
+	bat_entry->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_LINKFIX); /* type */
+	bat_entry->dptrl = cpu_to_le32((u32)rx_q->desc_dma);
+
+	reg_val = FIELD_PREP(GWDCC_BALR, 1); /* Base address load request */
+	reg_val |= FIELD_PREP(GWDCC_DCP, 1); /* Chain priority */
+	reg_val |= FIELD_PREP(GWDCC_EDE, 1); /* Extended Descriptor Enable */
+
+#ifdef RSW2_RX_TS_DESC
+	reg_val |= FIELD_PREP(GWDCC_ETS, 1); /* Extended Timestamp Enable */
+#endif
+	iowrite32(reg_val, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDCC(q + rx_q->offset));
+
+	/* Activate RX interrupt for this q */
+	reg_queue = (q + rx_q->offset) / RSWITCH2_BITS_PER_REG;
+	bit_queue = (q + rx_q->offset) % RSWITCH2_BITS_PER_REG;
+
+	reg_val = GWDIE_DIE(bit_queue);
+	iowrite32(reg_val, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDIE(reg_queue));
+
+	return 0;
+}
+
+static int rswitch2_rx_ring_init(struct net_device *ndev, uint q, uint q_offset)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rsw2_rx_q_data *rx_q;
+	struct sk_buff *skb;
+	int ring_size;
+	int i;
+
+	eth_port = rswitch2_netdev_get_rx_q(ndev, q, &rx_q);
+	BUG_ON(!eth_port);
+
+	rsw2 = eth_port->rsw2;
+
+	rx_q->offset = q_offset;
+	rsw2->port_backref[q + q_offset].port_num = eth_port->port_num;
+	rsw2->port_backref[q + q_offset].port_q = q;
+
+	ring_size = sizeof(*rx_q->desc_ring) * (rx_q->entries + 1);
+
+	rx_q->skb = kcalloc(rx_q->entries, sizeof(*rx_q->skb), GFP_KERNEL);
+	if (!rx_q->skb)
+		goto skb_ptr_alloc_err;
+
+	for (i = 0; i < rx_q->entries; i++) {
+		skb = dev_alloc_skb(RSW2_PKT_BUF_SZ + RSW2_BUF_ALIGN - 1);
+		if (!skb) {
+			pr_err("Failed to allocate skb for Q %d\n", q + q_offset);
+			goto skb_alloc_err;
+		}
+		skb_reserve(skb, NET_IP_ALIGN);
+		rx_q->skb[i] = skb;
+	}
+
+	rx_q->desc_ring = dma_alloc_coherent(rsw2->dev, ring_size,
+					     	 	 	 	 &rx_q->desc_dma, GFP_KERNEL);
+
+	if (!rx_q->desc_ring)
+		goto dma_alloc_err;
+
+	rx_q->dirty_desc = 0;
+
+	return 0;
+
+dma_alloc_err:
+	while (--i < 0)
+		dev_kfree_skb(rx_q->skb[i]);
+skb_alloc_err:
+	kfree(rx_q->skb);
+skb_ptr_alloc_err:
+
+	return -ENOMEM;
+}
+
+
+static int rswitch2_tx_ring_init(struct net_device *ndev, uint q, uint q_offset)
+{
+	struct rswitch2_eth_port *eth_port;
+	struct rsw2_tx_q_data *tx_q;
+	struct rswitch2_drv *rsw2;
+	int ring_size;
+
+	eth_port = rswitch2_netdev_get_tx_q(ndev, q, &tx_q);
+	BUG_ON(!eth_port);
+
+	rsw2 = eth_port->rsw2;
+	tx_q->offset = q_offset;
+	rsw2->port_backref[q + q_offset].port_num = eth_port->port_num;
+	rsw2->port_backref[q + q_offset].port_q = q;
+
+	/* Allocate ptr to sk_bufs */
+	tx_q->skb = kcalloc(tx_q->entries, sizeof(*tx_q->skb), GFP_KERNEL);
+	if (!tx_q->skb)
+		goto tx_skb_err;
+
+	/* Allocate all TX descriptors plus 1 to point back */
+	ring_size = sizeof(*tx_q->desc_ring) * (tx_q->entries + 1);
+
+	tx_q->desc_ring = dma_alloc_coherent(rsw2->dev,
+					      ring_size, &tx_q->desc_dma, GFP_KERNEL);
+	if (!tx_q->desc_ring)
+		goto tx_ring_err;
+
+	tx_q->cur_desc = 0;
+
+	return 0;
+
+tx_ring_err:
+	kfree(tx_q->skb);
+tx_skb_err:
+	return -ENOMEM;
+}
+
+
+
+static int rswitch2_tx_ring_format(struct net_device *ndev, int q)
+{
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_dma_desc *bat_entry;
+	struct rsw2_tx_q_data *tx_q;
+	struct rswitch2_dma_ext_desc *tx_desc;
+	size_t ring_size;
+	u32 reg_val;
+	int i;
+
+	eth_port = rswitch2_netdev_get_tx_q(ndev, q, &tx_q);
+	BUG_ON(!eth_port);
+	rsw2 = eth_port->rsw2;
+
+	tx_q->cur_desc = 0;
+	tx_q->dirty_desc = 0;
+
+	ring_size = sizeof(*tx_q->desc_ring) * (tx_q->entries + 1);
+	memset(tx_q->desc_ring, 0, ring_size);
+
+	/* Build TX ring buffer */
+	for (i = 0, tx_desc = tx_q->desc_ring; i < tx_q->entries; i++, tx_desc++) {
+		tx_desc->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_EEMPTY);
+	}
+	/* Close the loop */
+	tx_desc->dptrl = cpu_to_le32((u32)tx_q->desc_dma);
+	tx_desc->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_LINKFIX); /* type */
+
+	bat_entry = rsw2->bat_addr[q + tx_q->offset];
+	bat_entry->die_dt = FIELD_PREP(RSW2_DESC_DT, DT_LINKFIX); /* type */
+	bat_entry->dptrl = cpu_to_le32((u32)tx_q->desc_dma);
+
+	reg_val = FIELD_PREP(GWDCC_BALR, 1); /* Base address load request */
+	reg_val |= FIELD_PREP(GWDCC_DCP, 1); /* Chain priority */
+	reg_val |= FIELD_PREP(GWDCC_DQT, 1); /* Transmission queue */
+	reg_val |= FIELD_PREP(GWDCC_EDE, 1); /* Extended Descriptor Enable */
+
+	iowrite32(reg_val, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWDCC(q + tx_q->offset));
+
+	return 0;
+}
+
+
+
+static u32 rswitch2_get_msglevel(struct net_device *ndev)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	return rsw2->msg_enable;
+}
+
+static void rswitch2_set_msglevel(struct net_device *ndev, u32 value)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	rsw2->msg_enable = value;
+}
+
+static const char rswitch2_gstrings_stats[][ETH_GSTRING_LEN] = {
+	"rx_queue_0_current",
+	"tx_queue_0_current",
+	"rx_queue_0_dirty",
+	"tx_queue_0_dirty",
+	"rx_queue_0_packets",
+	"tx_queue_0_packets",
+	"rx_queue_0_bytes",
+	"tx_queue_0_bytes",
+	"rx_queue_0_mcast_packets",
+	"rx_queue_0_errors",
+	"rx_queue_0_crc_errors",
+	"rx_queue_0_frame_errors",
+	"rx_queue_0_length_errors",
+	"rx_queue_0_missed_errors",
+	"rx_queue_0_over_errors",
+
+	"rx_queue_1_current",
+	"tx_queue_1_current",
+	"rx_queue_1_dirty",
+	"tx_queue_1_dirty",
+	"rx_queue_1_packets",
+	"tx_queue_1_packets",
+	"rx_queue_1_bytes",
+	"tx_queue_1_bytes",
+	"rx_queue_1_mcast_packets",
+	"rx_queue_1_errors",
+	"rx_queue_1_crc_errors",
+	"rx_queue_1_frame_errors",
+	"rx_queue_1_length_errors",
+	"rx_queue_1_missed_errors",
+	"rx_queue_1_over_errors",
+};
+
+static void rswitch2_get_ethtool_stats(struct net_device *ndev,
+				   struct ethtool_stats *estats, u64 *data)
+{
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_internal_port *intern_port;
+	int i = 0;
+	int q;
+
+	eth_port =  netdev_priv(ndev);
+	intern_port = eth_port->intern_port;
+
+	/* Port specific stats */
+	for (q = 0; q < NUM_BE_RX_QUEUES; q++) {
+		struct net_device_stats *stats = &eth_port->stats[q];
+
+		if (intern_port) {
+			data[i++] = intern_port->rx_q[q].cur_desc;
+			data[i++] = intern_port->tx_q[q].cur_desc;
+			data[i++] = intern_port->rx_q[q].dirty_desc;
+			data[i++] = intern_port->tx_q[q].dirty_desc;
+		}
+		data[i++] = stats->rx_packets;
+		data[i++] = stats->tx_packets;
+		data[i++] = stats->rx_bytes;
+		data[i++] = stats->tx_bytes;
+		data[i++] = stats->multicast;
+		data[i++] = stats->rx_errors;
+		data[i++] = stats->rx_crc_errors;
+		data[i++] = stats->rx_frame_errors;
+		data[i++] = stats->rx_length_errors;
+		data[i++] = stats->rx_missed_errors;
+		data[i++] = stats->rx_over_errors;
+	}
+}
+
+#define RSWITCH2_STATS_LEN	ARRAY_SIZE(rswitch2_gstrings_stats)
+
+static int rswicth2_get_sset_count(struct net_device *netdev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS:
+		return RSWITCH2_STATS_LEN;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static void rswitch2_get_strings(struct net_device *ndev, u32 stringset, u8 *data)
+{
+	switch (stringset) {
+	case ETH_SS_STATS:
+		memcpy(data, rswitch2_gstrings_stats, sizeof(rswitch2_gstrings_stats));
+		break;
+	}
+}
+
+/* FIXE: Distinguish queues here? */
+static void rswitch2_get_ringparam(struct net_device *ndev,
+			       struct ethtool_ringparam *ring)
+{
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_internal_port *intern_port;
+
+	eth_port = netdev_priv(ndev);
+	intern_port = eth_port->intern_port;
+	BUG_ON(intern_port == NULL);
+
+	ring->rx_max_pending = RSW2_BE_RX_RING_SIZE;
+	ring->tx_max_pending = RSW2_BE_TX_RING_SIZE;
+	ring->rx_pending = intern_port->rx_q[0].entries;
+	ring->tx_pending = intern_port->tx_q[0].entries;
+}
+
+static int rswitch2_set_ringparam(struct net_device *ndev,
+			      struct ethtool_ringparam *ring)
+{
+	/* TODO */
+	return 0;
+}
+
+static int rswitch2_get_ts_info(struct net_device *ndev, struct ethtool_ts_info *info)
+{
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_drv *rsw2;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	if(eth_port->intern_port) {
+		info->so_timestamping = SOF_TIMESTAMPING_RX_SOFTWARE |
+					SOF_TIMESTAMPING_SOFTWARE;
+		info->phc_index = -1;
+
+		return 0;
+	}
+
+	info->phc_index = ptp_clock_index(rsw2->ptp_drv->clock);
+	info->so_timestamping = SOF_TIMESTAMPING_TX_SOFTWARE |
+				SOF_TIMESTAMPING_RX_SOFTWARE |
+				SOF_TIMESTAMPING_SOFTWARE |
+				SOF_TIMESTAMPING_TX_HARDWARE |
+				SOF_TIMESTAMPING_RX_HARDWARE |
+				SOF_TIMESTAMPING_RAW_HARDWARE;
+	info->tx_types = BIT(HWTSTAMP_TX_OFF) | BIT(HWTSTAMP_TX_ON);
+	info->rx_filters = BIT(HWTSTAMP_FILTER_NONE) | BIT(HWTSTAMP_FILTER_ALL);
+
+	return 0;
+}
+
+static int rswitch2_phy_ethtool_set_link_ksettings(struct net_device *ndev,
+				   const struct ethtool_link_ksettings *cmd)
+{
+	struct phy_device *phydev = ndev->phydev;
+
+	__ETHTOOL_DECLARE_LINK_MODE_MASK(advertising);
+	u8 autoneg = cmd->base.autoneg;
+	u8 duplex = cmd->base.duplex;
+	u32 speed = cmd->base.speed;
+
+	if (!phydev)
+		return -ENODEV;
+
+
+	if (cmd->base.phy_address != phydev->mdio.addr)
+		return -EINVAL;
+
+	linkmode_copy(advertising, cmd->link_modes.advertising);
+
+	/* We make sure that we don't pass unsupported values in to the PHY */
+	linkmode_and(advertising, advertising, phydev->supported);
+
+	/* Verify the settings we care about. */
+	if (autoneg != AUTONEG_ENABLE && autoneg != AUTONEG_DISABLE)
+		return -EINVAL;
+
+	if (autoneg == AUTONEG_ENABLE && linkmode_empty(advertising))
+		return -EINVAL;
+
+	if (autoneg == AUTONEG_DISABLE &&
+	    (
+	     (duplex != DUPLEX_HALF &&
+	      duplex != DUPLEX_FULL)))
+		return -EINVAL;
+
+	phydev->autoneg = autoneg;
+
+	if (autoneg == AUTONEG_DISABLE) {
+		phydev->speed = speed;
+		phydev->duplex = duplex;
+	}
+
+	linkmode_copy(phydev->advertising, advertising);
+
+	linkmode_mod_bit(ETHTOOL_LINK_MODE_Autoneg_BIT,
+			 phydev->advertising, autoneg == AUTONEG_ENABLE);
+
+	phydev->master_slave_set = cmd->base.master_slave_cfg;
+	phydev->mdix_ctrl = cmd->base.eth_tp_mdix_ctrl;
+
+	/* Restart the PHY */
+	phy_start_aneg(phydev);
+
+	return 0;
+}
+
+
+static const struct ethtool_ops rswitch2_ethtool_ops = {
+	.nway_reset			= phy_ethtool_nway_reset,
+	.get_msglevel		= rswitch2_get_msglevel,
+	.set_msglevel		= rswitch2_set_msglevel,
+	.get_link			= ethtool_op_get_link,
+	.get_strings		= rswitch2_get_strings,
+	.get_ethtool_stats	= rswitch2_get_ethtool_stats,
+	.get_sset_count		= rswicth2_get_sset_count,
+	.get_ringparam		= rswitch2_get_ringparam,
+	.set_ringparam		= rswitch2_set_ringparam,
+	.get_ts_info		= rswitch2_get_ts_info,
+	.get_link_ksettings	= phy_ethtool_get_link_ksettings,
+	.set_link_ksettings	= rswitch2_phy_ethtool_set_link_ksettings,
+	.get_wol			= NULL,
+	.set_wol			= NULL,
+};
+
+
+static void rswitch2_init_port_mac(struct net_device *port_ndev)
+{
+	u32 reg_val;
+	int speed;
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+	phy_interface_t phy_iface;
+
+	eth_port = netdev_priv(port_ndev);
+	phy_port = eth_port->phy_port;
+	rsw2 = eth_port->rsw2;
+	BUG_ON(phy_port == NULL);
+
+	/* 1 Cycle extra hold time */
+	reg_val = FIELD_PREP(MPIC_PSMHT, 0x6);
+
+	/* Set clock divider */
+	reg_val |= FIELD_PREP(MPIC_PSMCS, 0x40);
+
+	/* speed for MAC will later updated when link comes up */
+	//phy_iface = phy_port->phy_iface;  //not yet initialised here
+	//phy_iface = eth_port->rsw2->port_data[eth_port->port_num-1].phy_iface;
+	phy_iface = phy_port->phy_iface;
+	switch (phy_iface) {
+		case PHY_INTERFACE_MODE_SGMII :
+			//speed may chage during link up. MII is fixed
+			speed = 1000;
+			reg_val |= FIELD_PREP(MPIC_LSC, rsw2_rmac_1000mbps);
+			reg_val |= FIELD_PREP(MPIC_PIS, rsw2_rmac_gmii);
+			break;
+		case PHY_INTERFACE_MODE_USXGMII :
+			//speed may chage during link up. MII is fixed
+			speed = 2500;
+			reg_val |= FIELD_PREP(MPIC_LSC, rsw2_rmac_2500mbps);
+			reg_val |= FIELD_PREP(MPIC_PIS, rsw2_rmac_xgmii);
+			break;
+		default:
+			rsw2_err(MSG_GEN, "Unsupported MAC xMII format %s (%d) on port %d\n",phy_modes(phy_iface), phy_iface, eth_port->port_num-1);
+			//return -EINVAL;
+	}
+
+	iowrite32(reg_val, phy_port->rmac_base_addr + RSW2_RMAC_MPIC);
+
+	//TODO: This is part of the MDIO access function, pre-conf has no effect
+	reg_val = ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MPSM);
+	reg_val |= MPSM_MFF;
+	iowrite32(reg_val, phy_port->rmac_base_addr + RSW2_RMAC_MPSM);
+
+
+	/* Enable broad-/multi-/uni-cast reception of eMAC and pMAC frames*/
+	reg_val = MRAFC_BCENE;
+	reg_val |= MRAFC_MCENE;
+	reg_val |= MRAFC_UCENE;
+	reg_val |= MRAFC_BCENP;
+	reg_val |= MRAFC_MCENP;
+	reg_val |= MRAFC_UCENP;
+	iowrite32(reg_val, phy_port->rmac_base_addr + RSW2_RMAC_MRAFC);
+}
+
+
+
+
+static void rswitch2_init_mac_addr(struct net_device *ndev)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct device_node *port_node;
+//	struct device_node *dev_node;
+	u8 *macaddr;
+	int ret;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+
+	port_node = rswitch2_get_port_node(rsw2, (eth_port->port_num - rsw2->num_of_cpu_ports));
+//	dev_node = ndev->dev.of_node;
+	ndev->dev.of_node = port_node;
+
+	if(eth_port->intern_port) {
+		ret = eth_platform_get_mac_address(rsw2->dev, ndev->dev_addr);
+		if ((ret == 0) && is_valid_ether_addr(ndev->dev_addr)) {
+			/* device tree or NVMEM values are valid so use them */
+			rsw2_info(MSG_GEN, "MAC: '%s' Got valid MAC from eth_platform_get_mac_address()\n", ndev->name);
+
+		}
+		else {
+			rsw2_info(MSG_GEN, "MAC: '%s' INVALID from eth_platform_get_mac_address()\n", ndev->name);
+			eth_hw_addr_random(ndev);
+		}
+	}
+	else {
+		/* Although this calls of_get_mac_addr_nvmem() internally
+		 * it can't succeed because it expects dev.of_node to belong to
+		 * a platform device, which is not possible for the 'port' sub-node
+		 */
+		ret = eth_platform_get_mac_address(&ndev->dev, ndev->dev_addr);
+		if((ret == 0) && is_valid_ether_addr(ndev->dev_addr)) {
+			rsw2_info(MSG_GEN, "MAC: '%s' VALID MAC from eth_platform_get_mac_address()\n", ndev->name);
+		}
+		else if((nvmem_get_mac_address(&ndev->dev, ndev->dev_addr) == 0) &&
+				is_valid_ether_addr(ndev->dev_addr)) {
+			rsw2_info(MSG_GEN, "MAC: '%s' VALID from nvmem_get_mac_address()\n", ndev->name);
+		}
+		else {
+			rsw2_info(MSG_GEN,"MAC: '%s' using random MAC\n", ndev->name);
+			eth_hw_addr_random(ndev);
+		}
+	}
+	macaddr = (u8 *)ndev->dev_addr;
+	rsw2_notice(MSG_GEN,"Assigned MAC %.2X:%.2X:%.2X:%.2X:%.2X:%.2X to '%s'\n",
+			macaddr[0], macaddr[1],
+			macaddr[2], macaddr[3],
+			macaddr[4], macaddr[5],
+			ndev->name);
+
+//	ndev->dev.of_node = dev_node;
+
+}
+
+
+
+
+// FIXME: don't re-invent base functions
+static void rswitch_modify(void __iomem *addr, u32 reg, u32 clear, u32 set)
+{
+	iowrite32((ioread32(addr + reg) & ~clear) | set, addr + reg);
+}
+// FIXME
+#define MMIS1_CLEAR_FLAGS       0xf
+
+#define MPSM_PRD_SHIFT		16
+#define MPSM_PRD_MASK		GENMASK(31, MPSM_PRD_SHIFT)
+
+#define MDIO_READ_C45		0x03
+#define MDIO_WRITE_C45		0x01
+
+static int rswitch2_mdio_access(struct net_device *ndev, bool read,
+				   int phyad, int devad, int regad, int data)
+{
+	int pop = read ? MDIO_READ_C45 : MDIO_WRITE_C45;
+	u32 reg_val;
+	int ret;
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_physical_port *phy_port;
+	struct rswitch2_eth_port *eth_port;
+
+	eth_port = netdev_priv(ndev);
+	phy_port = eth_port->phy_port;
+	rsw2 = eth_port->rsw2;
+
+	/* No match device */
+	if (devad == 0xffffffff)
+		return 0;
+
+	/* Clear completion flags */
+	writel(MMIS1_CLEAR_FLAGS, phy_port->rmac_base_addr + RSW2_RMAC_MMIS1);
+
+	/* Submit address to PHY (MDIO_ADDR_C45 << 13) */
+	reg_val = MPSM_PSME | MPSM_MFF;
+	iowrite32((regad << 16) | (devad << 8) | (phyad << 3) | reg_val, phy_port->rmac_base_addr + RSW2_RMAC_MPSM);
+
+	ret = readl_poll_timeout(phy_port->rmac_base_addr + RSW2_RMAC_MMIS1, reg_val,
+						reg_val & MMIS1_PAACS,
+						RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		rsw2_err(MSG_GEN, "RMAC setting MDIO address timed out\n");
+		return ret;
+	}
+
+	/* Clear address completion flag */
+	rswitch_modify(phy_port->rmac_base_addr, RSW2_RMAC_MMIS1, MMIS1_PAACS, MMIS1_PAACS);
+
+	/* Read/Write PHY register */
+	if (read) {
+		// FIXME: Why writel and iowrite32
+		reg_val = MPSM_PSME | MPSM_MFF;
+		writel((pop << 13) | (devad << 8) | (phyad << 3) | reg_val, phy_port->rmac_base_addr + RSW2_RMAC_MPSM);
+
+		ret = readl_poll_timeout(phy_port->rmac_base_addr + RSW2_RMAC_MMIS1, reg_val,
+							reg_val & MMIS1_PRACS,
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+		if (ret != 0) {
+			rsw2_err(MSG_GEN, "RMAC read MDIO timed out\n");
+			return ret;
+		}
+		/* Read data */
+		ret = (ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MPSM) & MPSM_PRD_MASK) >> 16;
+		//printk("RSW2 MDIO read: phyad: %d, devad: %d, regad: %04x, ret:%04x \n", phyad, devad, regad, ret);
+
+		/* Clear read completion flag */
+		rswitch_modify(phy_port->rmac_base_addr, RSW2_RMAC_MMIS1, MMIS1_PRACS, MMIS1_PRACS);
+
+
+
+	} else {
+		reg_val = MPSM_PSME | MPSM_MFF;
+		iowrite32((data << 16) | (pop << 13) | (devad << 8) | (phyad << 3) | reg_val,
+				phy_port->rmac_base_addr + RSW2_RMAC_MPSM);
+
+		rsw2_dbg(MSG_GEN, "RSW2 MDIO write: phyad: %d, devad: %d, regad: %04x, data:%04x \n", phyad, devad, regad, data);
+		ret = readl_poll_timeout(phy_port->rmac_base_addr + RSW2_RMAC_MMIS1, reg_val,
+							reg_val & MMIS1_PWACS,
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+		if (ret != 0) {
+			netdev_err(ndev, "RMAC write MDIO timed out\n");
+			return ret;
+		}
+		/* Clear write completion flag */
+		rswitch_modify(phy_port->rmac_base_addr, RSW2_RMAC_MMIS1, MMIS1_PWACS, MMIS1_PWACS);
+	}
+
+	return ret;
+}
+
+static int rswitch2_mdio_read(struct mii_bus *bus, int addr, int regnum)
+{
+	struct net_device *ndev = bus->priv;
+	int mode, devad, regad;
+
+	mode = regnum & MII_ADDR_C45;
+	devad = (regnum >> MII_DEVADDR_C45_SHIFT) & 0x1f;
+	regad = regnum & MII_REGADDR_C45_MASK;
+
+	/* Not support Clause 22 access method */
+	if (!mode) {
+		// FIXME
+		//printk("NOT C45 regnum=%04x  return 0\n", regnum);
+
+		return 0;
+	}
+	//printk("rswitch2_mdio_read(): '%s' addr: 0x%.4x  devad: 0x%.4x  regad: 0x%.4x", ndev->name, addr, devad, regad);
+	return rswitch2_mdio_access(ndev, true, addr, devad, regad, 0);
+}
+
+static int rswitch2_mdio_write(struct mii_bus *bus, int addr, int regnum, u16 val)
+{
+	struct net_device *ndev = bus->priv;
+	int mode, devad, regad;
+
+	mode = regnum & MII_ADDR_C45;
+	devad = (regnum >> MII_DEVADDR_C45_SHIFT) & 0x1f;
+	regad = regnum & MII_REGADDR_C45_MASK;
+
+	/* Not support Clause 22 access method */
+	if (!mode) {
+		// FIXME
+		//printk("NOT C45 regnum=%04x  return 0\n", regnum);
+
+		return 0;
+	}
+	//printk("rswitch2_mdio_write(): '%s' addr: 0x%.4x  devad: 0x%.4x  regad: 0x%.4x  val: 0x%.4x", ndev->name, addr, devad, regad, val);
+	return rswitch2_mdio_access(ndev, false, addr, devad, regad, val);
+}
+
+static int rswitch2_mdio_reset(struct mii_bus *bus)
+{
+	/* TODO */
+	return 0;
+}
+
+static int rswitch2_mdio_init(struct net_device *ndev, unsigned int port_num)
+{
+	struct mii_bus *mii_bus;
+	struct device_node *dn_port;
+	struct device_node *dn_phy;
+	int phy_addr;
+	int ret;
+
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+	phy_port = eth_port->phy_port;
+
+	/* Only physical ports have MDIO interface */
+	BUG_ON(phy_port == NULL);
+
+	mii_bus = mdiobus_alloc();
+	if (!mii_bus)
+		return -ENOMEM;
+
+	mii_bus->name = "rswitch_mii";
+	sprintf(mii_bus->id, ndev->name, port_num);
+	mii_bus->priv = ndev;
+	mii_bus->read = rswitch2_mdio_read;
+	mii_bus->write = rswitch2_mdio_write;
+	mii_bus->reset = rswitch2_mdio_reset;
+	mii_bus->parent = rsw2->dev;
+	mii_bus->owner = THIS_MODULE;
+
+	dn_port = rswitch2_get_port_node(rsw2, port_num);
+	if (!dn_port)
+		return -ENODEV;
+
+	dn_phy = of_parse_phandle(dn_port, "phy-handle", 0);
+	if (dn_phy) {
+		ret = of_mdiobus_register(mii_bus, dn_port);
+		if (ret < 0) {
+			rsw2_err(MSG_GEN, "Failed to register MDIO bus for Port %d\n", port_num);
+			of_node_put(dn_phy);
+			goto mdio_bus_free;
+		}
+
+		ret = of_mdio_parse_addr(&mii_bus->dev, dn_phy);
+		of_node_put(dn_phy);
+		if (ret < 0) {
+			rsw2_err(MSG_GEN, "Failed to get PHY address for Port %d\n", port_num);
+			goto mdio_unreg;
+		}
+		phy_addr = ret;
+		dev_info(rsw2->dev, "MDIO: Phy is at addr %d\n", phy_addr);
+
+		phy_port->phy = to_phy_device(&mii_bus->mdio_map[phy_addr]->dev);
+		if (!phy_port->phy) {
+			rsw2_err(MSG_GEN, "PHY not found for Port %d\n", port_num);
+			ret = -ENODEV;
+			goto mdio_unreg;
+		}
+	} else if (of_phy_is_fixed_link(dn_port)) {
+		ret = mdiobus_register(mii_bus);
+		if (ret < 0) {
+			rsw2_err(MSG_GEN, "Failed to register MDIO bus for fixed PHY for Port %d\n", port_num);
+			goto mdio_bus_free;
+		}
+
+		ret = of_phy_register_fixed_link(dn_port);
+		if (ret < 0) {
+			rsw2_err(MSG_GEN, "Failed to register fixed link PHY for Port %d\n", port_num);
+			goto mdio_unreg;
+		}
+
+		phy_port->phy = of_phy_find_device(dn_port);
+		if (!phy_port->phy) {
+			rsw2_err(MSG_GEN, "Failed to get fixed link PHY for Port %d\n", port_num);
+			ret = -ENODEV;
+			goto mdio_unreg;
+		}
+	} else {
+		rsw2_err(MSG_GEN, "No PHY defined for Port %d\n", port_num);
+		ret = -EINVAL;
+		goto dn_port_put;
+	}
+
+	phy_port->phy->interface = phy_port->phy_iface;
+	phy_port->mii_bus = mii_bus;
+
+	of_node_put(dn_port);
+
+	return 0;
+
+
+mdio_unreg:
+	mdiobus_unregister(mii_bus);
+mdio_bus_free:
+	mdiobus_free(mii_bus);
+dn_port_put:
+	of_node_put(dn_port);
+
+	return ret;
+}
+
+static int rswitch2_get_phy_config(struct net_device *ndev)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+	struct device_node *port_node;
+	int ret;
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+	phy_port = eth_port->phy_port;
+
+	port_node = rswitch2_get_port_node(rsw2, (eth_port->port_num - rsw2->num_of_cpu_ports));
+	if(port_node) {
+		ret = of_get_phy_mode(port_node, &phy_port->phy_iface);
+		if (ret != 0) {
+			rsw2_err(MSG_GEN, "of_get_phy_mode failed\n");
+			return -ENODEV;
+		}
+		else {
+			rsw2_info(MSG_GEN, "Got phy_iface mode: %s (%d)\n", phy_modes(phy_port->phy_iface), phy_port->phy_iface);
+		}
+	}
+	of_node_put(port_node);
+
+	return 0;
+}
+
+static int rswitch2_init_physical_port(struct rswitch2_drv *rsw2, unsigned int port_num)
+{
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_physical_port *phy_port;
+	struct net_device *port_ndev;
+	int ret;
+	char port_name[3];
+	int port_name_len;
+	size_t num_of_rx_queues;
+	size_t num_of_tx_queues;
+	uint q;
+
+	port_ndev = alloc_etherdev_mqs(sizeof(struct rswitch2_eth_port), 1, 1);
+	if (!port_ndev)
+		return -ENOMEM;
+
+	phy_port = kzalloc(sizeof(struct rswitch2_physical_port), GFP_KERNEL);
+	if (phy_port < 0) {
+		rsw2_err(MSG_GEN,  "Failed to allocate physical port\n");
+		ret = -ENOMEM;
+		goto phy_port_err;
+	}
+	SET_NETDEV_DEV(port_ndev, rsw2->dev);
+
+	ether_setup(port_ndev);
+
+	eth_port = netdev_priv(port_ndev);
+	eth_port->ndev = port_ndev;
+	eth_port->rsw2 = rsw2;
+	eth_port->port_num = (port_num + rsw2->num_of_cpu_ports);
+	eth_port->phy_port = phy_port;
+
+	port_ndev->base_addr = (unsigned long)rsw2->etha_base_addrs[port_num];
+	rsw2_dbg(MSG_GEN, "Phy port %d: rsw2->etha_base_addrs[%d] = 0x%px\n", port_num, port_num, rsw2->etha_base_addrs[port_num]);
+
+
+	/* FIXME */
+	port_ndev->irq = rsw2->rxtx_irqs[port_num];
+
+	phy_port->rmac_base_addr = rsw2->etha_base_addrs[port_num] + RSW2_RMAC_OFFSET;
+	phy_port->serdes_chan_addr = rsw2->serdes_base_addr
+			+ (RSW2_SERDES_CHANNEL_OFFSET * port_num);
+
+
+	strncpy(port_ndev->name, RSW2_NETDEV_BASENAME, sizeof(port_ndev->name) - sizeof(port_name) - 1);
+	port_name_len = snprintf(port_name, sizeof(port_name), "p%1u", eth_port->port_num-1);
+	strncat(port_ndev->name, port_name, port_name_len);
+
+	ret = dev_alloc_name(port_ndev, port_ndev->name);
+	if (ret < 0) {
+		rsw2_err(MSG_GEN, "Failed to alloc dev_name()\n");
+		goto phy_port_err;
+	}
+	dev_set_name(&port_ndev->dev, port_ndev->name);
+
+	ret = rswitch2_emac_set_state(port_ndev, emac_disable);
+	if (ret < 0) {
+		rsw2_err(MSG_GEN, "Port set state 'disable' failed\n");
+		goto cleanup_phy_port;
+	}
+
+	ret = rswitch2_emac_set_state(port_ndev, emac_reset);
+	if (ret < 0) {
+		rsw2_err(MSG_GEN, "Port set state 'reset' failed\n");
+		goto cleanup_phy_port;
+	}
+
+	ret = rswitch2_emac_set_state(port_ndev, emac_disable);
+	if (ret < 0) {
+		rsw2_err(MSG_GEN, "Port set state 'disable' failed\n");
+		goto cleanup_phy_port;
+	}
+
+	ret = rswitch2_emac_set_state(port_ndev, emac_config);
+	if (ret < 0) {
+		rsw2_err(MSG_GEN, "Port set state 'config' failed\n");
+		goto cleanup_phy_port;
+	}
+
+	/* FIXME: Error handling */
+	rswitch2_get_phy_config(port_ndev);
+
+	/* MAC related setup */
+	rswitch2_init_mac_addr(port_ndev);
+	rswitch2_port_set_mac_addr(port_ndev);
+	rswitch2_init_port_mac(port_ndev);
+
+	/* Change to OPERATION Mode */
+	ret = rswitch2_emac_set_state(port_ndev, emac_operation);
+	if (ret < 0) {
+		netdev_err(port_ndev, "Port set state 'operation' failed\n");
+		goto cleanup_phy_port;
+	}
+
+
+	/* Link Verification */
+	/* TODO CMARD: I'm not sure if this is the correct position. The link verification
+	 * is a feature to ask the far end MAC if it is pre-emption capable. But this can
+	 * only happen when the physical link is established.
+	 */
+	/*{
+		u32 reg_val;
+		/ * Request Link Verification * /
+		reg_val = ioread32(phy_port->rmac_base_addr + RSW2_RMAC_MPSM);
+			reg_val |= MPSM_MFF;
+			iowrite32(reg_val, phy_port->rmac_base_addr + RSW2_RMAC_MPSM);
+
+
+		iowrite32(MLVC_PLV, phy_port->rmac_base_addr + RSW2_RMAC_MLVC);
+		ret = readl_poll_timeout(phy_port->rmac_base_addr + RSW2_RMAC_MLVC, reg_val,
+							reg_val & MLVC_PLV,
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+		if (ret != 0) {
+			dev_err(rsw2->dev, "RMAC Preemption link verification timed out\n");
+			return ret;
+		} else {
+			dev_info(rsw2->dev, "RMAC Preemption link verified!\n");
+		}
+	}*/
+
+	port_ndev->min_mtu = 2048 - (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN);
+	port_ndev->max_mtu = ETH_MIN_MTU;
+	port_ndev->netdev_ops = &rswitch2_netdev_ops;
+	port_ndev->ethtool_ops = &rswitch2_ethtool_ops;
+
+	ret = rswitch2_mdio_init(port_ndev, port_num);
+	if (ret != 0) {
+		rsw2_err(MSG_GEN, "rswitch2_mdio_init failed: %d\n", ret);
+		goto cleanup_phy_port;
+	}
+
+	rswitch2_port_set_mac_addr(port_ndev);
+
+
+#if 0
+	ret = rswitch2_phy_init(port_ndev, port_num);
+	if (ret != 0) {
+		dev_err(rsw2->dev, "rswitch2_phy_init failed: %d\n", ret);
+		goto cleanup_phy_port;
+	}
+
+	ret = rswitch2_serdes_init(port_ndev, port_num);
+	if (ret != 0) {
+		dev_err(rsw2->dev, "rswitch2_serdes_init failed: %d\n", ret);
+		//FIXME: clean up error handling
+		goto cleanup_phy_port;
+	}
+	else {
+		dev_err(rsw2->dev, "rswitch2_serdes_init SUCCESS\n");
+
+	}
+#endif
+
+	num_of_rx_queues = ARRAY_SIZE(phy_port->rx_q);
+	num_of_tx_queues = ARRAY_SIZE(phy_port->tx_q);
+
+	/* Set ring layout */
+	for (q = 0; q < num_of_tx_queues; q++) {
+		phy_port->tx_q[q].entries = RSW2_LL_TX_RING_SIZE;
+	}
+	for (q = 0; q < num_of_rx_queues; q++) {
+		phy_port->rx_q[q].entries = RSW2_LL_RX_RING_SIZE;
+	}
+
+	netif_napi_add(port_ndev, &phy_port->rx_q[0].napi, rswitch2_poll, 64);
+
+	ret = phy_connect_direct(port_ndev, phy_port->phy, &rswitch2_phy_state_change, phy_port->phy_iface);
+	if (ret != 0) {
+		rsw2_err(MSG_GEN, "%s: phy_connect_direct() failed: %d\n", port_ndev->name, ret);
+		return ret;
+	} else {
+		rsw2_notice(MSG_GEN, "phy_connect_direct(): attached '%s' PHY driver: %d\n", port_ndev->phydev->drv->name , ret);
+	}
+
+	rsw2->ports[port_num + rsw2->num_of_cpu_ports] = eth_port;
+
+	return 0;
+
+cleanup_phy_port:
+	kfree(phy_port);
+phy_port_err:
+	free_netdev(port_ndev);
+
+	return ret;
+}
+
+static int rswitch2_init_internal_port(struct rswitch2_drv *rsw2, unsigned int port_num)
+{
+	int ret;
+	struct net_device *ndev = NULL;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_internal_port *intern_port;
+	int q;
+
+	if (port_num > rsw2->num_of_cpu_ports)
+		return -EINVAL;
+
+	ndev = alloc_etherdev_mqs(sizeof(struct rswitch2_eth_port),
+			NUM_BE_TX_QUEUES, NUM_BE_RX_QUEUES);
+	if (!ndev)
+		return -ENOMEM;
+
+	eth_port = netdev_priv(ndev);
+	memset(eth_port, 0, sizeof(*eth_port));
+
+	ndev->base_addr = (unsigned long)rsw2->gwca_base_addrs[port_num];
+
+	rsw2_info(MSG_GEN, "Assigning irq %d to internal port %d\n", rsw2->rxtx_irqs[port_num], port_num);
+
+	ndev->irq = rsw2->rxtx_irqs[port_num];
+	strncpy(ndev->name, RSW2_NETDEV_BASENAME, sizeof(ndev->name) - 1);
+
+	ret = dev_alloc_name(ndev, ndev->name);
+	if (ret < 0) {
+		rsw2_err(MSG_GEN, "Failed to alloc dev_name()\n");
+		goto intern_port_err;
+	}
+
+	dev_set_name(&ndev->dev, ndev->name);
+
+	SET_NETDEV_DEV(ndev, rsw2->dev);
+	ether_setup(ndev);
+
+	intern_port = kzalloc(sizeof(struct rswitch2_internal_port), GFP_KERNEL);
+	if (intern_port < 0) {
+		rsw2_err(MSG_GEN, "Failed to allocate internal port\n");
+		ret = -ENOMEM;
+		goto intern_port_err;
+	}
+	memset(intern_port, 0, sizeof(*intern_port));
+
+	/* Initialize private data */
+	eth_port->ndev = ndev;
+	eth_port->rsw2 = rsw2;
+	eth_port->port_num = port_num;
+	eth_port->intern_port = intern_port;
+	eth_port->phy_port = NULL;
+
+	/* Get MAC address and set it in HW */
+	rsw2_dbg(MSG_GEN, "Start init the MAC\n");
+
+	rswitch2_init_mac_addr(ndev);
+	rswitch2_intern_set_mac_addr(ndev);
+
+	/* Set ring layout */
+	for (q = 0; q < NUM_BE_TX_QUEUES; q++) {
+		intern_port->tx_q[q].entries = RSW2_BE_TX_RING_SIZE;
+	}
+	for (q = 0; q < NUM_BE_RX_QUEUES; q++) {
+		intern_port->rx_q[q].entries = RSW2_BE_RX_RING_SIZE;
+	}
+
+	//spin_lock_init(&eth_port->lock);
+
+	ndev->netdev_ops = &rswitch2_netdev_ops;
+
+	ndev->ethtool_ops = &rswitch2_ethtool_ops;
+	ndev->max_mtu = 2048 - (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN);
+	ndev->min_mtu = ETH_MIN_MTU;
+
+	for (q = 0; q < NUM_BE_RX_QUEUES; q++)
+		netif_napi_add(ndev, &intern_port->rx_q[q].napi, rswitch2_poll, 64);
+
+	rsw2->ports[port_num] = eth_port;
+
+	return 0;
+
+intern_port_err:
+	free_netdev(ndev);
+
+	return ret;
+}
+
+static void rswitch2_free_rings(struct net_device *ndev)
+{
+	struct rswitch2_drv *rsw2;
+	struct rswitch2_eth_port *eth_port;
+	struct rswitch2_internal_port *intern_port;
+	struct rswitch2_physical_port *phy_port;
+	uint num_of_rx_queues;
+	uint num_of_tx_queues;
+	uint q;
+#ifdef RSW2_RX_TS_DESC
+	struct rswitch2_dma_ext_ts_desc *rx_desc;
+#else
+	struct rswitch2_dma_ext_desc *rx_desc;
+#endif
+
+
+	eth_port = netdev_priv(ndev);
+	rsw2 = eth_port->rsw2;
+
+	intern_port = eth_port->intern_port;
+	phy_port = eth_port->phy_port;
+
+	if(intern_port) {
+		num_of_rx_queues = ARRAY_SIZE(intern_port->rx_q);
+		num_of_tx_queues = ARRAY_SIZE(intern_port->tx_q);
+	} else {
+		num_of_rx_queues = ARRAY_SIZE(phy_port->rx_q);
+		num_of_tx_queues = ARRAY_SIZE(phy_port->tx_q);
+	}
+
+
+	for (q = 0; q < num_of_rx_queues; q++) {
+		uint q_entry;
+		size_t ring_size;
+		struct rsw2_rx_q_data *rx_q;
+
+		(void)rswitch2_netdev_get_rx_q(ndev, q, &rx_q);
+
+		ring_size = sizeof(*rx_q->desc_ring) * (rx_q->entries + 1);
+
+		for(q_entry = 0; q_entry < rx_q->entries; q_entry++) {
+			if(rx_q->skb[q_entry] != NULL) {
+				rx_desc = &rx_q->desc_ring[q_entry];
+				dma_unmap_single(ndev->dev.parent, le32_to_cpu(rx_desc->dptrl),
+									RSW2_PKT_BUF_SZ, DMA_FROM_DEVICE);
+				dev_kfree_skb_any(rx_q->skb[q_entry]);
+			}
+		}
+
+
+		if(rx_q->desc_ring) {
+			dma_free_coherent(rsw2->dev, ring_size, rx_q->desc_ring, rx_q->desc_dma);
+			rx_q->desc_ring = NULL;
+			rx_q->desc_dma = 0;
+		}
+	}
+
+
+	for (q = 0; q < num_of_tx_queues; q++) {
+		size_t ring_size;
+		struct rsw2_tx_q_data *tx_q;
+
+		(void)rswitch2_netdev_get_tx_q(ndev, q, &tx_q);
+
+		ring_size = sizeof(*tx_q->desc_ring) * (tx_q->entries + 1);
+
+		if(tx_q->desc_ring) {
+			dma_free_coherent(rsw2->dev, ring_size, tx_q->desc_ring, tx_q->desc_dma);
+			tx_q->desc_ring = NULL;
+			tx_q->desc_dma = 0;
+		}
+	}
+}
+static void rswitch2_free_bat(struct rswitch2_drv *rsw2)
+{
+	size_t bat_size;
+
+	bat_size = sizeof(struct rswitch2_dma_desc *) * NUM_ALL_QUEUES;
+
+	if (rsw2->bat_dma_addr)
+		dma_free_coherent(rsw2->dev, bat_size, rsw2->bat_addr[0],
+						rsw2->bat_dma_addr);
+
+	kfree(rsw2->bat_addr);
+}
+
+
+static void rswitch2_disable_ports(struct rswitch2_drv *rsw2)
+{
+
+	uint cur_port;
+	uint total_ports = rsw2->num_of_cpu_ports + rsw2->num_of_tsn_ports;
+
+	for (cur_port = 0; cur_port < total_ports; cur_port++) {
+		struct net_device *ndev;
+		struct rswitch2_eth_port *eth_port;
+		struct rswitch2_internal_port *intern_port;
+
+		eth_port = rsw2->ports[cur_port];
+		if(!eth_port)
+			continue;
+
+
+		ndev = eth_port->ndev;
+		intern_port = eth_port->intern_port;
+
+		if (intern_port != NULL) {
+			int q;
+			if (ndev)
+				netif_tx_stop_all_queues(ndev);
+
+			rswitch2_disable_rx(ndev);
+
+			for (q = 0; q < NUM_BE_RX_QUEUES; q++)
+				netif_napi_del(&intern_port->rx_q[q].napi);
+
+			rswitch2_free_rings(ndev);
+			kfree(eth_port->intern_port);
+			eth_port->intern_port = NULL;
+		} else if (eth_port->phy_port != NULL) {
+			kfree(eth_port->phy_port);
+			eth_port->phy_port = NULL;
+		}
+
+		if (ndev->phydev != NULL) {
+			if (phy_is_started(ndev->phydev))
+				phy_stop(ndev->phydev);
+		}
+
+	}
+}
+
+int rswitch2_eth_init(struct rswitch2_drv *rsw2)
+{
+	int ret;
+	int i;
+	unsigned int cur_port;
+	unsigned int cur_irq;
+	unsigned int num_of_cpus;
+	uint cur_rx_q;
+	uint cur_tx_q;
+	unsigned int total_ports = rsw2->num_of_cpu_ports + rsw2->num_of_tsn_ports;
+
+	spin_lock_init(&rsw2->lock);
+
+	ret = rswitch2_gwca_init(rsw2);
+	if (ret < 0) {
+		rsw2_err(MSG_GEN, "Failed to initialize GWCA: %d\n", ret);
+		return ret;
+
+	}
+
+	rsw2->ports = kcalloc(total_ports, sizeof(*rsw2->ports), GFP_KERNEL);
+	if (!rsw2->ports)
+		return -ENOMEM;
+
+	ret = rswitch2_bat_init(rsw2);
+	if (ret < 0) {
+		rsw2_err(MSG_GEN, "Base table initialization failed: %d\n", ret);
+		goto bat_init_err;
+	}
+
+	/* Debug message level */
+	rsw2->msg_enable = RSW2_DEF_MSG_ENABLE;
+
+	num_of_cpus = num_online_cpus();
+
+	//dev_info(rsw2->dev, "Detected %d CPUs cores - creating 1 BE queue per core", rsw2->num_of_cpu_ports);
+
+	for (cur_port = 0; cur_port < rsw2->num_of_cpu_ports; cur_port++) {
+		struct net_device *ndev;
+
+		ret = rswitch2_init_internal_port(rsw2, cur_port);
+		if (ret < 0)
+			goto port_init_err;
+
+		ndev = rsw2->ports[cur_port]->ndev;
+
+		// FIXME: ARRAY_SIZE()
+		for(cur_tx_q = 0; cur_tx_q < NUM_BE_TX_QUEUES; cur_tx_q++) {
+
+			/* Init TX best effort queues */
+			ret = rswitch2_tx_ring_init(ndev, cur_tx_q, RSW2_BE_TX_Q_OFFSET);
+			if (ret < 0) {
+				rsw2_err(MSG_GEN, "TX ring init for '%s' failed\n", ndev->name);
+				goto port_init_err;
+			}
+			rsw2_info(MSG_GEN, "BE rswitch2_tx_ring_init() done\n");
+
+			rswitch2_tx_ring_format(ndev, cur_tx_q);
+			if (ret < 0) {
+				rsw2_err(MSG_GEN, "TX ring format for '%s' failed\n", ndev->name);
+				goto port_init_err;
+			}
+			rsw2_info(MSG_GEN,"BE rswitch2_tx_ring_format() done\n");
+		}
+		for(cur_rx_q = 0; cur_rx_q < NUM_BE_RX_QUEUES; cur_rx_q++) {
+
+			/* Init RX best effort queues */
+			rsw2_info(MSG_GEN, "BE Initializing RX ring %d\n", cur_rx_q);
+
+			ret = rswitch2_rx_ring_init(ndev, cur_rx_q, RSW2_BE_RX_Q_OFFSET);
+			if (ret < 0) {
+				rsw2_err(MSG_GEN, "RX ring %d init for '%s' failed\n", cur_rx_q, ndev->name);
+				goto port_init_err;
+			}
+			rsw2_info(MSG_GEN, "BE Formating ring RX ring %d\n", cur_rx_q);
+			rswitch2_rx_ring_format(ndev, cur_rx_q);
+			if (ret < 0) {
+				rsw2_err(MSG_GEN, "RX ring %d format for '%s' failed\n", cur_rx_q, ndev->name);
+				goto port_init_err;
+			}
+
+
+		}
+	}
+
+	for (cur_port = 0; cur_port < rsw2->num_of_tsn_ports; cur_port++) {
+		struct net_device *ndev;
+		struct device_node *dn_port;
+
+		dn_port = rswitch2_get_port_node(rsw2, cur_port);
+		if(!dn_port) {
+			rsw2_err(MSG_GEN, "Port %d has invalid device tree setting. Skipping.\n", cur_port);
+			of_node_put(dn_port);
+			continue;
+		} else if (!of_device_is_available(dn_port)) {
+			rsw2_notice(MSG_GEN, "Port %d is disabled in device tree.\n", cur_port);
+			of_node_put(dn_port);
+			continue;
+		}
+		of_node_put(dn_port);
+
+		ret = rswitch2_init_physical_port(rsw2, cur_port);
+		if (ret < 0) {
+			rsw2_err(MSG_GEN, "Failed to initialize port %d: %d\n", cur_port, ret);
+			continue;
+		}
+		rsw2_info(MSG_GEN, "rswitch2_init_physical_port(%d) done\n", cur_port);
+
+		ndev = rsw2->ports[cur_port + rsw2->num_of_cpu_ports]->ndev;
+		for(cur_tx_q = 0; cur_tx_q < RSW2_LL_TX_PER_PORT_QUEUES; cur_tx_q++) {
+
+			/* Init TX link local queues */
+			ret = rswitch2_tx_ring_init(ndev, cur_tx_q, RSW2_LL_TX_Q_OFFSET + cur_port);
+			if (ret < 0) {
+				rsw2_err(MSG_GEN, "TX ring init for '%s' failed\n", ndev->name);
+				goto port_init_err;
+				return ret;
+			}
+			rsw2_info(MSG_GEN, "LL rswitch2_tx_ring_init() done\n");
+
+			rswitch2_tx_ring_format(ndev, cur_tx_q);
+			if (ret < 0) {
+				rsw2_err(MSG_GEN, "TX ring format for '%s' failed\n", ndev->name);
+				goto port_init_err;
+			}
+			rsw2_info(MSG_GEN, "LL rswitch2_tx_ring_format() done\n");
+		}
+		for(cur_rx_q = 0; cur_rx_q < RSW2_LL_RX_PER_PORT_QUEUES; cur_rx_q++) {
+
+			/* Init RX link local queues */
+			rsw2_info(MSG_GEN, "LL Initializing RX ring %d\n", cur_rx_q);
+
+			ret = rswitch2_rx_ring_init(ndev, cur_rx_q, RSW2_LL_RX_Q_OFFSET + cur_port);
+			if (ret < 0) {
+				rsw2_err(MSG_GEN, "RX ring %d init for '%s' failed\n", cur_rx_q, ndev->name);
+				goto port_init_err;
+			}
+
+			rsw2_info(MSG_GEN, "LL Formating ring RX ring %d\n", cur_rx_q);
+			rswitch2_rx_ring_format(ndev, cur_rx_q);
+			if (ret < 0) {
+				rsw2_err(MSG_GEN, "RX ring %d format for '%s' failed\n", cur_rx_q, ndev->name);
+				goto port_init_err;
+			}
+
+		}
+	}
+
+	/* FIXME: return value */
+	rswitch2_ts_ring_init(rsw2);
+
+
+	for (cur_port = 0; cur_port < rsw2->num_of_tsn_ports + rsw2->num_of_cpu_ports; cur_port++) {
+		struct net_device *ndev;
+
+		if(!rsw2->ports[cur_port])
+			continue;
+
+		ndev = rsw2->ports[cur_port ]->ndev;
+		ret = register_netdev(ndev);
+		if (ret < 0) {
+			rsw2_err(MSG_GEN, "Failed to register netdev: %d\n", ret);
+			goto port_init_err;
+		}
+		else
+			rsw2_info(MSG_GEN, "Register netdev '%s'sucessfully\n", ndev->name);
+	}
+
+	ret = rswitch2_gwca_set_state(rsw2, gwmc_operation);
+	if (ret < 0) {
+		rsw2_err(MSG_GEN, "Failed to set GWCA operation state!\n");
+		goto port_init_err;
+	}
+
+	/* Request data IRQs */
+	for(cur_irq = 0; cur_irq < rsw2->num_of_rxtx_irqs; cur_irq++) {
+		ret = request_irq(rsw2->rxtx_irqs[cur_irq], rswitch2_eth_interrupt, IRQ_TYPE_LEVEL_HIGH,
+						RSWITCH2_NAME, rsw2);
+		if (ret < 0) {
+			rsw2_err(MSG_GEN, "Failed to request data IRQ(%d): %d\n", rsw2->rxtx_irqs[cur_irq], ret);
+			goto bat_init_err;
+		}
+	}
+
+	/* Request status IRQs */
+	for(cur_irq = 0; cur_irq < rsw2->num_of_status_irqs; cur_irq++) {
+		ret = request_irq(rsw2->status_irqs[cur_irq], rswitch2_status_interrupt, IRQ_TYPE_LEVEL_HIGH,
+						RSWITCH2_NAME, rsw2);
+		if (ret < 0) {
+			rsw2_err(MSG_GEN, "Failed to request status IRQ(%d): %d\n", rsw2->status_irqs[cur_irq], ret);
+			goto bat_init_err;
+		}
+	}
+
+	/* Set max frame size to enable data storage in internal RAM */
+	for( i = 0; i < 8; i++) {
+		iowrite32(0xFFFF, rsw2->gwca_base_addrs[0] +RSW2_GCWA_GWRMFSC(i));
+	}
+
+	iowrite32(0x0, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWRDQC);
+
+	return 0;
+
+port_init_err:
+	rswitch2_disable_ports(rsw2);
+
+bat_init_err:
+
+	return ret;
+}
+
+void rswitch2_eth_exit(struct rswitch2_drv *rsw2)
+{
+	struct rswitch2_eth_port *eth_port;
+	uint cur_irq;
+	int i;
+	size_t ts_ring_size;
+	uint ring_entries;
+	uint cur_port;
+	uint total_ports = rsw2->num_of_cpu_ports + rsw2->num_of_tsn_ports;
+
+	/* Set max frame size to zero to avoid data is stored in internal RAM */
+	for( i = 0; i < 8; i++) {
+		iowrite32(0x0, rsw2->gwca_base_addrs[0] +RSW2_GCWA_GWRMFSC(i));
+	}
+	iowrite32(0xFF, rsw2->gwca_base_addrs[0] + RSW2_GCWA_GWRDQC);
+
+
+	for(cur_irq = 0; cur_irq < rsw2->num_of_rxtx_irqs; cur_irq++) {
+		free_irq(rsw2->rxtx_irqs[cur_irq], rsw2);
+	}
+	for(cur_irq = 0; cur_irq < rsw2->num_of_status_irqs; cur_irq++) {
+		free_irq(rsw2->status_irqs[cur_irq], rsw2);
+	}
+
+	for (cur_port = 0; cur_port < total_ports; cur_port++) {
+		struct net_device *ndev;
+		struct rswitch2_internal_port *intern_port;
+		struct rswitch2_physical_port *phy_port;
+
+
+		eth_port = rsw2->ports[cur_port];
+		if(!eth_port)
+			continue;
+
+		ndev = eth_port->ndev;
+		intern_port = eth_port->intern_port;
+		phy_port = eth_port->phy_port;
+
+		rtnl_lock();
+		if(netif_running(ndev))
+			dev_close(ndev);
+		rtnl_unlock();
+
+		if (phy_port) {
+            uint cur_q;
+            uint num_of_rx_queues;
+            uint num_of_tx_queues;
+
+            phy_disconnect(phy_port->phy);
+            phy_device_remove(phy_port->phy);
+			phy_device_free(phy_port->phy);
+
+			mdiobus_unregister(phy_port->mii_bus);
+			mdiobus_free(phy_port->mii_bus);
+
+			num_of_rx_queues = ARRAY_SIZE(phy_port->rx_q);
+			num_of_tx_queues = ARRAY_SIZE(phy_port->tx_q);
+
+			rswitch2_free_rings(ndev);
+
+			for (cur_q = 0; cur_q < num_of_rx_queues; cur_q++) {
+				netif_napi_del(&phy_port->rx_q[cur_q].napi);
+				kfree(phy_port->rx_q[cur_q].skb);
+			}
+
+			for (cur_q = 0; cur_q < num_of_tx_queues; cur_q++)
+				kfree(phy_port->tx_q[cur_q].skb);
+
+			kfree(eth_port->phy_port);
+			eth_port->phy_port = NULL;
+
+		} else if (intern_port) {
+			uint cur_q;
+			uint num_of_rx_queues;
+			uint num_of_tx_queues;
+
+			num_of_rx_queues = ARRAY_SIZE(intern_port->rx_q);
+			num_of_tx_queues = ARRAY_SIZE(intern_port->tx_q);
+
+			rswitch2_free_rings(ndev);
+
+			for (cur_q = 0; cur_q < num_of_rx_queues; cur_q++) {
+				netif_napi_del(&intern_port->rx_q[cur_q].napi);
+				kfree(intern_port->rx_q[cur_q].skb);
+			}
+
+			for (cur_q = 0; cur_q < num_of_tx_queues; cur_q++)
+				kfree(intern_port->tx_q[cur_q].skb);
+
+			kfree(eth_port->intern_port);
+			eth_port->intern_port = NULL;
+		}
+		unregister_netdev(ndev);
+		free_netdev(ndev);
+	}
+
+	ring_entries = rsw2->num_of_tsn_ports * MAX_TS_Q_ENTRIES_PER_PORT;
+	ts_ring_size = sizeof(*rsw2->ts_desc_ring) * (ring_entries + 1);
+	dma_free_coherent(rsw2->dev, ts_ring_size, rsw2->ts_desc_ring, rsw2->ts_desc_dma);
+	dma_free_coherent(rsw2->dev, sizeof(*rsw2->bat_ts_addr), rsw2->bat_ts_addr, rsw2->bat_ts_dma_addr);
+
+	rswitch2_free_bat(rsw2);
+	kfree(rsw2->ports);
+}
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2_eth.h b/drivers/net/ethernet/renesas/rswitch2/rswitch2_eth.h
new file mode 100644
index 000000000000..679a962e6868
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2_eth.h
@@ -0,0 +1,491 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Renesas RSwitch2 Gateway Common Agent device driver
+ *
+ * Copyright (C) 2021 Renesas Electronics Corporation
+ *
+ */
+
+#ifndef _RSWITCH2_ETH_H
+#define _RSWITCH2_ETH_H
+
+#include <linux/phy.h>
+#include <linux/netdevice.h>
+#include <linux/timer.h>
+
+#define RSW2_DEF_MSG_ENABLE \
+		(NETIF_MSG_LINK	  | \
+		 NETIF_MSG_TIMER  | \
+		 NETIF_MSG_RX_ERR | \
+		 NETIF_MSG_TX_ERR)
+
+#define RSW2_NETDEV_BASENAME	"sw%d"
+
+/* Maximum different Q types per port */
+#define MAX_Q_TYPES_PER_PORT			4
+
+/* Maximum timestamp descriptor Q entries */
+#define MAX_TS_Q_ENTRIES_PER_PORT	256
+
+
+/* Queues use for link local data start at 8 */
+#define RSW2_LL_RX_Q_OFFSET		8
+
+/* Reserve 8 queues use for link local data */
+#define RSW2_LINK_LOCAL_RX_Q_NUM	8
+#define RSW2_LINK_LOCAL_TX_Q_NUM	8
+
+/* Pre-defined link local RX queues */
+enum rsw2_link_local_rx_queues {
+	rsw2_ll_rx_q_port0	= RSW2_LL_RX_Q_OFFSET,
+	rsw2_ll_rx_q_port1,
+	rsw2_ll_rx_q_port2,
+	rsw2_ll_rx_q_port3,
+	rsw2_ll_rx_q_port4,
+	rsw2_ll_rx_q_port5,
+	rsw2_ll_rx_q_port6,
+	rsw2_ll_rx_q_port7,
+	rsw2_ll_rx_q_max_entry,
+};
+#define RSW2_LL_RX_RING_SIZE 128
+
+#define RSW2_LL_TX_Q_OFFSET (rsw2_ll_rx_q_max_entry)
+
+/* Pre-defined link local TX queues */
+enum rsw2_link_local_tx_queues {
+	rsw2_ll_tx_q_port0	= RSW2_LL_TX_Q_OFFSET,
+	rsw2_ll_tx_q_port1,
+	rsw2_ll_tx_q_port2,
+	rsw2_ll_tx_q_port3,
+	rsw2_ll_tx_q_port4,
+	rsw2_ll_tx_q_port5,
+	rsw2_ll_tx_q_port6,
+	rsw2_ll_tx_q_port7,
+	rsw2_ll_tx_q_max_entry,
+};
+#define RSW2_LL_TX_RING_SIZE 64
+
+#define RSW2_LL_RX_PER_PORT_QUEUES 1
+#define RSW2_LL_TX_PER_PORT_QUEUES 1
+
+#define RSW2_BE_RX_Q_OFFSET (rsw2_ll_tx_q_max_entry)
+
+/* Pre-defined best effort RX queues */
+enum rsw2_rx_queues {
+	rsw2_be_rx_q_0		= RSW2_BE_RX_Q_OFFSET,
+	rsw2_be_rx_q_1,
+	rsw2_be_rx_q_2,
+	rsw2_be_rx_q_3,
+	rsw2_be_rx_q_4,
+	rsw2_be_rx_q_5,
+	rsw2_be_rx_q_6,
+	rsw2_be_rx_q_7,
+	rsw2_be_rx_q_8,
+	rsw2_be_rx_q_9,
+	rsw2_be_rx_q_10,
+	rsw2_be_rx_q_11,
+	rsw2_be_rx_q_12,
+	rsw2_be_rx_q_13,
+	rsw2_be_rx_q_14,
+	rsw2_be_rx_q_15,
+	rsw2_be_rx_q_max_entry,
+};
+
+#define RSW2_BE_RX_RING_SIZE 1024
+
+
+
+#define RSW2_BE_TX_Q_OFFSET 96
+
+/* Pre-defined best effort TX queues */
+enum rsw2_tx_queues {
+	rsw2_be_tx_q_0		= RSW2_BE_TX_Q_OFFSET,
+	rsw2_be_tx_q_1,
+	rsw2_be_tx_q_2,
+	rsw2_be_tx_q_3,
+	rsw2_be_tx_q_4,
+	rsw2_be_tx_q_5,
+	rsw2_be_tx_q_6,
+	rsw2_be_tx_q_7,
+	rsw2_be_tx_q_max_entry,
+};
+#define RSW2_BE_TX_RING_SIZE 512
+
+#define RX_BAT_START		RSW2_BE_RX_Q_OFFSET
+#define TX_BAT_START		RSW2_BE_TX_Q_OFFSET
+
+#define BAT_ENTRIES			128
+
+#define NUM_BE_RX_QUEUES	(rsw2_be_rx_q_max_entry - RSW2_BE_RX_Q_OFFSET)
+#define NUM_BE_TX_QUEUES	(rsw2_be_tx_q_max_entry - RSW2_BE_TX_Q_OFFSET)
+
+#define NUM_ALL_QUEUES (NUM_BE_TX_QUEUES + NUM_BE_RX_QUEUES)
+
+/* Queues after the pre-defined once, can be used for dynamic allocation */
+#define DYNAMIC_QUEUE_OFFSET	NUM_ALL_QUEUES
+
+/* Max. descriptor per frame */
+#define RX_MAX_DESC_PER_FRAME 16
+#define TX_MAX_DESC_PER_FRAME 16
+
+//#define RX_MAX_DESC_PER_FRAME 32
+//#define TX_MAX_DESC_PER_FRAME 32
+
+/* If set too small packets from the the stack lead to too many descriptor per frame
+ * This will let the driver stall as there is currently no recovery. Other values than
+ * 4095 should be only used for debugging split descriptor behavior
+ */
+#define RSWITCH2_MAX_DESC_SIZE 4095
+
+
+#define RSW2_BUF_ALIGN	128
+#define RSW2_PKT_BUF_SZ 1538
+
+
+/* Default VLAN settings */
+#define RSWITCH2_DEF_CTAG_VLAN_ID	1
+#define RSWITCH2_DEF_CTAG_PCP		0
+#define RSWITCH2_DEF_CTAG_DEI		0
+#define RSWITCH2_DEF_STAG_VLAN_ID	1
+#define RSWITCH2_DEF_STAG_PCP		0
+#define RSWITCH2_DEF_STAG_DEI		0
+
+
+
+enum rsw2_desc_dt {
+	/* Empty data descriptors */
+	DT_FEMPTY_IS	= 1,
+	DT_FEMPTY_IC	= 2,
+	DT_FEMPTY_ND	= 3,
+	DT_FEMPTY		= 4,
+	DT_FEMPTY_START	= 5,
+	DT_FEMPTY_MID	= 6,
+	DT_FEMPTY_END	= 7,
+	/* Data descriptors */
+	DT_FSINGLE		= 8,
+	DT_FSTART		= 9,
+	DT_FMID			= 10,
+	DT_FEND			= 11,
+	/* Chain control */
+	DT_LINKFIX		= 0,
+	DT_LEMPTY		= 12,
+	DT_EEMPTY		= 13,
+	DT_LINK			= 14,
+	DT_EOS			= 15
+};
+
+
+struct rswitch2_dma_desc {
+	__le16 info_ds;		/* Descriptor size */
+	u8 die_dt;			/* Descriptor interrupt enable and type */
+	__u8  dptrh;		/* Descriptor pointer MSB */
+	__le32 dptrl;		/* Descriptor pointer LSW */
+} __packed;
+
+#if 0
+/* The Ethernet TS descriptor definitions. */
+struct rswitch2_dma_ts_desc {
+	__le16 info_ds;		/* Descriptor size */
+	u8 die_dt;			/* Descriptor interrupt enable and type */
+	__u8  dptrh;		/* Descriptor pointer MSB */
+	__le32 dptrl;		/* Descriptor pointer LSW */
+	__le32 ts_nsec;
+	__le32 ts_sec;
+} __packed;
+#endif
+struct rswitch2_dma_ts_desc {
+	__le16 info_ds;		/* Descriptor size */
+	u8 die_dt;			/* Descriptor interrupt enable and type */
+	__u8  : 8;			/* Unused */
+	__u8  tsun;			/* Timestamp unified number */
+	__u8  src_port_num;
+	__u8  dest_port_num;
+	__u8  tn;			/* Timer number */
+	__le32 ts_nsec;
+	__le32 ts_sec;
+} __packed;
+
+struct rswitch2_dma_ext_desc {
+	__le16 info_ds;		/* Descriptor size & INFO0*/
+	u8 die_dt;			/* Descriptor interrupt enable and type */
+	u8  dptrh;			/* Descriptor pointer MSB */
+	__le32 dptrl;		/* Descriptor pointer LSW */
+	__le64 info1;		/* Descriptor INFO1  */
+} __packed;
+
+struct rswitch2_dma_ext_ts_desc {
+	__le16 info_ds;	/* Descriptor size */
+	u8 die_dt;	/* Descriptor interrupt enable and type */
+	__u8  dptrh;	/* Descriptor pointer MSB */
+	__le32 dptrl;	/* Descriptor pointer LSW */
+	__le64 info1;
+	__le32 ts_nsec;
+	__le32 ts_sec;
+} __packed;
+
+
+
+#define RSW2_DESC_DS	GENMASK(11, 0)
+#define RSW2_DESC_INFO0	GENMASK(15, 12)
+#define RSW2_DESC_INFO0_SEC	BIT_MASK(13)
+#define RSW2_DESC_INFO0_FI	BIT_MASK(12)
+
+#define RSW2_DESC_DT	GENMASK(7, 4)
+#define RSW2_DESC_DIE	BIT_MASK(3)
+#define RSW2_DESC_AXIE	BIT_MASK(2)
+#define RSW2_DESC_DSE	BIT_MASK(1)
+#define RSW2_DESC_ERR	BIT_MASK(0)
+
+#define RSW2_DESC_INFO1_SEC	BIT_MASK(0)
+#define RSW2_DESC_INFO1_FI	BIT_MASK(1)
+#define RSW2_DESC_INFO1_FMT	BIT_MASK(2) /* Descriptor format */
+enum rsw2_info1_format {
+	direct_desc = 1
+};
+#define RSW2_DESC_INFO1_TXC		BIT_MASK(3)		/* TX Timestamp capture */
+#define RSW2_DESC_INFO1_IET		BIT_MASK(4)		/* Timestamp insertion request */
+#define RSW2_DESC_INFO1_CRT		BIT_MASK(5)		/* Residence time calculation request */
+#define RSW2_DESC_INFO1_TN		BIT_MASK(6)		/* Timer utilized for capture/insertion */
+#define RSW2_DESC_INFO1_TSUN	GENMASK(15, 8)	/* Timestamp unique number*/
+#define RSW2_DESC_INFO1_RN		GENMASK(23, 16) /* Routing valid */
+#define RSW2_DESC_INFO1_RV		BIT_MASK(27)	/* Routing valid */
+#define RSW2_DESC_INFO1_IPV		GENMASK(30, 28)	/* Internal priority value / Target priority of the frame */
+#define RSW2_DESC_INFO1_FW		BIT_MASK(31)	/* The FCS contained in the frame is wrong */
+#define RSW2_DESC_INFO1_CSD0	GENMASK(38, 32)/* CPU sub destination for GWCA0 */
+#define RSW2_DESC_INFO1_CSD1	GENMASK(46, 40)/* CPU sub destination for GWCA1 */
+#define RSW2_DESC_INFO1_DV		GENMASK(54, 48)/* Destination vector */
+
+
+#define RSW2_STAT_TIMER_INTERVALL (3000)
+#define RSW2_SERDES_OP_TIMER_INTERVALL (500)
+#define RSW2_SERDES_OP_TIMER_INTERVALL_FIRST (1500)
+#define RSW2_SERDES_OP_RETRIES (5)
+
+#define RSW2_RX_TS_DESC 1
+
+
+struct rswitch2_dma_extts_desc {
+	__le16 info_ds;		/* Descriptor size */
+	u8 die_dt;			/* Descriptor interrupt enable and type */
+	__u8  dptrh;		/* Descriptor pointer MSB */
+	__le32 dptrl;		/* Descriptor pointer LSW */
+	__le64 info1;		/* Descriptor pointer */
+	__le32 ts_nsec;
+	__le32 ts_sec;
+} __packed;
+
+struct rsw2_rx_q_data {
+#ifdef RSW2_RX_TS_DESC
+	struct rswitch2_dma_ext_ts_desc *desc_ring;
+#else
+	struct rswitch2_dma_ext_desc *desc_ring;
+#endif
+
+	dma_addr_t desc_dma;
+	//u32 bat_entry;
+	size_t entries;
+	u32 cur_desc;	/* Consumer ring indices */
+	u32 dirty_desc;	/* Producer ring indices */
+	struct sk_buff **skb;
+	struct napi_struct napi;
+	uint offset;
+};
+
+struct rsw2_tx_q_data {
+	/* FIXME: check if there better type then rswitch2_dma_ext_desc */
+	struct rswitch2_dma_ext_desc *desc_ring;
+	dma_addr_t desc_dma;
+	//u32 bat_entry;
+	size_t entries;
+	u32 cur_desc;	/* Consumer ring indices */
+	u32 dirty_desc;	/* Producer ring indices */
+	struct sk_buff **skb;
+	uint offset;
+};
+
+
+struct rswitch2_physical_port {
+	void __iomem *rmac_base_addr;
+	void __iomem *serdes_chan_addr;
+	struct mii_bus *mii_bus;
+	struct phy_device *phy;
+	phy_interface_t phy_iface;
+
+	struct rsw2_rx_q_data rx_q[RSW2_LL_RX_PER_PORT_QUEUES];
+	struct rsw2_tx_q_data tx_q[RSW2_LL_TX_PER_PORT_QUEUES];
+	u8 ts_tag;
+	struct sk_buff *ts_skb[MAX_TS_Q_ENTRIES_PER_PORT];
+	u64 rx_pkt_cnt;
+	u64 rx_byte_cnt;
+	u64 tx_pkt_cnt;
+	u64 tx_byte_cnt;
+	struct timer_list serdes_usxgmii_op_timer;
+	u32 serdes_usxgmii_op_cnt;
+
+};
+
+struct rswitch2_internal_port {
+	struct rsw2_rx_q_data rx_q[NUM_BE_RX_QUEUES];
+	struct rsw2_tx_q_data tx_q[NUM_BE_TX_QUEUES];
+	u64 rx_pkt_cnt;
+	u64 rx_byte_cnt;
+	u64 tx_pkt_cnt;
+	u64 tx_byte_cnt;
+	u32 rx_over_errors;
+	u32 rx_fifo_errors;
+	struct work_struct work;
+};
+
+struct rswitch2_eth_port {
+	struct net_device *ndev;
+	struct rswitch2_drv *rsw2;
+
+	/* FIXME */
+	struct net_device_stats stats[NUM_BE_RX_QUEUES];
+	struct timer_list stat_timer;
+
+	uint lock_count;
+
+
+
+	unsigned int port_num;
+	struct rswitch2_physical_port *phy_port;
+	struct rswitch2_internal_port *intern_port;
+
+};
+
+int rswitch2_eth_init(struct rswitch2_drv *rsw2);
+void rswitch2_eth_exit(struct rswitch2_drv *rsw2);
+
+/* Register definitions */
+
+#define RSW2_ETHA_EAMC		0x0000 /* Ethernet Agent Mode Configuration */
+#define EAMC_OPC		GENMASK(1, 0)
+
+#define RSW2_ETHA_EAMS		0x0004 /* Ethernet Agent Mode Status */
+#define EAMS_OPS		GENMASK(1, 0)
+
+enum emac_op {
+	emac_reset		= 0,
+	emac_disable	= 1,
+	emac_config		= 2,
+	emac_operation	= 3,
+};
+
+#define RSW2_ETHA_EAMC		0x0000 /* Ethernet Agent Mode Configuration */
+
+#define RSW2_ETHA_EATDQSC	0x0014 /* Ethernet Agent TX Descriptor Queue Security Configuration */
+
+#define RSW2_ETHA_EATDQC	0x0018 /* Ethernet Agent TX Descriptor Queue Configuration */
+
+#define RSW2_ETHA_EATDQAC	0x001C /* Ethernet Agent TX Descriptor Queue Arbitration Configuration */
+
+#define RSW2_ETHA_EATPEC	0x0020 /* Ethernet Agent TX Pre-Emption Configuration */
+
+#define RSW2_ETHA_EATMFSC(q)	(0x0040 + 0x4 * (q)) /* Ethernet Agent Transmission Maximum Frame Size Configuration q */
+
+#define RSW2_ETHA_EATDQDC(q)	(0x0060 + 0x4 * (q)) /* Ethernet Agent Transmission Descriptor Queue Depth Configuration q */
+
+#define RSW2_ETHA_EATDQM(q)		(0x0080 + 0x4 * (q)) /* Ethernet Agent Transmission Descriptor Queue q Monitoring */
+
+#define RSW2_ETHA_EATDQMLM(q)	(0x00A0 + 0x4 * (q)) /* Ethernet Agent Transmission Descriptor Queue q Max Level Monitoring */
+#define RSW2_ETHA_EATDQMLME(q)	(0x00C0 + 0x4 * (q)) /* Ethernet Agent Transmission Descriptor Queue q Max Level Monitoring Emu */
+
+#define RSW2_ETHA_EAVCC		0x0130 /* Ethernet Agent VLAN control configuration */
+
+#define RSW2_ETHA_EAVTC		0x0134 /* Ethernet Agent VLAN TAG configuration */
+
+#define RSW2_ETHA_EARTFC	0x0138 /* Ethernet Agent Reception TAG Filtering Configuration */
+
+#define RSW2_ETHA_EACAEC	0x0200 /* Ethernet Agent CBS Admin Enable Configuration */
+
+#define RSW2_ETHA_EACC		0x0204 /* Ethernet Agent CBS Configuration */
+
+#define RSW2_ETHA_EACAIVC(q)	(0x0220 + 0x4 * (q)) /* Ethernet Agent CBS Admin Increment Value Configuration q */
+
+#define RSW2_ETHA_EACAULC(q)	(0x0240 + 0x4 * (q)) /* Ethernet Agent CBS Admin Upper Limit Configuration q */
+
+#define RSW2_ETHA_EACOEM		0x0260 /* Ethernet Agent CBS Oper Enable Monitoring */
+
+#define RSW2_ETHA_EACOIVM(q)	(0x0280 + 0x4 * (q)) /* Ethernet Agent CBS Oper Increment Value Monitoring q */
+
+#define RSW2_ETHA_EACOULM(q)	(0x02A0 + 0x4 * (q)) /* Ethernet Agent CBS Oper Upper Limit Monitoring q */
+
+#define RSW2_ETHA_EACGSM		0x02C0 /* Ethernet Agent CBS Gate State Monitoring */
+
+#define RSW2_ETHA_EATASC		0x0300 /* Ethernet Agent TAS Configuration */
+
+#define RSW2_ETHA_EATASIGSC		0x0304 /* Ethernet Agent TAS Initial Gate State Configuration */
+
+#define RSW2_ETHA_EATASENC(i)	(0x0320 + 0x4 * (i)) /* Ethernet Agent TAS Entry Number Configuration i */
+
+#define RSW2_ETHA_EATASENM(i)	(0x0360 + 0x4 * (i)) /* Ethernet Agent TAS Entry Number Monitoring i */
+
+#define RSW2_ETHA_EATASCSTC0	0x03A0 /* Ethernet Agent TAS Cycle Start Time Configuration 0 */
+
+#define RSW2_ETHA_EATASCSTC1	0x03A4 /* Ethernet Agent TAS Cycle Start Time Configuration 1 */
+
+#define RSW2_ETHA_EATASCSTM0	0x03A8 /* Ethernet Agent TAS Cycle Start Time Monitoring 0 */
+
+#define RSW2_ETHA_EATASCSTM1	0x03AC /* Ethernet Agent TAS Cycle Start Time Monitoring 1 */
+
+#define RSW2_ETHA_EATASCTC	0x03B0 /* Ethernet Agent TAS Cycle Time Configuration */
+
+#define RSW2_ETHA_EATASCTM	0x03B4 /* Ethernet Agent TAS Cycle Time Monitoring */
+
+#define RSW2_ETHA_EATASGL0	0x03C0 /* Ethernet Agent TAS Gate Learn 0 */
+
+#define RSW2_ETHA_EATASGL1	0x03C4 /* Ethernet Agent TAS Gate Learn 1 */
+
+#define RSW2_ETHA_EATASGLR	0x03C8 /* Ethernet Agent TAS Gate Learn Result */
+
+#define RSW2_ETHA_EATASGR	0x03D0 /* Ethernet Agent TAS Gate Read */
+
+#define RSW2_ETHA_EATASGRR	0x03D4 /* Ethernet Agent TAS Gate Read Result */
+
+#define RSW2_ETHA_EATASHCC	0x03E0 /* Ethernet Agent TAS Hardware Calibration Configuration */
+
+#define RSW2_ETHA_EATASRIRM	0x03E4 /* Ethernet Agent TAS RAM Initialization Register Monitoring */
+
+#define RSW2_ETHA_EATASSM	0x03E8 /* Ethernet Agent TAS Status Monitoring */
+
+#define RSW2_ETHA_EAUSMFSECN	0x0400 /* Ethernet Agent Switch Minimum Frame Size Error CouNter */
+#define RSW2_ETHA_EAUSMFSECNE	0x0480 /* Ethernet Agent Switch Minimum Frame Size Error CouNter Emu */
+
+#define RSW2_ETHA_EATFECN		0x0404 /* Ethernet Agent TAG Filtering Error CouNter */
+#define RSW2_ETHA_EATFECNE		0x0484 /* Ethernet Agent TAG Filtering Error CouNter Emu */
+
+#define RSW2_ETHA_EAFSECN		0x0408 /* Ethernet Agent Frame Size Error CouNter */
+#define RSW2_ETHA_EAFSECNE		0x0488 /* Ethernet Agent Frame Size Error CouNter Emu */
+
+#define RSW2_ETHA_EADQOECN		0x040C /* Ethernet Agent Descriptor Queue Overflow Error CouNter */
+#define RSW2_ETHA_EADQOECNE		0x048C /* Ethernet Agent Descriptor Queue Overflow Error CouNter Emu */
+
+#define RSW2_ETHA_EADQSECN		0x0410 /* Ethernet Agent Descriptor Queue Security Error CouNter */
+#define RSW2_ETHA_EADQSECNE		0x0490 /* Ethernet Agent Descriptor Queue Security Error CouNter Emu */
+
+#define RSW2_ETHA_EAEIS0	0x0500 /* Ethernet Agent Error Interrupt Status 0 */
+
+#define RSW2_ETHA_EAEIE0	0x0504 /* Ethernet Agent Error Interrupt Enable 0 */
+
+#define RSW2_ETHA_EAEID0	0x0508 /* Ethernet Agent Error Interrupt Disable 0 */
+
+#define RSW2_ETHA_EAEIS1	0x0510 /* Ethernet Agent Error Interrupt Status 1 */
+
+#define RSW2_ETHA_EAEIE1	0x0514 /* Ethernet Agent Error Interrupt Enable 1 */
+
+#define RSW2_ETHA_EAEID1	0x0518 /* Ethernet Agent Error Interrupt Disable 1 */
+
+#define RSW2_ETHA_EAEIS2	0x0520 /* Ethernet Agent Error Interrupt Status 2 */
+
+#define RSW2_ETHA_EAEIE2	0x0524 /* Ethernet Agent Error Interrupt Enable 2 */
+
+#define RSW2_ETHA_EAEID2	0x0528 /* Ethernet Agent Error Interrupt Disable 2 */
+
+#define RSW2_ETHA_EASCR		0x0580 /* Ethernet Agent Security Configuration Register */
+
+
+
+
+
+
+#endif /* _RSWITCH2_ETH_H */
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2_fwd.c b/drivers/net/ethernet/renesas/rswitch2/rswitch2_fwd.c
new file mode 100644
index 000000000000..66a8a0283a0c
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2_fwd.c
@@ -0,0 +1,515 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Renesas RSwitch2 Ethernet CPU port device driver
+ *
+ * Copyright (C) 2019-2021 Renesas Electronics Corporation
+ *
+ */
+#include <linux/err.h>
+#include <linux/kernel.h>
+#include <linux/iopoll.h>
+#include <linux/etherdevice.h>
+#include "rswitch2.h"
+#include "rswitch2_fwd.h"
+
+static int rsw2_fwd_find_free_cascade_filter_slot(struct rswitch2_drv *rsw2) {
+
+	u32 filter_conf;
+	uint slot_num;
+
+	for(slot_num = 0; slot_num < RSW2_FWD_THBF_N; slot_num++) {
+		filter_conf = ioread32(rsw2->fwd_base_addr + RSW2_FWD_FWCFC(slot_num));
+
+		if(filter_conf == 0) {
+			rsw2_info(MSG_FWD, "Cascade filter slot %d is unused\n", slot_num);
+
+			break;
+		}
+	}
+
+	if(slot_num >= RSW2_FWD_THBF_N) {
+		return -EFILTER_LIST_FULL;
+	}
+
+	return (int) slot_num;
+}
+
+
+static int rsw2_fwd_find_free_3byte_filter_slot(struct rswitch2_drv *rsw2) {
+	u32 filter_conf;
+	u32 filter_val0;
+	u32 filter_val1;
+	uint slot_num;
+
+	for(slot_num = 0; slot_num < RSW2_FWD_THBF_N; slot_num++) {
+		filter_conf = ioread32(rsw2->fwd_base_addr + RSW2_FWD_FWTHBFC(slot_num));
+		filter_val0 = ioread32(rsw2->fwd_base_addr + RSW2_FWD_FWTHBFV0C(slot_num));
+		filter_val1 = ioread32(rsw2->fwd_base_addr + RSW2_FWD_FWTHBFV1C(slot_num));
+
+		if((filter_conf == 0) && (filter_val0 == 0) && (filter_val1 == 0)) {
+			//printk("3byte filter slot %d is unused\n", slot_num);
+
+			break;
+		}
+	}
+
+	if(slot_num >= RSW2_FWD_THBF_N) {
+		return -EFILTER_LIST_FULL;
+	}
+
+
+	return (int) slot_num;
+}
+
+static int rsw2_fwd_add_cascade_filter(struct rswitch2_drv *rsw2, struct cascade_filter *filter) {
+
+	int ret;
+	u32 filter_conf;
+
+	uint filter_slot;
+	uint filter_map_entry;
+
+	ret = rsw2_fwd_find_free_cascade_filter_slot(rsw2);
+	if(ret < 0) {
+		return ret;
+	}
+
+	filter_slot = (uint)ret;
+	filter->stream_id = filter_slot;
+
+	filter_conf = FIELD_PREP(FWCFC_CFPFFV, filter->pframe_bitmap);
+	filter_conf |= FIELD_PREP(FWCFC_CFEFFV, filter->eframe_bitmap);
+
+	for(filter_map_entry = 0; filter_map_entry < RSW2_FWD_CFMF_N; filter_map_entry++) {
+		u32 reg_val;
+
+		reg_val = FIELD_PREP(FWCFMC_CFFV, filter->entry[filter_map_entry].enabled);
+		reg_val |= FIELD_PREP(FWCFMC_CFFN, filter->entry[filter_map_entry].id);
+
+		iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWCFMC(filter_slot, filter_map_entry));
+	}
+
+	iowrite32(filter_conf, rsw2->fwd_base_addr + RSW2_FWD_FWCFC(filter_slot));
+
+	return ret;
+}
+
+
+static int rsw2_fwd_add_3byte_filter(struct rswitch2_drv *rsw2, struct three_byte_filter *filter) {
+	int ret = 0;
+	u32 filter_conf;
+	u32 filter_val0;
+	u32 filter_val1;
+	uint filter_slot;
+
+	ret = rsw2_fwd_find_free_3byte_filter_slot(rsw2);
+	if(ret < 0) {
+		return ret;
+	}
+
+	filter_slot = (uint)ret;
+
+	filter_conf = FIELD_PREP(FWTHBFC_THBFUM, filter->mode);
+	filter_conf |= FIELD_PREP(FWTHBFC_THBFOV, filter->offset);
+
+
+	switch(filter->mode) {
+	case pf_mask_mode:
+		filter_val0  = FIELD_PREP(FWTHBFV0C_THBFV0B0, filter->m.val[2]);
+		filter_val0 |= FIELD_PREP(FWTHBFV0C_THBFV0B1, filter->m.val[1]);
+		filter_val0 |= FIELD_PREP(FWTHBFV0C_THBFV0B2, filter->m.val[0]);
+
+		filter_val1  = FIELD_PREP(FWTHBFV0C_THBFV1B0, filter->m.mask[2]);
+		filter_val1 |= FIELD_PREP(FWTHBFV0C_THBFV1B1, filter->m.mask[1]);
+		filter_val1 |= FIELD_PREP(FWTHBFV0C_THBFV1B2, filter->m.mask[0]);
+
+		filter->m.id = 2 * (filter_slot + RSW2_FWD_TWBF_N);
+		break;
+
+	case pf_expand_mode:
+		filter_val0  = FIELD_PREP(FWTHBFV0C_THBFV0B0, filter->e.val[2]);
+		rsw2_dbg(MSG_FWD, "filter->e.val[0]: 0x%2x filter_val0: 0x%8x\n", filter->e.val[0], filter_val0);
+		filter_val0 |= FIELD_PREP(FWTHBFV0C_THBFV0B1, filter->e.val[1]);
+		rsw2_dbg(MSG_FWD, "filter->e.val[1]: 0x%2x filter_val0: 0x%8x\n", filter->e.val[1], filter_val0);
+		filter_val0 |= FIELD_PREP(FWTHBFV0C_THBFV0B2, filter->e.val[0]);
+		rsw2_dbg(MSG_FWD, "filter->e.val[2]: 0x%2x filter_val0: 0x%8x\n", filter->e.val[2], filter_val0);
+		filter_val1  = FIELD_PREP(FWTHBFV0C_THBFV1B0, filter->e.val[5]);
+		rsw2_dbg(MSG_FWD, "filter->e.val[3]: 0x%2x filter_val1: 0x%8x\n", filter->e.val[3], filter_val1);
+		filter_val1 |= FIELD_PREP(FWTHBFV0C_THBFV1B1, filter->e.val[4]);
+		rsw2_dbg(MSG_FWD, "filter->e.val[4]: 0x%2x filter_val1: 0x%8x\n", filter->e.val[4], filter_val1);
+		filter_val1 |= FIELD_PREP(FWTHBFV0C_THBFV1B2, filter->e.val[3]);
+		rsw2_dbg(MSG_FWD, "filter->e.val[5]: 0x%2x filter_val1: 0x%8x\n", filter->e.val[5], filter_val1);
+
+		filter->e.id = 2 * (filter_slot + RSW2_FWD_TWBF_N);
+		break;
+
+	case pf_precise_mode:
+		filter_val0  = FIELD_PREP(FWTHBFV0C_THBFV0B0, filter->p.val[2]);
+		filter_val0 |= FIELD_PREP(FWTHBFV0C_THBFV0B1, filter->p.val[1]);
+		filter_val0 |= FIELD_PREP(FWTHBFV0C_THBFV0B2, filter->p.val[0]);
+
+		filter_val1  = FIELD_PREP(FWTHBFV0C_THBFV1B0, filter->p.val2[2]);
+		filter_val1 |= FIELD_PREP(FWTHBFV0C_THBFV1B1, filter->p.val2[1]);
+		filter_val1 |= FIELD_PREP(FWTHBFV0C_THBFV1B2, filter->p.val2[0]);
+
+		filter->p.id = 2 * (filter_slot + RSW2_FWD_TWBF_N);
+		filter->p.id2 = filter->p.id + 1;
+		break;
+	}
+
+
+	iowrite32(filter_val0, rsw2->fwd_base_addr + RSW2_FWD_FWTHBFV0C(filter_slot));
+	iowrite32(filter_val1, rsw2->fwd_base_addr + RSW2_FWD_FWTHBFV1C(filter_slot));
+	iowrite32(filter_conf, rsw2->fwd_base_addr + RSW2_FWD_FWTHBFC(filter_slot));
+
+	return ret;
+}
+
+static int rsw2_fwd_add_l3_entry(struct rswitch2_drv *rsw2, u32 stream_id, u32 src_port_vec, u32 dest_port_vec, u32 cpu_q) {
+
+	int ret;
+	u32 reg_val;
+
+	/* FIXME: FIELD_PREP */
+	reg_val = 0;
+
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWLTHTL0);
+
+	/* StreamID upper part - we only use lowest 32bit */
+	iowrite32(0, rsw2->fwd_base_addr + RSW2_FWD_FWLTHTL1);
+	iowrite32(0, rsw2->fwd_base_addr + RSW2_FWD_FWLTHTL2);
+	iowrite32(0, rsw2->fwd_base_addr + RSW2_FWD_FWLTHTL3);
+	iowrite32(stream_id, rsw2->fwd_base_addr + RSW2_FWD_FWLTHTL4);
+
+	reg_val = FIELD_PREP(FWLTHTL7_LTHSLVL, src_port_vec);
+
+// 	reg_val |= FIELD_PREP(FWLTHTL7_LTHRNL, 47 /*- FIXME: Rule number */);
+//	reg_val |= FIELD_PREP(FWLTHTL7_LTHRVL, 1);
+
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWLTHTL7);
+
+	reg_val = FIELD_PREP(FWLTHTL8_LTHCSDL, cpu_q);
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWLTHTL8(0));
+
+	reg_val = FIELD_PREP(FWLTHTL9_LTHDVL, dest_port_vec);
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWLTHTL9);
+
+
+	/* Wait for learning result */
+	ret = readl_poll_timeout(rsw2->fwd_base_addr + RSW2_FWD_FWLTHTIM, reg_val,
+						FIELD_GET(FWLTHTIM_LTHTR, reg_val),
+						RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		pr_err("L3 table learning timed out\n");
+		return ret;
+	}
+
+	reg_val = ioread32(rsw2->fwd_base_addr + RSW2_FWD_FWLTHTLR);
+	if(FIELD_GET(FWLTHTLR_LTHLF, reg_val)) {
+		/* FIXME: Check others error bits */
+		rsw2_err(MSG_FWD, "Learning failed\n");
+	}
+	else {
+		rsw2_info(MSG_FWD, "Learning succeeded\n");
+	}
+	return ret;
+}
+
+int rsw2_fwd_add_l2_entry(struct rswitch2_drv *rsw2, const u8 *macaddr, u32 src_port_vec, u32 dest_port_vec, u32 cpu_q) {
+
+	int ret;
+	u32 reg_val;
+
+	iowrite32(0, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL0);
+
+	reg_val = ((macaddr[0] & 0xff) << 8) | macaddr[1];
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL1);
+
+	reg_val = ((macaddr[2] & 0xff) << 24);
+	reg_val |= ((macaddr[3] & 0xff) << 16);
+	reg_val |= ((macaddr[4] & 0xff) << 8) | macaddr[5];
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL2);
+
+	/* Lock vector learn on all ports */
+	reg_val = FIELD_PREP(FWMACTL3_MACDSLVL, dest_port_vec);
+	reg_val |= FIELD_PREP(FWMACTL3_MACSSLVL, src_port_vec);
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL3);
+
+	reg_val = FIELD_PREP(FWMACTL4_MACCSDL, cpu_q);
+
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL4(0));
+
+	/* CPU port */
+	reg_val = FIELD_PREP(FWMACTL5_MACDVL, (1 << rsw2->num_of_tsn_ports));
+
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL5);
+
+	ret = readl_poll_timeout(rsw2->fwd_base_addr + RSW2_FWD_FWMACTLR, reg_val,
+							(FIELD_GET(FWMACTLR_MACTL, reg_val) == 0),
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		rsw2_err(MSG_FWD, "MAC Table Learn timed out\n");
+		return ret;
+	} else {
+		rsw2_info(MSG_FWD, "MAC Table entry learned: 0x%.8x\n", reg_val);
+	}
+
+	return 0;
+}
+
+int rsw2_fwd_del_l2_entry(struct rswitch2_drv *rsw2, const u8 *macaddr) {
+	int ret;
+	u32 reg_val;
+
+
+
+	reg_val = FIELD_PREP(FWMACTL0_MACED, 1);
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL0);
+
+	reg_val = ((macaddr[0] & 0xff) << 8) | macaddr[1];
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL1);
+
+	reg_val = ((macaddr[2] & 0xff) << 24);
+	reg_val |= ((macaddr[3] & 0xff) << 16);
+	reg_val |= ((macaddr[4] & 0xff) << 8) | macaddr[5];
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL2);
+
+	iowrite32(0, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL5);
+
+	ret = readl_poll_timeout(rsw2->fwd_base_addr + RSW2_FWD_FWMACTLR, reg_val,
+							(FIELD_GET(FWMACTLR_MACTL, reg_val) == 0),
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		rsw2_err(MSG_FWD, "MAC Table delete timed out\n");
+		return ret;
+	} else {
+		rsw2_info(MSG_FWD, "MAC Table entry deleted: 0x%.8x\n", reg_val);
+	}
+
+	return 0;
+}
+
+
+
+
+
+int rswitch2_fwd_init(struct rswitch2_drv *rsw2)
+{
+	int cur_port;
+	int num_of_ports = (rsw2->num_of_cpu_ports + rsw2->num_of_tsn_ports);
+	u32 reg_val;
+	int ret;
+
+	/* Simple static forward configuration to allow access to internal port */
+
+
+	/* Enable access for both APBs */
+	/* TODO: Needs to be checked on S4 */
+	iowrite32(0xFFFFFFFF, rsw2->fwd_base_addr + RSW2_FWD_FWSCR27);
+	iowrite32(0xFFFFFFFF, rsw2->fwd_base_addr + RSW2_FWD_FWSCR28);
+	iowrite32(0xFFFFFFFF, rsw2->fwd_base_addr + RSW2_FWD_FWSCR29);
+	iowrite32(0xFFFFFFFF, rsw2->fwd_base_addr + RSW2_FWD_FWSCR30);
+	iowrite32(0xFFFFFFFF, rsw2->fwd_base_addr + RSW2_FWD_FWSCR31);
+	iowrite32(0xFFFFFFFF, rsw2->fwd_base_addr + RSW2_FWD_FWSCR32);
+	iowrite32(0xFFFFFFFF, rsw2->fwd_base_addr + RSW2_FWD_FWSCR33);
+	iowrite32(0xFFFFFFFF, rsw2->fwd_base_addr + RSW2_FWD_FWSCR34);
+
+	/* Enable IPv4 traffic. Configure MAC handling */
+	reg_val = FWPC0_LTHTA | FWPC0_IP4UE | FWPC0_IP4TE /*| FWPC0_IP4OE*/;
+	reg_val |= FWPC0_MACDSA | FWPC0_MACSSA | FWPC0_MACHLA | FWPC0_MACHMA;
+
+	// FIXME: check if needed
+	reg_val |= FWPC0_L2SE;
+
+
+	for (cur_port = 0; cur_port < num_of_ports; cur_port++) {
+		iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWPC0(cur_port));
+	}
+
+	/* Specification seems to be wrong */
+#if 0
+	for (cur_port = 0; cur_port < num_of_ports; cur_port++) {
+
+		reg_val = ioread32(rsw2->fwd_base_addr + RSW2_FWD_FWPC1(i));
+		printk("REG: 0x%.8x\n", rsw2->fwd_base_addr + RSW2_FWD_FWPC1(i));
+		printk("READ RSW2_FWD_FWPC1(%d): 0x%.8x\n", i, reg_val);
+		reg_val |= FIELD_PREP(FWPC1_DDE, 1);
+		reg_val |= FIELD_PREP(FWPC1_DDSL, 1);
+		printk("WRITE: RSW2_FWD_FWPC1(%d): 0x%.8x\n", i, reg_val);
+		iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWPC1(i));
+	}
+#endif
+
+	reg_val = ioread32(rsw2->fwd_base_addr + RSW2_FWD_FWPC1(rsw2->num_of_tsn_ports));
+	reg_val |= FIELD_PREP(FWPC1_DDE, 1);
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWPC1(rsw2->num_of_tsn_ports));
+
+	reg_val = ioread32(rsw2->fwd_base_addr + RSW2_FWD_FWMACHEC);
+	reg_val |= FIELD_PREP(FWMACHEC_MACHMUE, (RSWITCH2_FWD_MAX_HASH_ENTRIES - 1));
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACHEC);
+
+	/* Initialize MAC table learning */
+
+	/* Wait for MAC learning to complete */
+	iowrite32(FWMACTIM_MACTIOG, rsw2->fwd_base_addr + RSW2_FWD_FWMACTIM);
+
+	ret = readl_poll_timeout(rsw2->fwd_base_addr + RSW2_FWD_FWMACTIM, reg_val,
+						FIELD_GET(FWMACTIM_MACTR, reg_val),
+						RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		rsw2_err(MSG_FWD, "Initialization of MAC table timed out\n");
+		return ret;
+	}
+
+#if 0
+	/* Static entry for GWCA port */
+	reg_val = ((rsw2_own_multicast_mac[0] & 0xff) << 8) | rsw2_own_multicast_mac[1];
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL1);
+
+	reg_val = ((rsw2_own_multicast_mac[2] & 0xff) << 24);
+	reg_val |= ((rsw2_own_multicast_mac[3] & 0xff) << 16);
+	reg_val |= ((rsw2_own_multicast_mac[4] & 0xff) << 8) | rsw2_own_multicast_mac[5];
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL2);
+
+	/* Lock vector learn on all ports */
+	reg_val = FIELD_PREP(FWMACTL3_MACDSLVL, ((1 << num_of_ports) - 1));
+	reg_val |= FIELD_PREP(FWMACTL3_MACSSLVL, ((1 << num_of_ports) - 1));
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL3);
+
+	/* TODO: PTP queue number might need to be adjusted, depending on GWCA implementation */
+	reg_val = FIELD_PREP(FWMACTL4_MACCSDL, 24);
+
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL4(0));
+
+	reg_val = FIELD_PREP(FWMACTL5_MACDVL, (1 << rsw2->num_of_tsn_ports));
+
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACTL5);
+
+	ret = readl_poll_timeout(rsw2->fwd_base_addr + RSW2_FWD_FWMACTLR, reg_val,
+							(FIELD_GET(FWMACTLR_MACTL, reg_val) == 0),
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		pr_err("MAC Table Learn timed out\n");
+		return ret;
+	} else {
+		pr_err("MAC Table entry learned: 0x%.8x\n", reg_val);
+	}
+
+	ret = rsw2_fwd_add_l2_entry(rsw2, rsw2_own_multicast_mac,  ((1 << num_of_ports) - 1),  ((1 << num_of_ports) - 1), 24);
+	if (ret != 0) {
+		pr_err("MAC Table Learn timed out\n");
+		return ret;
+	} else {
+		pr_err("MAC Table entry learned: 0x%.8x\n", reg_val);
+	}
+#endif
+
+
+	/* Configure aging */
+	reg_val = FIELD_PREP(FWMACAGUSP_MACAGUSP, 0x4E);
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACAGUSP);
+
+	reg_val = FIELD_PREP(FWMACAGC_MACAGT, RSWITCH2_MAC_AGING_TIME);
+	reg_val |= FWMACAGC_MACAGE | FWMACAGC_MACAGSL;
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWMACAGC);
+
+	reg_val = FIELD_PREP(FWVLANTEC_VLANTMUE, (RSWITCH2_MAX_VLAN_ENTRIES - 1));
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWVLANTEC);
+
+	iowrite32(FWVLANTIM_VLANTIOG, rsw2->fwd_base_addr + RSW2_FWD_FWVLANTIM);
+
+	ret = readl_poll_timeout(rsw2->fwd_base_addr + RSW2_FWD_FWVLANTIM, reg_val,
+							FIELD_GET(FWVLANTIM_VLANTR, reg_val),
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		rsw2_err(MSG_FWD, "Initialization of VLAN table timed out\n");
+		return ret;
+	}
+
+	/* Configure port forwarding */
+	for (cur_port = 0; cur_port < num_of_ports; cur_port++) {
+		reg_val = FIELD_PREP(FWPBFC_PBDV, ((~(1 << cur_port)) & ((1 << num_of_ports) - 1)));
+		iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWPBFC(cur_port));
+
+		//reg_val = cur_port + 3;
+		reg_val = 24;
+		iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWPBFCSDC(0, cur_port));
+	}
+
+	iowrite32(0, rsw2->fwd_base_addr + RSW2_FWD_FWIP4SC);
+
+	/* Set max. hash entries and collisions */
+	reg_val = FIELD_PREP(FWLTHHEC_LTHHMUE, 0x7FF);
+	reg_val |= FIELD_PREP(FWLTHHEC_LTHHMC, 0x3FF);
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWLTHHEC);
+
+	/* Initialize L3 table */
+	iowrite32(FWLTHTIM_LTHTIOG, rsw2->fwd_base_addr + RSW2_FWD_FWLTHTIM);
+
+	ret = readl_poll_timeout(rsw2->fwd_base_addr + RSW2_FWD_FWLTHTIM, reg_val,
+						FIELD_GET(FWLTHTIM_LTHTR, reg_val),
+						RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		rsw2_err(MSG_FWD, "Initialization of L3 table timed out\n");
+		return ret;
+	}
+
+	/* FIXME */
+	{
+		int i;
+		int lret;
+		struct three_byte_filter filter;
+		struct cascade_filter cc_filter[3 /* num of phy. ports */];
+
+		filter.mode = pf_expand_mode;
+		filter.offset = 0;
+
+		/* Set PTP multicast address */
+		if (!mac_pton("01:80:C2:00:00:0E", filter.e.val))
+			printk("mac_pton() failed\n");
+
+		lret = rsw2_fwd_add_3byte_filter(rsw2, &filter);
+		//printk("lret = %d filter.e.id = %d\n", lret, filter.e.id);
+
+		memset(cc_filter, 0 , sizeof(cc_filter));
+
+		for( i = 0; i < 3; i++) {
+			set_bit(i, &cc_filter[i].eframe_bitmap);
+			set_bit(i, &cc_filter[i].pframe_bitmap);
+
+
+			cc_filter[i].entry[0].id = filter.e.id;
+			cc_filter[i].entry[0].enabled = true;
+
+			rsw2_fwd_add_cascade_filter(rsw2, &cc_filter[i]);
+
+			rsw2_fwd_add_l3_entry(rsw2, cc_filter[i].stream_id, cc_filter[i].eframe_bitmap, 0x8 /* CPU only */, /*25*/ 8 + i);
+		}
+	}
+
+	return 0;
+}
+
+void rswitch2_fwd_exit(struct rswitch2_drv *rsw2)
+{
+	int ret;
+	u32 reg_val;
+
+	/* Wait for MAC learning to complete */
+	iowrite32(FWMACTIM_MACTIOG, rsw2->fwd_base_addr + RSW2_FWD_FWMACTIM);
+
+	ret = readl_poll_timeout(rsw2->fwd_base_addr + RSW2_FWD_FWMACTIM, reg_val,
+			reg_val & FWMACTIM_MACTR,
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0)
+		rsw2_err(MSG_FWD, "Initialization of MAC table timed out\n");
+
+	reg_val = FWVLANTIM_VLANTIOG;
+	iowrite32(reg_val, rsw2->fwd_base_addr + RSW2_FWD_FWVLANTIM);
+
+	ret = readl_poll_timeout(rsw2->fwd_base_addr + RSW2_FWD_FWVLANTIM, reg_val,
+							reg_val & FWVLANTIM_VLANTR,
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0)
+		rsw2_err(MSG_FWD, "Initialization of VLAN table timed out\n");
+
+}
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2_fwd.h b/drivers/net/ethernet/renesas/rswitch2/rswitch2_fwd.h
new file mode 100644
index 000000000000..c7e5d609ca50
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2_fwd.h
@@ -0,0 +1,602 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Renesas RSwitch2 Forwarding Engine device driver
+ *
+ * Copyright (C) 2021 Renesas Electronics Corporation
+ *
+ */
+
+#ifndef _RSWITCH2_FWD_H
+#define _RSWITCH2_FWD_H
+
+#include "rswitch2.h"
+
+#include <linux/bits.h>
+
+#define RSWITCH2_FWD_MAX_HASH_ENTRIES	2048
+
+#define RSW2_FWD_TWBF_N (48)	/* Number of two-byte filters */
+#define RSW2_FWD_THBF_N (16)	/* Number of three-byte filters number */
+#define RSW2_FWD_FOBF_N (48)	/* Number of four-byte filters number */
+#define RSW2_FWD_RAGF_N (16)	/* Number of range-byte filters number */
+#define RSW2_FWD_CADF_N (64)	/* Number of cascade filters number */
+#define RSW2_FWD_CFMF_N (7)		/* Number of cascade mapped filters */
+
+
+/* FIXME: Enum error list */
+#define EFILTER_LIST_FULL (1)
+
+enum perfect_filter_mode {
+	pf_mask_mode = 0,
+	pf_expand_mode = 1,
+	pf_precise_mode = 2,
+};
+
+
+struct three_byte_filter {
+	u8 offset;
+	enum perfect_filter_mode mode;
+	union {
+		u8 val[3];
+		u8 mask[3];
+		uint id;
+	} m;
+
+	union {
+		u8 val[6];
+		uint id;
+	} e;
+
+	union {
+		u8 val[3];
+		u8 val2[3];
+		uint id;
+		uint id2;
+	} p;
+};
+
+struct cascade_filter {
+	ulong pframe_bitmap;
+	ulong eframe_bitmap;
+	struct {
+		bool enabled;
+		uint id;
+	} entry[RSW2_FWD_CFMF_N];
+	ulong stream_id;
+};
+
+
+
+
+#define RSW2_FWD_FWGC		0x0000 /* Forwarding Engine General Configuration */
+#define RSW2_FWD_FWTTC0		0x0010 /* Forwarding Engine TAG TPID Configuration 0 */
+#define RSW2_FWD_FWTTC1		0x0014 /* Forwarding Engine TAG TPID Configuration 1 */
+#define RSW2_FWD_FWCEPTC	0x0020 /* Forwarding Engine CPU Exceptional Path Target Configuration */
+#define RSW2_FWD_FWCEPRC0	0x0024 /* Forwarding Engine CPU Exceptional Path Reason Configuration 0 */
+#define RSW2_FWD_FWCEPRC1	0x0028 /* Forwarding Engine CPU Exceptional Path Reason Configuration 1 */
+#define RSW2_FWD_FWCEPRC2	0x002C /* Forwarding Engine CPU Exceptional Path Reason Configuration 2 */
+#define RSW2_FWD_FWCLPTC	0x0030 /* Forwarding Engine CPU Learning Path Target Configuration */
+#define RSW2_FWD_FWCLPRC	0x0034 /* Forwarding Engine CPU Learning Path Reason Configuration */
+#define RSW2_FWD_FWCMPTC	0x0040 /* Forwarding Engine CPU Mirroring Path Target Configuration */
+#define RSW2_FWD_FWEMPTC	0x0044 /* Forwarding Engine Ethernet Mirroring Path Target Configuration */
+#define RSW2_FWD_FWSDMPTC	0x0050 /* Forwarding Engine Source-Destination Mirroring Path Target Configuration */
+#define RSW2_FWD_FWSDMPVC	0x0054 /* Forwarding Engine Source-Destination Mirroring Path Vector Configuration */
+
+/* Forwarding Engine Level Based WaterMark Configuration i (i=0..PORT_N) */
+#define RSW2_FWD_FWLBWMC(i)	(0x0080 + (0x4 * (i)))
+
+/* Forwarding Engine Port Configuration 0 i (i=0..PORT_N) */
+#define RSW2_FWD_FWPC0(i)	(0x0100 + (0x10 * (i)))
+#define FWPC0_VLANRUS	BIT_MASK(30)
+#define FWPC0_VLANRU	BIT_MASK(29)
+#define FWPC0_VLANSA	BIT_MASK(28)
+#define FWPC0_MACHMA	BIT_MASK(27)
+#define FWPC0_MACHLA	BIT_MASK(26)
+#define FWPC0_MACRUSSA	BIT_MASK(25)
+#define FWPC0_MACRUSA	BIT_MASK(24)
+#define FWPC0_MACSSA	BIT_MASK(23)
+#define FWPC0_MACRUDSA	BIT_MASK(22)
+#define FWPC0_MACRUDA	BIT_MASK(21)
+#define FWPC0_MACDSA	BIT_MASK(20)
+#define FWPC0_L2SE		BIT_MASK(9)
+#define FWPC0_IP6OE		BIT_MASK(8)
+#define FWPC0_IP6TE		BIT_MASK(7)
+#define FWPC0_IP6UE		BIT_MASK(6)
+#define FWPC0_IP4OE		BIT_MASK(5)
+#define FWPC0_IP4TE		BIT_MASK(4)
+#define FWPC0_IP4UE		BIT_MASK(3)
+#define FWPC0_LTHRUSS	BIT_MASK(2)
+#define FWPC0_LTHRUS	BIT_MASK(1)
+#define FWPC0_LTHTA		BIT_MASK(0)
+
+/* Forwarding Engine Port Configuration 1 i (i=0..PORT_N) */
+#define RSW2_FWD_FWPC1(i)	(0x0104 + (0x10 * (i)))
+#define FWPC1_LTHFM		GENMASK(22, 16)
+#define FWPC1_DDSL		BIT_MASK(1)
+#define FWPC1_DDE		BIT_MASK(0)
+
+/* Forwarding Engine Port Configuration 2 i (i=0..PORT_N) */
+#define RSW2_FWD_FWPC2(i)		(0x0108 + (0x10 * (i)))
+
+/* ForWarding engine TWo Byte Filter Configuration i (i=0..PFL_TWBF_N-1) */
+#define RSW2_FWD_FWTWBFC(i)		(0x1000 + (0x10 * (i)))
+
+/* ForWarding engine TWo Byte Filter Value Configuration i (i=0..PFL_TWBF_N-1) */
+#define RSW2_FWD_FWTWBFVC(i)	(0x1004 + (0x10 * (i)))
+
+/* ForWarding engine THree Byte Filter Configuration i (i=0..PFL_THBF_N-1) */
+#define RSW2_FWD_FWTHBFC(i)		(0x1400 + (0x10 * (i)))
+#define FWTHBFC_THBFOV		GENMASK(23, 16)
+#define FWTHBFC_THBFUM		GENMASK(1, 0)
+
+/* ForWarding engine THree Byte Filter Value Configuration 0 i (i=0..PFL_THBF_N-1) */
+#define RSW2_FWD_FWTHBFV0C(i)	(0x1404 + (0x10 * (i)))
+#define FWTHBFV0C_THBFV0B2 		GENMASK(23, 16)
+#define FWTHBFV0C_THBFV0B1 		GENMASK(15, 8)
+#define FWTHBFV0C_THBFV0B0 		GENMASK(7, 0)
+
+/* ForWarding engine THree Byte Filter Value Configuration 1 i (i=0..PFL_THBF_N-1) */
+#define RSW2_FWD_FWTHBFV1C(i)	(0x1408 + (0x10 * (i)))
+#define FWTHBFV0C_THBFV1B2 		GENMASK(23, 16)
+#define FWTHBFV0C_THBFV1B1 		GENMASK(15, 8)
+#define FWTHBFV0C_THBFV1B0 		GENMASK(7, 0)
+
+/* ForWarding engine FOur Byte Filter Configuration i (i=0..PFL_FOBF_N-1) */
+#define RSW2_FWD_FWFOBFC(i)		(0x1800 + (0x10 * (i)))
+
+/* ForWarding engine FOur Byte Filter Value 0 Configuration i (i=0..PFL_FOBF_N-1) */
+#define RSW2_FWD_FWFOBFV0C(i)	(0x1804 + (0x10 * (i)))
+
+/* ForWarding engine FOur Byte Filter Value 1 Configuration i (i=0..PFL_FOBF_N-1) */
+#define RSW2_FWD_FWFOBFV1C(i)	(0x1808 + (0x10 * (i)))
+
+/* Range Filter Configuration i (i=0..PFL_RAGF_N-1) */
+#define RSW2_FWD_FWRFC(i)		(0x1C00 + (0x10 * (i)))
+
+/* Forwarding Engine Range Filter Value Configuration i (i=0..PFL_RAGF_N-1) */
+#define RSW2_FWD_FWRFVC(i)		(0x1C04 + (0x10 * (i)))
+
+/* Forwarding Engine Cascade Filter Configuration i (i=0..PFL_CADF_N-1) */
+#define RSW2_FWD_FWCFC(i)		(0x2000 + (0x40 * (i)))
+#define FWCFC_CFPFFV			GENMASK(19, 16) /* Cascade Filter P-Frame Filter Valid */
+#define FWCFC_CFEFFV			GENMASK( 6,  0) /* Cascade Filter E-Frame Filter Valid */
+
+/* Forwarding Engine Cascade Filter Mapping Config. i j (i=0..PFL_CADF_N-1) (j=0..PFL_CFMF_N)*/
+#define RSW2_FWD_FWCFMC(i, j)	(0x2004 + (0x40 * (i)) + (0x4 * (j)))
+#define FWCFMC_CFFV				BIT_MASK(15)
+#define FWCFMC_CFFN				GENMASK(14, 0)
+
+#define RSW2_FWD_FWIP4SC	0x4008 /* Forwarding Engine IPv4 Stream Configuration */
+#define RSW2_FWD_FWIP6SC	0x4018 /* Forwarding Engine IPv6 Stream Configuration */
+#define RSW2_FWD_FWIP6OC	0x401C /* Forwarding Engine IPv6 Offset Configuration */
+#define RSW2_FWD_FWL2SC		0x4020 /* Forwarding Engine Layer 2 Stream Configuration */
+#define RSW2_FWD_FWSFHEC	0x4030 /* Forwarding Engine Stream Filter Hash Equation Configuration */
+#define RSW2_FWD_FWSHCR0	0x4040 /* Forwarding Engine Software Hash Calculation Request 0 */
+#define RSW2_FWD_FWSHCR1	0x4044 /* Forwarding Engine Software Hash Calculation Request 1 */
+#define RSW2_FWD_FWSHCR2	0x4048 /* Forwarding Engine Software Hash Calculation Request 2 */
+#define RSW2_FWD_FWSHCR3	0x404C /* Forwarding Engine Software Hash Calculation Request 3 */
+#define RSW2_FWD_FWSHCR4	0x4050 /* Forwarding Engine Software Hash Calculation Request 4 */
+#define RSW2_FWD_FWSHCR5	0x4054 /* Forwarding Engine Software Hash Calculation Request 5 */
+#define RSW2_FWD_FWSHCR6	0x4058 /* Forwarding Engine Software Hash Calculation Request 6 */
+#define RSW2_FWD_FWSHCR7	0x405C /* Forwarding Engine Software Hash Calculation Request 7 */
+#define RSW2_FWD_FWSHCR8	0x4060 /* Forwarding Engine Software Hash Calculation Request 8 */
+#define RSW2_FWD_FWSHCR9	0x4064 /* Forwarding Engine Software Hash Calculation Request 9 */
+#define RSW2_FWD_FWSHCR10	0x4068 /* Forwarding Engine Software Hash Calculation Request 10*/
+#define RSW2_FWD_FWSHCR11	0x406C /* Forwarding Engine Software Hash Calculation Request 11*/
+#define RSW2_FWD_FWSHCR12	0x4070 /* Forwarding Engine Software Hash Calculation Request 12*/
+#define RSW2_FWD_FWSHCR13	0x4074 /* Forwarding Engine Software Hash Calculation Request 13*/
+#define RSW2_FWD_FWSHCRR	0x4078 /* Forwarding Engine Software Hash Calculation Request Result */
+#define RSW2_FWD_FWLTHHEC	0x4090 /* Forwarding Engine L3 Hash Entry Configuration */
+#define FWLTHHEC_LTHHMUE	GENMASK(26, 16)
+#define FWLTHHEC_LTHHMC		GENMASK( 9, 0)
+#define RSW2_FWD_FWLTHHC	0x4094 /* Forwarding Engine L3 Hash Configuration */
+#define RSW2_FWD_FWLTHTL0	0x40A0 /* Forwarding Engine L3 Table Learn 0 */
+#define RSW2_FWD_FWLTHTL1	0x40A4 /* Forwarding Engine L3 Table Learn 1 */
+#define RSW2_FWD_FWLTHTL2	0x40A8 /* Forwarding Engine L3 Table Learn 2 */
+#define RSW2_FWD_FWLTHTL3	0x40AC /* Forwarding Engine L3 Table Learn 3 */
+#define RSW2_FWD_FWLTHTL4	0x40B0 /* Forwarding Engine L3 Table Learn 4 */
+#define RSW2_FWD_FWLTHTL5	0x40B4 /* Forwarding Engine L3 Table Learn 5 */
+#define RSW2_FWD_FWLTHTL6	0x40B8 /* Forwarding Engine L3 Table Learn 5 */
+#define RSW2_FWD_FWLTHTL7	0x40BC /* Forwarding Engine L3 Table Learn 7 */
+#define FWLTHTL7_LTHSLVL	GENMASK(22, 16)	/* L3 Source Lock Vector Learn */
+#define FWLTHTL7_LTHRVL		BIT_MASK(15)	/* L3 Routing Number Learn Valid */
+#define FWLTHTL7_LTHRNL		GENMASK(7, 0)	/* L3 Routing Number Learn */
+
+/* Forwarding Engine L3 Table Learn 8 i (i=0..PORT_GWCA_N-1) */
+#define RSW2_FWD_FWLTHTL8(i)	(0x40C0 + (0x4 * (i)))
+#define FWLTHTL8_LTHCSDL	GENMASK(6, 0)	/* L3 CPU Sub-Destination Learn */
+
+#define RSW2_FWD_FWLTHTL9	0x40D0 /* Forwarding Engine L3 Table Learn 9 */
+#define FWLTHTL9_LTHCMEL	BIT_MASK(21)  /* L3 CPU Mirroring Enable Learn */
+#define FWLTHTL9_LTHEMEL 	BIT_MASK(20)  /* L3 Ethernet Mirroring Enable Learn */
+#define FWLTHTL9_LTHIPUL 	BIT_MASK(19)  /* L3 Internal Priority Update Learn */
+#define FWLTHTL9_LTHIPVL 	GENMASK(18, 16) /* L3 Internal Priority Value Learn */
+#define FWLTHTL9_LTHDVL 	GENMASK(6, 0)	/* L3 Destination Vector Learn */
+#define RSW2_FWD_FWLTHTLR	0x40D4 /* Forwarding Engine L3 Table Learn Result */
+#define FWLTHTLR_LTHTL 		BIT_MASK(31) /* L3 Table Learn */
+#define FWLTHTLR_LTHLCN		GENMASK(25, 16)
+#define FWLTHTLR_LTHLO BIT_MASK(3) /* L3 Learn Overwrite */
+#define FWLTHTLR_LTHLEF BIT_MASK(2) /* L3 Learn ECC Fail */
+#define FWLTHTLR_LTHLSF BIT_MASK(1) /* L3 Learn Security Fail */
+#define FWLTHTLR_LTHLF BIT_MASK(0) /* L3 Learn Fail */
+#define RSW2_FWD_FWLTHTIM	0x40E0 /* Forwarding Engine L3 Table Initialization Monitoring */
+#define FWLTHTIM_LTHTR		BIT_MASK(1)
+#define FWLTHTIM_LTHTIOG	BIT_MASK(0)
+#define RSW2_FWD_FWLTHTEM	0x40E4 /* Forwarding Engine L3 Table Entry Monitoring */
+#define RSW2_FWD_FWLTHTS0	0x4100 /* Forwarding Engine L3 Table Search 0 */
+#define RSW2_FWD_FWLTHTS1	0x4104 /* Forwarding Engine L3 Table Search 1 */
+#define RSW2_FWD_FWLTHTS2	0x4108 /* Forwarding Engine L3 Table Search 2 */
+#define RSW2_FWD_FWLTHTS3	0x410C /* Forwarding Engine L3 Table Search 3 */
+#define RSW2_FWD_FWLTHTS4	0x4110 /* Forwarding Engine L3 Table Search 4 */
+#define RSW2_FWD_FWLTHTSR0	0x4120 /* Forwarding Engine L3 Table Search Result 0 */
+#define RSW2_FWD_FWLTHTSR1	0x4124 /* Forwarding Engine L3 Table Search Result 1 */
+#define RSW2_FWD_FWLTHTSR2	0x4128 /* Forwarding Engine L3 Table Search Result 2 */
+#define RSW2_FWD_FWLTHTSR3	0x412C /* Forwarding Engine L3 Table Search Result 3 */
+
+/* Forwarding Engine L3 Table Search Result 4 i (i=0..PORT_GWCA_N-1) */
+#define RSW2_FWD_FWLTHTSR4(i)	(0x4130 + (0x4 * (i)))
+
+#define RSW2_FWD_FWLTHTSR5	0x4140 /* Forwarding Engine L3 Table Search Result 5 */
+#define RSW2_FWD_FWLTHTR	0x4150 /* Forwarding Engine L3 Table Read */
+#define RSW2_FWD_FWLTHTRR0	0x4154 /* Forwarding Engine L3 Table Read Result 0 */
+#define RSW2_FWD_FWLTHTRR1	0x4158 /* Forwarding Engine L3 Table Read Result 1 */
+#define RSW2_FWD_FWLTHTRR2	0x415C /* Forwarding Engine L3 Table Read Result 2 */
+#define RSW2_FWD_FWLTHTRR3	0x4160 /* Forwarding Engine L3 Table Read Result 3 */
+#define RSW2_FWD_FWLTHTRR4	0x4164 /* Forwarding Engine L3 Table Read Result 4 */
+#define RSW2_FWD_FWLTHTRR5	0x4168 /* Forwarding Engine L3 Table Read Result 5 */
+#define RSW2_FWD_FWLTHTRR6	0x416C /* Forwarding Engine L3 Table Read Result 6 */
+#define RSW2_FWD_FWLTHTRR7	0x4170 /* Forwarding Engine L3 Table Read Result 7 */
+#define RSW2_FWD_FWLTHTRR8	0x4174 /* Forwarding Engine L3 Table Read Result 8 */
+
+/* Forwarding Engine L3 Table Read Result 9 i (i=0..PORT_GWCA_N-1) */
+#define RSW2_FWD_FWLTHTRR9(i)	(0x4180 + (0x4 * (i)))
+
+#define RSW2_FWD_FWLTHTRR10	0x4190 /* Forwarding Engine L3 Table Read Result 10 */
+
+#define RSW2_FWD_FWMACHEC	0x4620 /* Forwarding Engine MAC Hash Entry Configuration */
+#define FWMACHEC_MACHMUE	GENMASK(26, 16)
+#define FWMACHEC_MACHMC		GENMASK(9, 0)
+
+
+#define RSW2_FWD_FWMACHC	0x4624 /* Forwarding Engine MAC Hash Configuration */
+#define RSW2_FWD_FWMACTL0	0x4630 /* Forwarding Engine MAC Table Learn 0 */
+#define FWMACTL0_MACED		BIT_MASK(16)
+
+#define RSW2_FWD_FWMACTL1	0x4634 /* Forwarding Engine MAC Table Learn 1 */
+#define FWMACTL1_MACMALP0	GENMASK(15, 0)
+
+#define RSW2_FWD_FWMACTL2	0x4638 /* Forwarding Engine MAC Table Learn 2 */
+#define FWMACTL2_MACMALP1	GENMASK(31, 0)
+
+#define RSW2_FWD_FWMACTL3	0x463C /* Forwarding Engine MAC Table Learn 3 */
+#define FWMACTL3_MACDSLVL	GENMASK(22, 16)
+#define FWMACTL3_MACSSLVL	GENMASK(6, 0)
+
+/* Forwarding Engine MAC Table Learn 4 (i=0..PORT_GWCA_N-1) */
+#define RSW2_FWD_FWMACTL4(i)	(0x4640 + (0x4 * (i)))
+#define FWMACTL4_MACCSDL		GENMASK(6, 0)
+
+#define RSW2_FWD_FWMACTL5	0x4650 /* Forwarding Engine MAC Table Learn 5 */
+#define FWMACTL5_MACCMEL	BIT_MASK(21)
+#define FWMACTL5_MACEMEL	BIT_MASK(20)
+#define FWMACTL5_MACIPUL	BIT_MASK(19)
+#define FWMACTL5_MACIPVL	GENMASK(18, 16)
+#define FWMACTL5_MACDVL		GENMASK(6, 0)
+
+#define RSW2_FWD_FWMACTLR	0x4654 /* Forwarding Engine MAC Table Learn Result */
+#define FWMACTLR_MACTL		BIT_MASK(31)
+#define FWMACTLR_MACLCN		GENMASK(25, 16)
+#define FWMACTLR_MACLO		BIT_MASK(3)
+#define FWMACTLR_MACLEF		BIT_MASK(2)
+#define FWMACTLR_MACLSF		BIT_MASK(1)
+#define FWMACTLR_MACLF		BIT_MASK(0)
+
+
+#define RSW2_FWD_FWMACTIM	0x4660 /* Forwarding Engine MAC Table Initialization Monitoring */
+#define FWMACTIM_MACTR		BIT_MASK(1)
+#define FWMACTIM_MACTIOG	BIT_MASK(0)
+
+#define RSW2_FWD_FWMACTEM	0x4664 /* Forwarding Engine MAC Table Entry Monitoring */
+#define FWMACTEM_MACTUEN	GENMASK(28, 16)
+#define FWMACTEM_MACTEN		GENMASK(10, 0)
+
+#define RSW2_FWD_FWMACTS0	0x4670 /* Forwarding Engine MAC Table Search 0 */
+#define RSW2_FWD_FWMACTS1	0x4674 /* Forwarding Engine MAC Table Search 1 */
+#define RSW2_FWD_FWMACTSR0	0x4678 /* Forwarding Engine MAC Table Search Result 0 */
+#define RSW2_FWD_FWMACTSR1	0x467C /* Forwarding Engine MAC Table Search Result 1 */
+
+/* Forwarding Engine MAC Table Search Result 2 (i=0..PORT_GWCA_N-1) */
+#define RSW2_FWD_FWMACTSR2(i)	(0x4680 + (0x4 * (i)))
+
+#define RSW2_FWD_FWMACTSR3	0x4690 /* Forwarding Engine MAC Table Search Result 3 */
+#define RSW2_FWD_FWMACTR	0x46A0 /* Forwarding Engine MAC Table Read */
+#define RSW2_FWD_FWMACTRR0	0x46A4 /* Forwarding Engine MAC Table Read Result 0 */
+#define RSW2_FWD_FWMACTRR1	0x46A8 /* Forwarding Engine MAC Table Read Result 1 */
+#define RSW2_FWD_FWMACTRR2	0x46AC /* Forwarding Engine MAC Table Read Result 2 */
+#define RSW2_FWD_FWMACTRR3	0x46B0 /* Forwarding Engine MAC Table Read Result 3 */
+#define RSW2_FWD_FWMACTRR4	0x46B4 /* Forwarding Engine MAC Table Read Result 4 */
+
+/* Forwarding Engine MAC Table Read Result 5 (i=0..PORT_GWCA_N-1) */
+#define RSW2_FWD_FWMACTRR5(i)	(0x46C0 + (0x4 * (i)))
+
+#define RSW2_FWD_FWMACTRR6	0x46D0 /* Forwarding Engine MAC Table Read Result 6 */
+
+#define RSW2_FWD_FWMACAGUSP	0x4880 /* Forwarding Engine MAC AGing US Prescaler Configuration*/
+#define FWMACAGUSP_MACAGUSP	GENMASK(9, 0)
+
+#define RSW2_FWD_FWMACAGC	0x4884 /* Forwarding Engine MAC AGing Configuration */
+#define FWMACAGC_MACDESOG	BIT_MASK(29)
+#define FWMACAGC_MACAGOG	BIT_MASK(28)
+#define FWMACAGC_MACDES		BIT_MASK(24)
+#define FWMACAGC_MACAGPM	BIT_MASK(18)
+#define FWMACAGC_MACAGSL	BIT_MASK(17)
+#define FWMACAGC_MACAGE		BIT_MASK(16)
+#define FWMACAGC_MACAGT		GENMASK(15, 0)
+
+#define RSW2_FWD_FWMACAGM0	0x4888 /* Forwarding Engine MAC AGing Monitoring 0 */
+#define RSW2_FWD_FWMACAGM1	0x488C /* Forwarding Engine MAC AGing Monitoring 1 */
+
+#define RSW2_FWD_FWVLANTEC	0x4900 /* Forwarding Engine VLAN Table Entry Configuration */
+#define FWVLANTEC_VLANTMUE	GENMASK(28, 16)
+
+#define RSW2_FWD_FWVLANTL0		0x4910 /* Forwarding Engine VLAN Table Learn 0 */
+#define RSW2_FWD_FWVLANTL1		0x4914 /* Forwarding Engine VLAN Table Learn 1 */
+#define RSW2_FWD_FWVLANTL2		0x4918 /* Forwarding Engine VLAN Table Learn 2 */
+
+/* Forwarding Engine VLAN Table Learn 3 (i=0..PORT_GWCA_N-1) */
+#define RSW2_FWD_FWVLANTL3(i)	(0x4920 + (0x4 * (i)))
+
+#define RSW2_FWD_FWVLANTL4		0x4930 /* Forwarding Engine VLAN Table Learn 4 */
+#define RSW2_FWD_FWVLANTLR		0x4934 /* Forwarding Engine VLAN Table Learn Result */
+#define RSW2_FWD_FWVLANTIM		0x4940 /* Forwarding Engine VLAN Table Initialization Monitoring */
+#define FWVLANTIM_VLANTR		BIT_MASK(1)
+#define FWVLANTIM_VLANTIOG		BIT_MASK(0)
+
+#define RSW2_FWD_FWVLANTEM		0x4944 /* Forwarding Engine VLAN Table Entry Monitoring */
+#define RSW2_FWD_FWVLANTS		0x4950 /* Forwarding Engine VLAN Table Search */
+#define RSW2_FWD_FWVLANTSR0		0x4954 /* Forwarding Engine VLAN Table Search Result 0 */
+#define RSW2_FWD_FWVLANTSR1		0x4958 /* Forwarding Engine VLAN Table Search Result 1 */
+
+/* Forwarding Engine VLAN Table Search Result 2 (i=0..PORT_GWCA_N-1) */
+#define RSW2_FWD_FWVLANTSR2(i)	(0x4960 + (0x4 * (i)))
+
+#define RSW2_FWD_FWVLANTSR3		0x4970 /* Forwarding Engine VLAN Table Search Result 3 */
+
+/* Forwarding Engine Port Based Forwarding Configuration i (i=0..PORT_N) */
+#define RSW2_FWD_FWPBFC(i)		(0x4A00 + (0x10 * (i)))
+#define FWPBFC_FAIFP	BIT_MASK(1)
+#define FWPBFC_IP6PDE	BIT_MASK(1)
+#define FWPBFC_IP4PDM	BIT_MASK(1)
+#define FWPBFC_IP4PDE	BIT_MASK(1)
+#define FWPBFC_PBSL		BIT_MASK(1)
+#define FWPBFC_PBCME	BIT_MASK(1)
+#define FWPBFC_PBEME	BIT_MASK(1)
+#define FWPBFC_PBIPU	BIT_MASK(1)
+#define FWPBFC_PBIPV	GENMASK(18, 16)
+#define FWPBFC_PBDV		GENMASK(6, 0)
+
+/* Forwarding Engine Port Based Forwarding CSD Configuration j i (j= 0..PORT_GWCA_N-1) (i= 0..PORT_N-1)*/
+#define RSW2_FWD_FWPBFCSDC(j, i)	(0x4A04 + (0x10 * (i)) + (0x4 * (j)))
+
+#define RSW2_FWD_FWL23URL0		0x4E00 /* Forwarding Engine Layer2/Layer3 Update Rule Learn 0 */
+#define RSW2_FWD_FWL23URL1		0x4E04 /* Forwarding Engine Layer2/Layer3 Update Rule Learn 1 */
+#define RSW2_FWD_FWL23URL2		0x4E08 /* Forwarding Engine Layer2/Layer3 Update Rule Learn 2 */
+#define RSW2_FWD_FWL23URL3		0x4E0C /* Forwarding Engine Layer2/Layer3 Update Rule Learn 3 */
+#define RSW2_FWD_FWL23URLR		0x4E10 /* Forwarding Engine Layer2/Layer3 Update Rule Learn Result */
+#define RSW2_FWD_FWL23UTIM		0x4E20 /* Forwarding Engine Layer2/Layer3 Update Table Initialization Monitoring */
+#define RSW2_FWD_FWL23URR		0x4E30 /* Forwarding Engine Layer2/Layer3 Update Rule Read */
+#define RSW2_FWD_FWL23URRR0		0x4E34 /* Forwarding Engine Layer2/Layer3 Update Rule Read Result 0 */
+#define RSW2_FWD_FWL23URRR1		0x4E38 /* Forwarding Engine Layer2/Layer3 Update Rule Read Result 1 */
+#define RSW2_FWD_FWL23URRR2		0x4E3C /* Forwarding Engine Layer2/Layer3 Update Rule Read Result 2 */
+#define RSW2_FWD_FWL23URRR3		0x4E40 /* Forwarding Engine Layer2/Layer3 Update Rule Read Result 3 */
+
+/* Forwarding Engine Layer2/Layer3 Update ReMapping Configuration i (i=0..LTH_REMAP_N) */
+#define RSW2_FWD_FWL23URMC(i)	(0x4F00 + (0x4 * (i)))
+
+/* Forwarding Engine PSFP MSDU Filter Global Configuration i (i=0..PSFP_MSDU_N-1) */
+#define RSW2_FWD_FWPMFGC(i)		(0x5000 + (0x4 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Configuration i (i=0..PSFP_GATE_N-1) */
+#define RSW2_FWD_FWPGFC(i)		(0x5100 + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Initial Gate State Configuration i (i=0..PSFP_GATE_N-1) */
+#define RSW2_FWD_FWPGFIGSC(i)	(0x5104 + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Entry Number Configuration i (i=0..PSFP_GATE_N-1) */
+#define RSW2_FWD_FWPGFENC(i)	(0x5108 + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Entry Number Monitoring i (i=0..PSFP_GATE_N-1) */
+#define RSW2_FWD_FWPGFENM(i)	(0x510C + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Cycle Start Time Configuration 0 i (i=0..PSFP_GATE_N-1)*/
+#define RSW2_FWD_FWPGFCSTC0(i)	(0x5110 + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Cycle Start Time Configuration 1 i (i=0..PSFP_GATE_N-1) */
+#define RSW2_FWD_FWPGFCSTC1(i)	(0x5114 + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Cycle Start Time Monitoring 0 i (i=0..PSFP_GATE_N-1) */
+#define RSW2_FWD_FWPGFCSTM0(i)	(0x5118 + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Cycle Start Time Monitoring 1 i (i=0..PSFP_GATE_N-1) */
+#define RSW2_FWD_FWPGFCSTM1(i)	(0x511C + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Cycle Time Configuration i (i=0..PSFP_GATE_N-1) */
+#define RSW2_FWD_FWPGFCTC(i)	(0x5120 + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Cycle Time Monitoring i (i=0..PSFP_GATE_N-1) */
+#define RSW2_FWD_FWPGFCTM(i)	(0x5124 + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Hardware Calibration Configuration i (i=0..PSFP_GATE_N-1) */
+#define RSW2_FWD_FWPGFHCC(i)	(0x5128 + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Status Monitoring i */
+#define RSW2_FWD_FWPGFSM(i)		(0x512C + (0x40 * (i)))
+
+/* Forwarding Engine PSFP Gate Filter Global Configuration i */
+#define RSW2_FWD_FWPGFGC(i)		(0x5130 + (0x40 * (i)))
+
+#define RSW2_FWD_FWPGFGL0		0x5500 /* Forwarding Engine PSFP Gate Filter Gate Learn 0 */
+#define RSW2_FWD_FWPGFGL1		0x5504 /* Forwarding Engine PSFP Gate Filter Gate Learn 1 */
+#define RSW2_FWD_FWPGFGLR		0x5508 /* Forwarding Engine PSFP Gate Filter Gate Learn Result */
+#define RSW2_FWD_FWPGFGR		0x5510 /* Forwarding Engine PSFP Gate Filter Gate Read */
+#define RSW2_FWD_FWPGFGRR0		0x5514 /* Forwarding Engine PSFP Gate Filter Read Result 0 */
+#define RSW2_FWD_FWPGFGRR1		0x5518 /* Forwarding Engine PSFP Gate Filter Read Result 1 */
+#define RSW2_FWD_FWPGFRIM		0x5520 /* Forwarding Engine PSFP Gate Filter RAM Initialization Monitoring */
+
+/* Forwarding Engine PSFP MeTeR Filter Configuration i (i=0..PSFP_MTR_N-1) */
+#define RSW2_FWD_FWPMTRFC(i)	(0x5600 + (0x20 * (i)))
+
+/* Forwarding Engine PSFP MeTeR CBS Configuration i (i=0..PSFP_MTR_N-1) */
+#define RSW2_FWD_FWPMTRCBSC(i)	(0x5604 + (0x20 * (i)))
+
+/* Forwarding Engine PSFP MeTeR CIR Configuration i (i=0..PSFP_MTR_N-1) */
+#define RSW2_FWD_FWPMTRCIRC(i)	(0x5608 + (0x20 * (i)))
+
+/* Forwarding Engine PSFP MeTeR EBS Configuration i (i=0..PSFP_DMTR_N-1) */
+#define RSW2_FWD_FWPMTREBSC(i)	(0x560C + (0x20 * (i)))
+
+/* Forwarding Engine PSFP MeTeR EIR Configuration i (i=0..PSFP_DMTR_N-1) */
+#define RSW2_FWD_FWPMTREIRC(i)	(0x5610 + (0x20 * (i)))
+
+/* Forwarding Engine PSFP MeTeR Filter Monitoring i (i=0..PSFP_MTR_N-1) */
+#define RSW2_FWD_FWPMTRFM(i)	(0x5614 + (0x20 * (i)))
+
+/* Forwarding Engine Direct Descriptor Forwarded Descriptor CouNter i */
+#define RSW2_FWD_FWDDFDCN(i)	(0x6300 + (0x20 * (i)))
+
+/* Forwarding Engine Direct Descriptor Forwarded Descriptor CouNter Emu i (i=0..PORT_SLOW_N-1) */
+#define RSW2_FWD_FWDDFDCNE(i)	(0x6E00 + (0x20 * (i)))
+
+/* Forwarding Engine Layer 3 Forwarded Descriptor CouNter i */
+#define RSW2_FWD_FWLTHFDCN(i)	(0x6304 + (0x20 * (i)))
+
+/* Forwarding Engine Layer 3 Forwarded Descriptor CouNter Emu i */
+#define RSW2_FWD_FWLTHFDCNE(i)	(0x6E04 + (0x20 * (i)))
+
+/* Forwarding Engine Layer 2 Forwarded Descriptor CouNter i */
+#define RSW2_FWD_FWLTWFDCN(i)	(0x630C + (0x20 * (i)))
+
+/* Forwarding Engine Layer 2 Forwarded Descriptor CouNter Emu i */
+#define RSW2_FWD_FWLTWFDCNE(i)	(0x6E0C + (0x20 * (i)))
+
+/* Forwarding Engine Port Based Forwarded Descriptor CouNter i */
+#define RSW2_FWD_FWPBFDCN(i)	(0x6310 + (0x20 * (i)))
+
+/* Forwarding Engine Port Based Forwarded Descriptor CouNter Emu i */
+#define RSW2_FWD_FWPBFDCNE(i)	(0x6E10 + (0x20 * (i)))
+
+/* Forwarding Engine MAC Hardware Learn CouNter i */
+#define RSW2_FWD_FWMHLCN(i)		(0x6314 + (0x20 * (i)))
+
+/* Forwarding Engine MAC Hardware Learn CouNter Emu i */
+#define RSW2_FWD_FWMHLCNE(i)	(0x6E14 + (0x20 * (i)))
+
+
+#define RSW2_FWD_FWWMRDCN(i)	(0x6504 + (0x20 * (i))) /* Forwarding Engine WaterMark Rejected Descriptor CouNter i */
+#define RSW2_FWD_FWWMRDCNE(i)	(0x7004 + (0x20 * (i))) /* Forwarding Engine WaterMark Rejected Descriptor CouNter Emu i */
+#define RSW2_FWD_FWDDRDCN(i)	(0x6508 + (0x20 * (i))) /* Forwarding Engine Direct Descriptor Rejected Descriptor CouNter i */
+#define RSW2_FWD_FWDDRDCNE(i)	(0x7008 + (0x20 * (i))) /* Forwarding Engine Direct Descriptor Rejected Descriptor CouNter Emu i */
+#define RSW2_FWD_FWLTHRDCN(i)	(0x650C + (0x20 * (i))) /* Forwarding Engine Layer 3 Rejected Descriptor CouNter i */
+#define RSW2_FWD_FWLTHRDCNE(i)	(0x700C + (0x20 * (i))) /* Forwarding Engine Layer 3 Rejected Descriptor CouNter Emu i*/
+#define RSW2_FWD_FWLTWRDCN(i)	(0x6514 + (0x20 * (i))) /* Forwarding Engine Layer 2 Rejected Descriptor CouNter i */
+#define RSW2_FWD_FWLTWRDCNE(i)	(0x7014 + (0x20 * (i))) /* Forwarding Engine Layer 2 Rejected Descriptor CouNter Emu i */
+#define RSW2_FWD_FWPBRDCN(i)	(0x6518 + (0x20 * (i))) /* Forwarding Engine Port Based Rejected Descriptor CouNter i */
+#define RSW2_FWD_FWPBRDCNE(i)	(0x7018 + (0x20 * (i))) /* Forwarding Engine Port Based Rejected Descriptor CouNter Emu i */
+
+#define RSW2_FWD_FWPMFDCN(i)	(0x6700 + (0x4 * (i))) /* Forwarding Engine PSFP MSDU Filtered Descriptor CouNter i*/
+#define RSW2_FWD_FWPMFDCNE(i)	(0x7200 + (0x4 * (i))) /* Forwarding Engine PSFP MSDU Filtered Descriptor CouNter Emu i*/
+#define RSW2_FWD_FWPGFDCN(i)	(0x6780 + (0x4 * (i))) /* Forwarding Engine PSFP Gate Filtered Descriptor CouNter i */
+#define RSW2_FWD_FWPGFDCNE(i)	(0x7280 + (0x4 * (i))) /* Forwarding Engine PSFP Gate Filtered Descriptor CouNter Emu i */
+#define RSW2_FWD_FWPMGDCN(i)	(0x6800 + (0x10 * (i))) /* Forwarding Engine PSFP Meter Green Descriptor CouNter i */
+#define RSW2_FWD_FWPMGDCNE(i)	(0x7300 + (0x10 * (i))) /* Forwarding Engine PSFP Meter Green Descriptor CouNter Emu i*/
+#define RSW2_FWD_FWPMYDCN(i)	(0x6804 + (0x10 * (i))) /* Forwarding Engine PSFP Meter Yellow Descriptor CouNter i */
+#define RSW2_FWD_FWPMYDCNE(i)	(0x7304 + (0x10 * (i))) /* Forwarding Engine PSFP Meter Yellow Descriptor CouNter Emu i */
+#define RSW2_FWD_FWPMRDCN(i)	(0x6808 + (0x10 * (i))) /* Forwarding Engine PSFP Meter Red Descriptor CouNter i */
+#define RSW2_FWD_FWPMRDCNE(i)	(0x7308 + (0x10 * (i))) /* Forwarding Engine PSFP Meter Red Descriptor CouNter Emu i */
+
+
+/* Forwarding Engine FRER Passed Packet CouNter i (i=0..FRER_RECE_N-1) */
+#define RSW2_FWD_FWFRPPCN(i)	(0x6A00 + (0x8 * (i)))
+/* Forwarding Engine FRER Passed Packet CouNter Emu i (i=0..FRER_RECE_N-1) */
+#define RSW2_FWD_FWFRPPCNE(i)	(0x7500 + (0x8 * (i)))
+
+/* Forwarding Engine FRER Discarded Packet CouNter i (i=0..FRER_RECE_N-1) */
+#define RSW2_FWD_FWFRDPCN(i)	(0x6A04 + (0x8 * (i)))
+/* Forwarding Engine FRER Discarded Packet CouNter Emu i (i=0..FRER_RECE_N-1) */
+#define RSW2_FWD_FWFRDPCNE(i)	(0x7504 + (0x8 * (i)))
+
+/* Forwarding Engine Error Interrupt Status 0 i */
+#define RSW2_FWD_FWEIS0(i)	(0x7900 + (0x10 * (i)))
+
+/* Forwarding Engine Error Interrupt Enable 0 i */
+#define RSW2_FWD_FWEIE0(i)	(0x7904 + (0x10 * (i)))
+
+/* Forwarding Engine Error Interrupt Disable 0 i (i=0..PORT_N) */
+#define RSW2_FWD_FWEID0(i)	(0x7908 + (0x10 * (i)))
+
+#define RSW2_FWD_FWEIS1		0x7A00 /* Forwarding Engine Error Interrupt Status 1 */
+#define RSW2_FWD_FWEIE1		0x7A04 /* Forwarding Engine Error Interrupt Enable 1 */
+#define RSW2_FWD_FWEID1		0x7A08 /* Forwarding Engine Error Interrupt Disable 1 */
+#define RSW2_FWD_FWEIS2		0x7A10 /* Forwarding Engine Error Interrupt Status 2 */
+#define RSW2_FWD_FWEIE2		0x7A14 /* Forwarding Engine Error Interrupt Enable 2 */
+#define RSW2_FWD_FWEID2		0x7A18 /* Forwarding Engine Error Interrupt Disable 2 */
+#define RSW2_FWD_FWEIS3		0x7A20 /* Forwarding Engine Error Interrupt Status 3 */
+#define RSW2_FWD_FWEIE3		0x7A24 /* Forwarding Engine Error Interrupt Enable 3 */
+#define RSW2_FWD_FWEID3		0x7A28 /* Forwarding Engine Error Interrupt Disable 3 */
+#define RSW2_FWD_FWEIS4		0x7A30 /* Forwarding Engine Error Interrupt Status 4 */
+#define RSW2_FWD_FWEIE4		0x7A34 /* Forwarding Engine Error Interrupt Enable 4 */
+#define RSW2_FWD_FWEID4		0x7A38 /* Forwarding Engine Error Interrupt Disable 4 */
+#define RSW2_FWD_FWEIS5		0x7A40 /* Forwarding Engine Error Interrupt Status 5 */
+#define RSW2_FWD_FWEIE5		0x7A44 /* Forwarding Engine Error Interrupt Enable 5 */
+#define RSW2_FWD_FWEID5		0x7A48 /* Forwarding Engine Error Interrupt Disable 5 */
+#define RSW2_FWD_FWMIS0		0x7C00 /* Forwarding Engine Monitoring Interrupt Status 0 */
+#define RSW2_FWD_FWMIE0		0x7C04 /* Forwarding Engine Monitoring Interrupt Enable 0 */
+#define RSW2_FWD_FWMID0		0x7C08 /* Forwarding Engine Monitoring Interrupt Disable 0 */
+#define RSW2_FWD_FWSCR0		0x7D00 /* Forwarding Engine Security Configuration Register 0 */
+#define RSW2_FWD_FWSCR1		0x7D04 /* Forwarding Engine Security Configuration Register 1 */
+#define RSW2_FWD_FWSCR2		0x7D08 /* Forwarding Engine Security Configuration Register 2 */
+#define RSW2_FWD_FWSCR3		0x7D0C /* Forwarding Engine Security Configuration Register 3 */
+#define RSW2_FWD_FWSCR4		0x7D10 /* Forwarding Engine Security Configuration Register 4 */
+#define RSW2_FWD_FWSCR5		0x7D14 /* Forwarding Engine Security Configuration Register 5 */
+#define RSW2_FWD_FWSCR6		0x7D18 /* Forwarding Engine Security Configuration Register 6 */
+#define RSW2_FWD_FWSCR7		0x7D1C /* Forwarding Engine Security Configuration Register 7 */
+#define RSW2_FWD_FWSCR8		0x7D20 /* Forwarding Engine Security Configuration Register 8 */
+#define RSW2_FWD_FWSCR9		0x7D24 /* Forwarding Engine Security Configuration Register 9 */
+#define RSW2_FWD_FWSCR10	0x7D28 /* Forwarding Engine Security Configuration Register 10 */
+#define RSW2_FWD_FWSCR11	0x7D2C /* Forwarding Engine Security Configuration Register 11 */
+#define RSW2_FWD_FWSCR12	0x7D30 /* Forwarding Engine Security Configuration Register 12 */
+#define RSW2_FWD_FWSCR13	0x7D34 /* Forwarding Engine Security Configuration Register 13 */
+#define RSW2_FWD_FWSCR14	0x7D38 /* Forwarding Engine Security Configuration Register 14 */
+#define RSW2_FWD_FWSCR15	0x7D3C /* Forwarding Engine Security Configuration Register 15 */
+#define RSW2_FWD_FWSCR16	0x7D40 /* Forwarding Engine Security Configuration Register 16 */
+#define RSW2_FWD_FWSCR17	0x7D44 /* Forwarding Engine Security Configuration Register 17 */
+#define RSW2_FWD_FWSCR18	0x7D48 /* Forwarding Engine Security Configuration Register 18 */
+#define RSW2_FWD_FWSCR19	0x7D4C /* Forwarding Engine Security Configuration Register 19 */
+#define RSW2_FWD_FWSCR20	0x7D50 /* Forwarding Engine Security Configuration Register 20 */
+#define RSW2_FWD_FWSCR21	0x7D54 /* Forwarding Engine Security Configuration Register 21 */
+#define RSW2_FWD_FWSCR22	0x7D58 /* Forwarding Engine Security Configuration Register 22 */
+#define RSW2_FWD_FWSCR23	0x7D5C /* Forwarding Engine Security Configuration Register 23 */
+#define RSW2_FWD_FWSCR25	0x7D64 /* Forwarding Engine Security Configuration Register 25 */
+#define RSW2_FWD_FWSCR26	0x7D68 /* Forwarding Engine Security Configuration Register 26 */
+#define RSW2_FWD_FWSCR27	0x7D6C /* Forwarding Engine Security Configuration Register 27 */
+#define RSW2_FWD_FWSCR28	0x7D70 /* Forwarding Engine Security Configuration Register 28 */
+#define RSW2_FWD_FWSCR29	0x7D74 /* Forwarding Engine Security Configuration Register 29 */
+#define RSW2_FWD_FWSCR30	0x7D78 /* Forwarding Engine Security Configuration Register 30 */
+#define RSW2_FWD_FWSCR31	0x7D7C /* Forwarding Engine Security Configuration Register 31 */
+#define RSW2_FWD_FWSCR32	0x7D80 /* Forwarding Engine Security Configuration Register 32 */
+#define RSW2_FWD_FWSCR33	0x7D84 /* Forwarding Engine Security Configuration Register 33 */
+#define RSW2_FWD_FWSCR34	0x7D88 /* Forwarding Engine Security Configuration Register 34 */
+#define RSW2_FWD_FWSCR35	0x7D8C /* Forwarding Engine Security Configuration Register 35 */
+#define RSW2_FWD_FWSCR36	0x7D90 /* Forwarding Engine Security Configuration Register 36 */
+#define RSW2_FWD_FWSCR37	0x7D94 /* Forwarding Engine Security Configuration Register 37 */
+#define RSW2_FWD_FWSCR38	0x7D98 /* Forwarding Engine Security Configuration Register 38 */
+#define RSW2_FWD_FWSCR39	0x7D9C /* Forwarding Engine Security Configuration Register 39 */
+#define RSW2_FWD_FWSCR40	0x7DA0 /* Forwarding Engine Security Configuration Register 40 */
+
+
+int rswitch2_fwd_init(struct rswitch2_drv *rsw2);
+void rswitch2_fwd_exit(struct rswitch2_drv *rsw2);
+
+int rsw2_fwd_del_l2_entry(struct rswitch2_drv *rsw2, const u8 *macaddr);
+int rsw2_fwd_add_l2_entry(struct rswitch2_drv *rsw2, const u8 *macaddr, u32 src_port_vec, u32 dest_port_vec, u32 cpu_q);
+
+#endif /* _RSWITCH2_FWD_H */
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2_gwca.h b/drivers/net/ethernet/renesas/rswitch2/rswitch2_gwca.h
new file mode 100644
index 000000000000..de5097808e3c
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2_gwca.h
@@ -0,0 +1,513 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Renesas RSwitch2 Gateway Common Agent device driver
+ *
+ * Copyright (C) 2021 Renesas Electronics Corporation
+ *
+ */
+#ifndef _RSWITCH2_GWCA_H
+#define _RSWITCH2_GWCA_H
+
+#include <linux/bits.h>
+
+#define RSW2_GWCA_FRM_PRIO_N	8		/* Priority number handled by GWCA */
+#define RSW2_PTP_TN				2		/* gPTP timer number connected to the switch */
+#define RSW2_AXI_CHAIN_N		128	/* AXI Descriptor Chain number */
+#define RSW2_AXI_RINC_N			8		/* RX incremental descriptor Chain number */
+#define RSW2_AXI_TLIM_N			32	/* TX rate limiter number */
+#define RSW2_PAS_LVL_N			2		/* Pause level number */
+
+
+#define RSW2_GCWA_GWMC			0x0000 /* GWCA Mode Configuration */
+#define GWMC_OPC				GENMASK(2, 0)
+
+#define RSW2_GCWA_GWMS			0x0004 /* GWCA Mode Status */
+#define GWMS_OPS				GENMASK(2, 0)
+
+enum gwmc_op {
+	gwmc_reset		= 0,
+	gwmc_disable	= 1,
+	gwmc_config		= 2,
+	gwmc_operation	= 3,
+};
+
+#define RSW2_GCWA_GWIRC			0x0010 /* GWCA IPV remapping configuration */
+#define GWIRC_IPVR(i)			(7 << (i * 4))
+
+#define RSW2_GCWA_GWRDQSC		0x0014 /* GWCA RX Descriptor Queue Security Configuration */
+#define GWRDQSC_RDQSL(i)		BIT_MASK(i)
+
+#define RSW2_GCWA_GWRDQC		0x0018 /* GWCA RX Descriptor Queue Control */
+#define GWRDQC_RDQD(i)			BIT_MASK(i)
+#define GWRDQC_RDQP(i)			(BIT_MASK(i) << 16)
+
+#define RSW2_GCWA_GWRDQAC		0x001C /* GWCA RX Descriptor Queue Arbitration Control */
+#define GWRDQAC_RDQA(i)			((0xF) << (i * 4))
+
+#define RSW2_GCWA_GWRGC			0x0020 /* GWCA RX General Configuration */
+#define GWRGC_RCPT				BIT_MASK(0)
+
+/* GWCA Reception Maximum Frame Size Configuration q (q=0.. FRM_PRIO_N-1) */
+#define RSW2_GCWA_GWRMFSC(q)	(0x0040 + 4 * (q))
+#define GWRMFSC_MFS				GENMASK(15, 0)
+
+/* GWCA Reception Descriptor Queue Depth Configuration q (q=0.. FRM_PRIO_N-1) */
+#define RSW2_GCWA_GWRDQDC(q)	(0x0060 + (4 * (q)))
+#define GWRDQDC_DQD				GENMASK(10, 0)
+
+/* GWCA RX Descriptor Queue q Monitoring (q=0.. FRM_PRIO_N-1) */
+#define RSW2_GCWA_GWRDQM(q)		(0x0080 + (4 * (q)))
+#define GWRDQM_DNQ				GENMASK(10, 0)
+
+
+/* GWCA RX Descriptor Queue q Max Level Monitoring (q=0.. FRM_PRIO_N-1) */
+#define RSW2_GCWA_GWRDQMLM(q)	(0x00A0 + (4 * (q)))
+
+/* GWCA RX Descriptor Queue q Max Level Monitoring EMU (q=0.. FRM_PRIO_N-1) */
+#define RSW2_GCWA_GWRDQMLME(q)	(0x00C0 + (4 * (q)))
+#define GWRDQM_DMLQ				GENMASK(10, 0)
+
+#define RSW2_GCWA_GWMTIRM		0x0100 /* GWCA Multicast Table Initialization Reg. Monitoring */
+#define GWMTIRM_MTR				BIT_MASK(1)
+#define GWMTIRM_MTIOG			BIT_MASK(0)
+
+#define RSW2_GCWA_GWMSTLS		0x0104 /* GWCA Multicast Table Learning Setting */
+#define GWMSTLS_MSENL			GENMASK(22, 16)
+#define GWMSTLS_MNL				GENMASK(10, 8)
+#define GWMSTLS_MNRCNL			GENMASK(6, 0)
+
+#define RSW2_GCWA_GWMSTLR		0x0108 /* GWCA Multicast Table Learning Result */
+#define GWMSTLR_MTL				BIT_MASK(31)
+#define GWMSTLR_MTLSF			BIT_MASK(1)
+#define GWMSTLR_MTLF			BIT_MASK(0)
+
+#define RSW2_GCWA_GWMSTSS		0x010C /* GWCA Multicast Table Searching Setting */
+#define GWMSTSS_MSENS			GENMASK(6, 0)
+
+#define RSW2_GCWA_GWMSTSR		0x0110 /* GWCA Multicast Table Searching Result */
+#define GWMSTSR_MTS				BIT_MASK(31)
+#define GWMSTSR_MTSEF			BIT_MASK(16)
+#define GWMSTSR_MNR				GENMASK(10, 8)
+#define GWMSTSR_MNRCNR			BIT_MASK(0)
+
+#define RSW2_GCWA_GWMAC0		0x0120 /* GWCA MAC address configuration 0 */
+#define GWMAC0_MAUP				GENMASK(15, 0)
+
+#define RSW2_GCWA_GWMAC1		0x0124 /* GWCA MAC address configuration 1 */
+#define GWMAC1_MADP				GENMASK(31, 0)
+
+#define RSW2_GCWA_GWVCC			0x0130 /* GWCA VLAN control configuration */
+#define GWVCC_VEM				GENMASK(18, 16)
+#define GWVCC_VIM				BIT_MASK(0)
+
+enum gwvcc_vlan_egress_mode {
+	no_vlan		= 0,
+	ctag		= 1,
+	hw_ctag		= 2,
+	sc_ctag		= 3,
+	hw_sc_ctag	= 4,
+};
+
+#define RSW2_GCWA_GWVTC			0x0134 /* GWCA VLAN TAG configuration */
+#define GWVTC_STD				BIT_MASK(31)
+#define GWVTC_STP				GENMASK(30, 28)
+#define GWVTC_STV				GENMASK(27, 16)
+#define GWVTC_CTD				BIT_MASK(15)
+#define GWVTC_CTP				GENMASK(14, 12)
+#define GWVTC_CTV				GENMASK(11, 0)
+
+#define RSW2_GCWA_GWTTFC		0x0138 /* GWCA Transmission TAG Filtering Configuration */
+#define GWTTFC_UT				BIT_MASK(8)
+#define GWTTFC_SCRT				BIT_MASK(7)
+#define GWTTFC_SCT				BIT_MASK(6)
+#define GWTTFC_CRT				BIT_MASK(5)
+#define GWTTFC_CT				BIT_MASK(4)
+#define GWTTFC_CSRT				BIT_MASK(3)
+#define GWTTFC_CST				BIT_MASK(2)
+#define GWTTFC_RT				BIT_MASK(1)
+#define GWTTFC_NT				BIT_MASK(0)
+
+/* GWCA TimeStamp Descriptor Chain Address Configuration 0 s (s=0 to PTP_TN-1) */
+#define RSW2_GCWA_GWTDCAC0(s)	(0x0140 + (8 * (s)))
+#define GWTDCAC0_TSCCAUP		GENMASK(7, 0)
+
+/* GWCA TimeStamp Descriptor Chain Address Configuration 1 s (s=0 to PTP_TN-1) */
+#define RSW2_GCWA_GWTDCAC1(s)	(0x0144 + (8 * (s)))
+
+/* GWCA TimeStamp Descriptor Chain Configuration s (s=0 to PTP_TN-1) */
+#define RSW2_GCWA_GWTSDCC(s)	(0x0160 + (4 * (s)))
+#define GWTSDCC_OSID			GENMASK(10, 8)
+#define GWTSDCC_DCS				BIT_MASK(1)
+#define GWTSDCC_TE				BIT_MASK(0)
+
+#define RSW2_GCWA_GWTSNM		0x0180 /* GWCA Agent TimeStamp Number Monitoring */
+
+#define RSW2_GCWA_GWTSMNM		0x0184 /* GWCA Agent TimeStamp Maximum Number Monitoring     */
+#define RSW2_GCWA_GWTSMNME		0x0188 /* GWCA Agent TimeStamp Maximum Number Monitoring Emu */
+#define GWTSMNM_TNTR			GENMASK(7, 0)
+
+#define RSW2_GCWA_GWAC			0x0190 /* GWCA AXI Control */
+#define GWAC_AMP				BIT_MASK(1)
+#define GWAC_AMPR				BIT_MASK(0)
+
+#define RSW2_GCWA_GWDCBAC0		0x0194 /* GWCA Descriptor Chain Base Address Configuration 0 */
+#define GWDCBAC0_DCBAUP			GENMASK(7, 0)
+
+#define RSW2_GCWA_GWDCBAC1		0x0198 /* GWCA Descriptor Chain Base Address Configuration 1 */
+#define GWDCBAC1_DCBADP			GENMASK(31, 0)
+
+#define RSW2_GCWA_GWMDNC		0x01A0 /* GWCA Maximum Descriptor Number Configuration */
+#define GWMDNC_TSDMN			GENMASK(17, 16)
+#define GWMDNC_TXDMN			GENMASK(12, 8)
+#define GWMDNC_RXDMN			GENMASK(4, 0)
+
+
+/* GWCA AXI Transmission Request Configuration i ( i = 0 to AXI_CHAIN_N/32-1) */
+#define RSW2_GCWA_GWTRC(i)		(0x0200 + (4 * (i)))
+#define GWTRC_TSR(t)			BIT_MASK(t)
+
+/*  GWCA Transmission Pause Configuration p ( p = 0 to PAS_LVL_N1-1) */
+#define RSW2_GCWA_GWTPC(p)		(0x0300 + (4 * (p)))
+#define GWTPC_PPPL				GENMASK(7, 0)
+
+#define RSW2_GCWA_GWARIRM		0x0380 /* GWCA AXI RAM Initialization Register Monitoring */
+#define GWARIRM_ARR				BIT_MASK(1)
+#define GWARIRM_ARIOG			BIT_MASK(0)
+
+/* GWCA AXI Descriptor chain Configuration i ( i = 0 to AXI_CHAIN_N-1) */
+#define RSW2_GCWA_GWDCC(i)		(0x0400 + (4 * (i)))
+#define GWDCC_OSID				GENMASK(31, 28)
+#define GWDCC_BALR				BIT_MASK(24)
+#define GWDCC_DCP				GENMASK(18, 16)
+#define GWDCC_DQT				BIT_MASK(11)
+#define GWDCC_SL				BIT_MASK(10)
+#define GWDCC_ETS				BIT_MASK(9)
+#define GWDCC_EDE				BIT_MASK(8)
+#define GWDCC_SM				GENMASK(1, 0)
+
+#define RSW2_GCWA_GWAARSS		0x0800 /* GWCA AXI Address RAM Searching Setting */
+#define GWAARSS_AARA			GENMASK(6, 0)
+
+#define RSW2_GCWA_GWAARSR0		0x0804 /* GWCA AXI Address RAM Searching Result 0 */
+#define GWAARSR0_AARS			BIT_MASK(31)
+#define GWAARSR0_AARSSF			BIT_MASK(17)
+#define GWAARSR0_AARSEF			BIT_MASK(16)
+#define GWAARSR0_ACARUP			GENMASK(7, 0)
+
+#define RSW2_GCWA_GWAARSR1		0x0808 /* GWCA AXI Address RAM Searching Result 1 */
+#define GWAARSR1_ACARDP			GENMASK(31, 0)
+
+/* GWCA Incremental Data Area Used Area Size i ( i=0 to AXI_RINC_N -1) */
+#define RSW2_GCWA_GWIDAUAS(i)	(0x0840 + (4 * (i)))
+#define GWIDAUAS_IDAUAS			GENMASK(23, 0)
+
+/* GWCA Incremental Data Area Size Monitoring i ( i=0 to AXI_RINC_N -1) */
+#define RSW2_GCWA_GWIDASM(i)	(0x0880 + (4 * (i)))
+#define GWIDASM_IDAS			GENMASK(23, 0)
+
+/* GWCA Incremental Data Area Start Address Monitoring 0 i ( i=0 to AXI_RINC_N -1) */
+#define RSW2_GCWA_GWIDASAM0(i)	(0x0900 + (8 * (i)))
+#define GWIDASAM0_IDASAUP		GENMASK(7, 0)
+
+/* GWCA Incremental Data Area Start Address Monitoring 1 i ( i=0 to AXI_RINC_N -1) */
+#define RSW2_GCWA_GWIDASAM1(i)	(0x0904 + (8 * (i)))
+#define GWIDASAM1_IDASADPi		GENMASK(31, 0)
+
+/* GWCA Incremental Data Area Current Address Monitoring 0 i ( i=0 to AXI_RINC_N -1) */
+#define RSW2_GCWA_GWIDACAM0(i)	(0x0980 + (8 * (i)))
+#define GWIDACAM0_IDACAUP		GENMASK(7, 0)
+
+/*  GWCA Incremental Data Area Current Address Monitoring 1 i ( i=0 to AXI_RINC_N -1) */
+#define RSW2_GCWA_GWIDACAM1(i)	(0x0984 + (8 * (i)))
+#define GWIDACAM1_IDACADP		GENMASK(31, 0)
+
+#define RSW2_GCWA_GWGRLC		0x0A00 /* GWCA Global Rate Limiter Configuration */
+#define GWGRLC_GRLULRS			BIT_MASK(17)
+#define GWGRLC_GRLE				BIT_MASK(16)
+#define GWGRLC_GRLIV			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWGRLULC		0x0A04 /* GWCA Global Rate Limiter Upper Limit Configuration */
+#define GWGRLULC_GRLUL			GENMASK(23, 0)
+
+/*  GWCA Rate limiter incremental value configuration i (i = 0 to AXI_TLIM_N-1) */
+#define RSW2_GCWA_GWRLC(i)		(0x0A80 + (8 * (i)))
+#define GWRLC_RLE				BIT_MASK(16)
+#define GWRLC_RLIV				GENMASK(11, 0)
+
+/*  GWCA Rate limiter upper limit configuration i (i = 0 to AXI_TLIM_N-1) */
+#define RSW2_GCWA_GWRLULC(i)	(0x0A84 + (8 * (i)))
+#define GWRLULC_RLUL			GENMASK(23, 0)
+
+#define RSW2_GCWA_GWIDPC		0x0B80 /* GWCA Interrupt Delay Prescaler Configuration */
+#define GWIDPC_IDPV				GENMASK(9, 0)
+
+/* GWCA Interrupt Delay Configuration i ( i = 0 to AXI_CHAIN_N-1) */
+#define RSW2_GCWA_GWIDC(i)		(0x0C00 + (4 * (i)))
+#define GWIDC_IDV				GENMASK(11, 0)
+
+
+#define RSW2_GCWA_GWRDCN		0x1000 /* GWCA Received Data CouNter */
+#define GWRDCN_RDN				GENMMASK(31, 0)
+
+
+#define RSW2_GCWA_GWRDCNE		0x1080 /* GWCA Received Data CouNter */
+#define GWRDCNE_TDN				GENMMASK(31, 0)
+
+#define RSW2_GCWA_GWTDCN		0x1004 /* GWCA Transmitted Data CouNter */
+#define GWTDCN_TN				GENMMASK(31, 0)
+
+#define RSW2_GCWA_GWTDCNE		0x1084 /* GWCA Transmitted Data CouNter */
+#define GWTDCNE_TSOVFEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWTSCN		0x1008 /* GWCA TimeStamp CouNter */
+#define GWTSCN_USMFSEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWTSCNE		0x1088 /* GWCA TimeStamp CouNter */
+#define GWTSCNE_TFEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWTSOVFECN	0x100C /* GWCA TimeStamp OVerFlow Error CouNter */
+#define GWTSOVFECN_SEQEN		GENMASK(15, 0)
+
+#define RSW2_GCWA_GWTSOVFECNE	0x108C /* GWCA TimeStamp OVerFlow Error CouNter */
+#define GWTSOVFECNE_SEQEN		GENMASK(15, 0)
+
+#define RSW2_GCWA_GWUSMFSECN	0x1010 /* GWCA Under Switch Minimum Frame Size Error CouNter */
+#define GWUSMFSECN_TXDNEN		GENMASK(15, 0)
+
+#define RSW2_GCWA_GWUSMFSECNE	0x1090 /* GWCA Under Switch Minimum Frame Size Error CouNter */
+#define GWUSMFSECNE_FSEN		GENMASK(15, 0)
+
+#define RSW2_GCWA_GWTFECN		0x1014 /* GWCA TAG Filtering Error CouNter */
+#define RSW2_GCWA_GWTFECNE		0x1094 /* GWCA TAG Filtering Error CouNter Emu */
+#define GWTFECN_TDFEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWSEQECN		0x1018 /* GWCA SEQuence Error CouNter */
+#define RSW2_GCWA_GWSEQECNE		0x1098 /* GWCA SEQuence Error CouNter Emu */
+#define GWSEQECN_SEQEN			GENMASK(15, 0)
+
+
+#define RSW2_GCWA_GWTXDNECN		0x1020 /* GWCA TX Descriptor Number Error CouNter */
+#define RSW2_GCWA_GWTXDNECNE	0x10A0 /* GWCA TX Descriptor Number Error CouNter Emu */
+#define GWTXDNECN_TXDNEN		GENMASK(15, 0)
+
+#define RSW2_GCWA_GWFSECN		0x1024 /* GWCA Frame Size Error CouNter */
+#define RSW2_GCWA_GWFSECNE		0x10A4 /* GWCA Frame Size Error CouNter Emu */
+#define GWFSECN_FSEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWTDFECN		0x1028 /* GWCA Timestamp Descriptor Full Error CouNter */
+#define RSW2_GCWA_GWTDFECNE		0x10A8 /* GWCA Timestamp Descriptor Full Error CouNter Emu */
+#define GWTDFECN_TDFEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWTSDNECN		0x102C /* GWCA Timestamp Descriptor Number Error CouNter */
+#define RSW2_GCWA_GWTSDNECNE	0x10AC /* GWCA Timestamp Descriptor Number Error CouNter Emu */
+#define GWTSDNECN_TSDNEN		GENMASK(15, 0)
+
+#define RSW2_GCWA_GWDQOECN		0x1030 /* GWCA Descriptor Queue Overflow Error CouNter */
+#define RSW2_GCWA_GWDQOECNE		0x10B0 /* GWCA Descriptor Queue Overflow Error CouNter Emu */
+#define GWDQOECN_DQOEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWDQSECN		0x1034 /* GWCA Descriptor Queue Security Error CouNter */
+#define RSW2_GCWA_GWDQSECNE		0x10B4 /* GWCA Descriptor Queue Security Error CouNter Emu */
+#define GWDQSECN_DQSEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWDFECN		0x1038 /* GWCA Descriptor Full Error CouNter */
+#define RSW2_GCWA_GWDFECNE		0x10B8 /* GWCA Descriptor Full Error CouNter Emu */
+#define GWDFECN_DFEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWDSECN		0x103C /* GWCA Descriptor Security Error CouNter */
+#define RSW2_GCWA_GWDSECNE		0x10BC /* GWCA Descriptor Security Error CouNter Emu */
+#define GWDSECN_DSEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWDSZECN		0x1040 /* GWCA Data SiZe Error CouNter */
+#define RSW2_GCWA_GWDSZECNE		0x10C0 /* GWCA Data SiZe Error CouNter Emu */
+#define GWDSZECN_DSZEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWDCTECN		0x1044 /* GWCA Descriptor Chain Type Error CouNter */
+#define RSW2_GCWA_GWDCTECNE		0x10C4 /* GWCA Descriptor Chain Type Error CouNter Emu */
+#define GWDCTECN_DCTEN			GENMASK(15, 0)
+
+#define RSW2_GCWA_GWRXDNECN		0x1048 /* GWCA RX Descriptor Number Error CouNter */
+#define RSW2_GCWA_GWRXDNECNE	0x10C8 /* GWCA RX Descriptor Number Error CouNter Emu */
+#define GWRXDNECN_RXDNEN		GENMASK(15, 0)
+
+/* GWCA Data Interrupt Status i ( i = 0 to AXI_CHAIN_N/32-1) */
+#define RSW2_GCWA_GWDIS(i)		(0x1100 + (0x10 * (i)))
+#define GWDIS_DIS(t)			BIT_MASK(t)
+
+/* GWCA Data Interrupt Enable i ( i = 0 to AXI_CHAIN_N/32-1) */
+#define RSW2_GCWA_GWDIE(i)		(0x1104 + (0x10 * (i)))
+#define GWDIE_DIE(t)			BIT_MASK(t)
+
+/*  GWCA Data Interrupt Disable i ( i = 0 to AXI_CHAIN_N/32-1) */
+#define RSW2_GCWA_GWDID(i)		(0x1108 + (0x10 * (i)))
+#define GWDID_DID(t)			BIT_MASK(t)
+
+/* GWCA Data Interrupt Disable i ( i = 0 to AXI_CHAIN_N/32-1) */
+#define RSW2_GCWA_GWDIDS(i)		(0x110C + (0x10 * (i)))
+#define GWDIDS_DIDS(t)			BIT_MASK(t)
+
+#define RSW2_GCWA_GWTSDIS		0x1180 /* GWCA TimeStamp Data Interrupt Status */
+#define GWTSDIS_TSDIS			GENMASK(1, 0)
+
+#define RSW2_GCWA_GWTSDIE		0x1184 /* GWCA TimeStamp Data Interrupt Enable */
+#define GWTSDIE_TSDIE			GENMASK(1, 0)
+
+#define RSW2_GCWA_GWTSDID		0x1188 /* GWCA TimeStamp Data Interrupt Disable */
+#define GWTSDID_TSDID			GENMASK(1, 0)
+
+#define RSW2_GCWA_GWEIS0		0x1190 /* GWCA Error Interrupt Status 0 */
+#define GWEIS0_TSDNES			GENMASK(29, 28)
+#define GWEIS0_TDFES			GENMASK(25, 24)
+#define GWEIS0_FSES				GENMASK(23, 16)
+#define GWEIS0_TSHES			BIT_MASK(15)
+#define GWEIS0_TXDNES			BIT_MASK(14)
+#define GWEIS0_SEQES			BIT_MASK(12)
+#define GWEIS0_TFES				BIT_MASK(11)
+#define GWEIS0_USMFSES			BIT_MASK(10)
+#define GWEIS0_TSOVFES			BIT_MASK(9)
+#define GWEIS0_L23UECCES		BIT_MASK(8)
+#define GWEIS0_TSECCES			BIT_MASK(7)
+#define GWEIS0_AECCES			BIT_MASK(6)
+#define GWEIS0_MECCES			BIT_MASK(5)
+#define GWEIS0_DSECCES			BIT_MASK(4)
+#define GWEIS0_PECCES			BIT_MASK(3)
+#define GWEIS0_TECCES			BIT_MASK(2)
+#define GWEIS0_DECCES			BIT_MASK(1)
+#define GWEIS0_AES				BIT_MASK(0)
+
+#define RSW2_GCWA_GWEIE0		0x1194 /* GWCA Error Interrupt Enable 0 */
+#define GWEIE0_TSDNEE			GENMASK(29, 28)
+#define GWEIE0_TDFEE			GENMASK(25, 24)
+#define GWEIE0_FSEE				GENMASK(23, 16)
+#define GWEIE0_TSHEE			BIT_MASK(15)
+#define GWEIE0_TXDNEE			BIT_MASK(14)
+#define GWEIE0_SEQEE			BIT_MASK(12)
+#define GWEIE0_TFEE				BIT_MASK(11)
+#define GWEIE0_USMFSEE			BIT_MASK(10)
+#define GWEIE0_TSOVFEE			BIT_MASK(9)
+#define GWEIE0_L23UECCEE		BIT_MASK(8)
+#define GWEIE0_TSECCEE			BIT_MASK(7)
+#define GWEIE0_AECCEE			BIT_MASK(6)
+#define GWEIE0_MECCEE			BIT_MASK(5)
+#define GWEIE0_DSECCEE			BIT_MASK(4)
+#define GWEIE0_PECCEE			BIT_MASK(3)
+#define GWEIE0_TECCEE			BIT_MASK(2)
+#define GWEIE0_DECCEE			BIT_MASK(1)
+#define GWEIE0_AEE				BIT_MASK(0)
+
+
+#define RSW2_GCWA_GWEID0		0x1198 /* GWCA Error Interrupt Disable 0 */
+#define GWEID0_TSDNED			GENMASK(29, 28)
+#define GWEID0_TDFED			GENMASK(25, 24)
+#define GWEID0_FSED				GENMASK(23, 16)
+#define GWEID0_TSHED			BIT_MASK(15)
+#define GWEID0_TXDNED			BIT_MASK(14)
+#define GWEID0_SEQED			BIT_MASK(12)
+#define GWEID0_TFED				BIT_MASK(11)
+#define GWEID0_USMFSED			BIT_MASK(10)
+#define GWEID0_TSOVFED			BIT_MASK(9)
+#define GWEID0_L23UECCED		BIT_MASK(8)
+#define GWEID0_TSECCED			BIT_MASK(7)
+#define GWEID0_AECCED			BIT_MASK(6)
+#define GWEID0_MECCED			BIT_MASK(5)
+#define GWEID0_DSECCED			BIT_MASK(4)
+#define GWEID0_PECCED			BIT_MASK(3)
+#define GWEID0_TECCED			BIT_MASK(2)
+#define GWEID0_DECCED			BIT_MASK(1)
+#define GWEID0_AED				BIT_MASK(0)
+
+#define RSW2_GCWA_GWEIS1		0x11A0 /* GWCA Error Interrupt Status 1 */
+#define GWEIS1_DQSES			GENMASK(23, 16)
+#define GWEIS1_DQOES			GENMASK(7, 0)
+
+#define RSW2_GCWA_GWEIE1		0x11A4 /* GWCA Error Interrupt Enable 1 */
+#define GWEIE1_DQSEE			GENMASK(23, 16)
+#define GWEIE1_DQOEE			GENMASK(7, 0)
+
+#define RSW2_GCWA_GWEID1		0x11A8 /* GWCA Error Interrupt Disable 1 */
+#define GWEID1_DQSED			GENMASK(23, 16)
+#define GWEID1_DQOED			GENMASK(7, 0)
+
+/* GWCA Error Interrupt Status 2 i ( i = 0 to AXI_CHAIN_N/32-1) */
+#define RSW2_GCWA_GWEIS2(i)		(0x1200 + (10 * (i)))
+#define GWEIS2_DFES(t)			BIT_MASK(t)
+
+/* GWCA Error Interrupt Enable 2 i ( i = 0 to AXI_CHAIN_N/32-1) */
+#define RSW2_GCWA_GWEIE2(i)		(0x1204 + (10 * (i)))
+#define GWEIE2_DFEE(t)			BIT_MASK(t)
+
+/* GWCA Error Interrupt Disable 2 i ( i = 0 to AXI_CHAIN_N/32-1) */
+#define RSW2_GCWA_GWEID2(i)		(0x1208 + (10 * (i)))
+#define GWEID2_DFED(t)			BIT_MASK(t)
+
+#define RSW2_GCWA_GWEIS3		0x1280 /* GWCA Error Interrupt Status 3 */
+#define GWEIS3_IAOES			GENMASK(7, 0)
+
+#define RSW2_GCWA_GWEIE3		0x1284 /* GWCA Error Interrupt Enable 3 */
+#define GWEIE3_IAOEE			GENMASK(7, 0)
+
+#define RSW2_GCWA_GWEID3		0x1288 /* GWCA Error Interrupt Disable 3 */
+#define GWEID3_IAOED			GENMASK(7, 0)
+
+#define RSW2_GCWA_GWEIS4		0x1290 /* GWCA Error Interrupt Status 4 */
+#define GWEIS4_DSECN			GENMASK(30, 24)
+#define GWEIS4_DSEIOS			BIT_MASK(17)
+#define GWEIS4_DSES				BIT_MASK(16)
+#define GWEIS4_DSSECN			GENMASK(14, 8)
+#define GWEIS4_DSSEIOS			BIT_MASK(1)
+#define GWEIS4_DSSES			BIT_MASK(0)
+
+#define RSW2_GCWA_GWEIE4		0x1294 /* GWCA Error Interrupt Enable 4 */
+#define GWEIE4_DSEIOE			BIT_MASK(17)
+#define GWEIE4_DSEE				BIT_MASK(16)
+#define GWEIE4_DSSEIOE			BIT_MASK(1)
+#define GWEIE4_DSSEE			BIT_MASK(0)
+
+#define RSW2_GCWA_GWEID4		0x1298 /* GWCA Error Interrupt Disable 4 */
+#define GWEID4_DSDIOD			BIT_MASK(17)
+#define GWEID4_DSDD				BIT_MASK(16)
+#define GWEID4_DSDEIOD			BIT_MASK(1)
+#define GWEID4_DSDED			BIT_MASK(0)
+
+#define RSW2_GCWA_GWEIS5		0x12A0 /* GWCA Error Interrupt Status 5 */
+#define GWEIS5_RXDNECN			GENMASK(30, 24)
+#define GWEIS5_RXDNEIOS			BIT_MASK(17)
+#define GWEIS5_RXDNES			BIT_MASK(16)
+#define GWEIS5_DCTECN			GENMASK(14, 8)
+#define GWEIS5_DCTEIOS			BIT_MASK(1)
+#define GWEIS5_DCTES			BIT_MASK(0)
+
+#define RSW2_GCWA_GWEIE5		0x12A4 /* GWCA Error Interrupt Enable 5 */
+#define GWEIE5_RXDNEIOE			BIT_MASK(17)
+#define GWEIE5_RXDNEE			BIT_MASK(16)
+#define GWEIE5_DCTEIOE			BIT_MASK(1)
+#define GWEIE5_DCTEE			BIT_MASK(0)
+
+#define RSW2_GCWA_GWEID5		0x12A8 /* GWCA Error Interrupt Disable 5 */
+#define GWEID5_RXDNEIOD			BIT_MASK(17)
+#define GWEID5_RXDNED			BIT_MASK(16)
+#define GWEID5_DCTEIOD			BIT_MASK(1)
+#define GWEID5_DCTED			BIT_MASK(0)
+
+#define RSW2_GCWA_GWSCR0		0x1800 /* GWCA Security Configuration Register 0 */
+#define GWSCR0_TRSL				GENMASK(29, 28)
+#define GWSCR0_TSQRSL			GENMASK(25, 24)
+#define GWSCR0_DQRSL			GENMASK(23, 16)
+#define GWSCR0_APRSL			GENMASK(15, 8)
+#define GWSCR0_EIRSL			BIT_MASK(7)
+#define GWSCR0_AXRSL			BIT_MASK(6)
+#define GWSCR0_TSRSL			BIT_MASK(5)
+#define GWSCR0_TGRSL			BIT_MASK(4)
+#define GWSCR0_MCRSL			BIT_MASK(3)
+#define GWSCR0_MTRSL			BIT_MASK(2)
+#define GWSCR0_RRSL				BIT_MASK(1)
+#define GWSCR0_MRSL				BIT_MASK(0)
+
+
+#define RSW2_GCWA_GWSCR1		0x1804 /* GWCA Security Configuration Register 1 */
+#define GWSCR1_CRSL				BIT_MASK(0)
+
+/* GWCA Security Configuration Register 2 i ( i = 0 to AXI_CHAIN_N/32-1) */
+#define RSW2_GCWA_GWSCR2(i)		(0x1900 + (4 * (i)))
+#define GWSCR2_ACRSL(t)			BIT_MASK(t)
+
+#endif /* _RSWITCH2_GWCA_H */
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2_main.c b/drivers/net/ethernet/renesas/rswitch2/rswitch2_main.c
new file mode 100644
index 000000000000..d6bdfa0d4485
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2_main.c
@@ -0,0 +1,143 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Renesas RSwitch2 Ethernet CPU port device driver
+ *
+ * Copyright (C) 2019-2021 Renesas Electronics Corporation
+ *
+ */
+#include <linux/delay.h>
+#include <linux/err.h>
+#include <linux/kernel.h>
+#include <linux/io.h>
+
+#include "../rtsn_ptp.h"
+#include "rswitch2_fwd.h"
+#include "rswitch2_eth.h"
+#include "rswitch2_coma.h"
+
+static void rswitch2_reset(struct rswitch2_drv *rsw2)
+{
+	iowrite32(RRC_RR, rsw2->coma_base_addr + RSW2_COMA_RRC);
+	iowrite32(0x0, rsw2->coma_base_addr + RSW2_COMA_RRC);
+
+	reset_control_assert(rsw2->sd_rst);
+	mdelay(1);
+	reset_control_deassert(rsw2->sd_rst);
+}
+
+static void rswitch2_clock_enable(struct rswitch2_drv *rsw2)
+{
+	u32 bitmask;
+	u32 reg_val;
+	unsigned int num_of_ports;
+
+	/* Enable clock on all ports */
+	num_of_ports = rsw2->num_of_tsn_ports + rsw2->num_of_cpu_ports;
+	bitmask = (1 << num_of_ports) - 1;
+
+	reg_val = FIELD_PREP(RCEC_ACE, bitmask);
+	reg_val |= RCEC_RCE;
+	iowrite32(reg_val, rsw2->coma_base_addr + RSW2_COMA_RCEC);
+}
+
+static void rswitch2_clock_disable(struct rswitch2_drv *rsw2)
+{
+	u32 bitmask;
+	u32 reg_val;
+	unsigned int num_of_ports;
+
+	/* Disable clock on all ports */
+	num_of_ports = rsw2->num_of_tsn_ports + rsw2->num_of_cpu_ports;
+	bitmask = (1 << num_of_ports) - 1;
+
+	reg_val = FIELD_PREP(RCDC_ACD, bitmask);
+	reg_val |= RCDC_RCD;
+	iowrite32(0, rsw2->coma_base_addr + RSW2_COMA_RCDC);
+}
+
+static int rswitch2_init_buffer_pool(struct rswitch2_drv *rsw2)
+{
+	int ret;
+	u32 reg_val;
+
+	reg_val = ioread32(rsw2->coma_base_addr + RSW2_COMA_CABPIRM);
+	if ((reg_val & CABPIRM_BPR) == CABPIRM_BPR) {
+		rsw2_err(MSG_GEN, "Buffer pool: Invalid state\n");
+		return -EINVAL;
+	}
+
+	iowrite32(CABPIRM_BPIOG, rsw2->coma_base_addr + RSW2_COMA_CABPIRM);
+
+	ret = readl_poll_timeout(rsw2->coma_base_addr + RSW2_COMA_CABPIRM, reg_val,
+							reg_val & CABPIRM_BPR,
+							RSWITCH2_REG_POLL_DELAY, RSWITCH2_REG_POLL_TIMEOUT);
+	if (ret != 0) {
+		rsw2_err(MSG_GEN, "Initialization of buffer pools timed out\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+int rswitch2_init(struct rswitch2_drv *rsw2)
+{
+	int ret;
+
+	rswitch2_reset(rsw2);
+
+	ret = rswitch2_init_buffer_pool(rsw2);
+	if (ret < 0)
+		return ret;
+
+	rswitch2_clock_enable(rsw2);
+
+	rsw2->ptp_drv = rtsn_ptp_alloc(rsw2->dev);
+	if (!rsw2->ptp_drv) {
+		rsw2_err(MSG_GEN, "Failed to allocate PTP driver struct\n");
+		goto err_ptp_init;
+	}
+	rsw2->ptp_drv->addr = rsw2->ptp_base_addr;
+
+	ret = rtsn_ptp_init(rsw2->ptp_drv, RTSN_PTP_REG_LAYOUT_S4, RTSN_PTP_CLOCK_S4);
+	if(ret != 0) {
+		rsw2_err(MSG_GEN, "Failed to initialize PTP clock: %d\n", ret);
+		goto err_ptp_init;
+	}
+
+	/* static configuration of the PPS output */
+	iowrite32(0, rsw2->sram_base_addr + RSW2_RSW0PPS0R0);  //1st output relates to timer 0
+	iowrite32(1, rsw2->sram_base_addr + RSW2_RSW0PPS1R0);  //2nd output relates to timer 1
+
+	rsw2_notice(MSG_GEN, "PTP clock initialized\n");
+
+
+	ret = rswitch2_fwd_init(rsw2);
+	if (ret < 0)
+		goto err_fwd_init;
+
+	ret = rswitch2_eth_init(rsw2);
+	if (ret < 0) {
+		rsw2_err(MSG_GEN, "Failed to init ethernet driver: %d\n", ret);
+		goto err_eth_init;
+	}
+
+	return 0;
+
+err_eth_init:
+	rswitch2_fwd_exit(rsw2);
+err_fwd_init:
+	kfree(rsw2->ptp_drv);
+err_ptp_init:
+	rswitch2_clock_disable(rsw2);
+	rswitch2_reset(rsw2);
+
+	return ret;
+}
+
+void rswitch2_exit(struct rswitch2_drv *rsw2)
+{
+	rswitch2_fwd_exit(rsw2);
+
+	rswitch2_eth_exit(rsw2);
+}
+
+
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2_platf.c b/drivers/net/ethernet/renesas/rswitch2/rswitch2_platf.c
new file mode 100644
index 000000000000..0451949ffbcb
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2_platf.c
@@ -0,0 +1,700 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Renesas RSwitch2 platform device driver
+ *
+ * Copyright (C) 2021, 2022 Renesas Electronics Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License version 2,
+ * as published by the Free Software Foundation.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/pm_runtime.h>
+#include <linux/clk.h>
+#include <linux/phy.h>
+
+#include "rswitch2.h"
+#include "rswitch2_platf.h"
+
+
+static const u8 rsw2_default_mac[ETH_ALEN] __aligned(2) = {
+	0x74, 0x90, 0x50, 0x00, 0x00, 0x00
+};
+
+#define RSW2_INTERNAL_PORT_MARKER 0xCC
+
+static int log_level_gen = LOGLEVEL_ERR;
+module_param(log_level_gen,int,0660);
+
+static int log_level_desc = LOGLEVEL_ERR;
+module_param(log_level_desc,int,0660);
+
+static int log_level_rxtx = LOGLEVEL_ERR;
+module_param(log_level_rxtx,int,0660);
+
+static int log_level_fwd = LOGLEVEL_ERR;
+module_param(log_level_fwd,int,0660);
+
+static int log_level_serdes = LOGLEVEL_ERR;
+module_param(log_level_serdes,int,0660);
+
+
+/* Device driver's private data structure */
+struct rswitch2_platf_driver_priv {
+	struct rswitch2_drv *rsw2;
+	struct clk *rsw_clk;
+	struct clk *phy_clk;
+};
+
+#ifdef RSW2_NEW_MII
+static int rswitch2_mii_read(struct mii_bus *bus, int addr, int regnum)
+{
+	struct rswitch_etha *etha = bus->priv;
+	int mode, devad, regad;
+
+	mode = regnum & MII_ADDR_C45;
+	devad = (regnum >> MII_DEVADDR_C45_SHIFT) & 0x1f;
+	regad = regnum & MII_REGADDR_C45_MASK;
+
+	/* Not support Clause 22 access method */
+	if (!mode)
+		return 0;
+
+	return rswitch_etha_set_access(etha, true, addr, devad, regad, 0);
+}
+
+static int rswitch2_mii_write(struct mii_bus *bus, int addr, int regnum, u16 val)
+{
+	struct rswitch_etha *etha = bus->priv;
+	int mode, devad, regad;
+
+	mode = regnum & MII_ADDR_C45;
+	devad = (regnum >> MII_DEVADDR_C45_SHIFT) & 0x1f;
+	regad = regnum & MII_REGADDR_C45_MASK;
+
+	/* Not support Clause 22 access method */
+	if (!mode)
+		return 0;
+
+	return rswitch_etha_set_access(etha, false, addr, devad, regad, val);
+}
+
+static int rswitch2_mii_reset(struct mii_bus *bus)
+{
+	/* TODO */
+	return 0;
+}
+
+
+
+/* FIXME: Per Port!!! */
+static int rswitch2_register_mii_bus(struct rswitch2_drv *rsw2)
+{
+	struct mii_bus *mii_bus;
+	struct device_node *port;
+	int err;
+
+	mii_bus = mdiobus_alloc();
+	if (!mii_bus)
+		return -ENOMEM;
+
+	mii_bus->name = "rswitch2_mii";
+	sprintf(mii_bus->id, "etha%d", rdev->etha->index);
+	mii_bus->priv = rdev->etha;
+	mii_bus->read = rswitch2_mii_read;
+	mii_bus->write = rswitch2_mii_write;
+	mii_bus->reset = rswitch2_mii_reset;
+	mii_bus->parent = &rdev->ndev->dev;
+
+	port = rswitch_get_port_node(rdev);
+	of_node_get(port);
+	err = of_mdiobus_register(mii_bus, port);
+	if (err < 0) {
+		mdiobus_free(mii_bus);
+		goto out;
+	}
+
+	rdev->etha->mii = mii_bus;
+
+out:
+	of_node_put(port);
+
+	return err;
+}
+#endif
+
+
+
+static void rswitch2_platf_set_base_addr(struct rswitch2_drv *rsw2)
+{
+	u8 __iomem *base_addr = rsw2->base_addr;
+
+	rsw2->fwd_base_addr = base_addr + RSWITCH2_MFWD_BASE_ADDR;
+	rsw2->fab_base_addr = base_addr + RSWITCH2_FAB_BASE_ADDR;
+	rsw2->coma_base_addr = base_addr + RSWITCH2_COMA_BASE_ADDR;
+	rsw2->etha_base_addrs[0] = base_addr + RSWITCH2_TSNA0_BASE_ADDR;
+	rsw2->etha_base_addrs[1] = base_addr + RSWITCH2_TSNA1_BASE_ADDR;
+	rsw2->etha_base_addrs[2] = base_addr + RSWITCH2_TSNA2_BASE_ADDR;
+	rsw2->gwca_base_addrs[0] = base_addr + RSWITCH2_GWCA0_BASE_ADDR;
+#if RSWITCH2_CPU_PORTS > 1
+	rsw2->gwca_base_addrs[1] = base_addr + RSWITCH2_GWCA1_BASE_ADDR;
+#endif
+	rsw2->ptp_base_addr = base_addr + RSWITCH2_GPTP_BASE_ADDR;
+}
+
+static int rswitch2_platf_request_irqs(struct rswitch2_drv *rsw2)
+{
+	struct platform_device *pdev;
+	const struct device_node *dn_irqs;
+	int dt_prop_len;
+	const char **irq_names;
+	int dt_max_irqs, dt_cur_irq;
+	int irq;
+	int ret;
+
+
+	pdev = container_of(rsw2->dev, struct platform_device, dev);
+
+	dn_irqs = of_get_property(rsw2->dev->of_node, "interrupt-names", &dt_prop_len);
+	if (!dn_irqs) {
+		rsw2_err(MSG_GEN, "No irqs specified in device tree\n");
+		return -EINVAL;
+	}
+	else {
+		rsw2_dbg(MSG_GEN, "Got irq array of len %d  from device tree nodes\n", dt_prop_len);
+	}
+
+	irq_names = devm_kcalloc(&pdev->dev, RSWITCH2_MAX_IRQS,
+			       sizeof(*irq_names), GFP_KERNEL);
+	if (!irq_names)
+		return -ENOMEM;
+
+
+	ret = of_property_read_string_array(rsw2->dev->of_node, "interrupt-names",
+			irq_names, RSWITCH2_MAX_IRQS);
+	if (ret < 0)
+		return ret;
+
+	dt_max_irqs = (unsigned int)ret;
+
+	for(dt_cur_irq = 0; dt_cur_irq < dt_max_irqs; dt_cur_irq++) {
+		size_t prop_str_len;
+
+		prop_str_len = strlen(irq_names[dt_cur_irq]);
+
+		rsw2_info(MSG_GEN, "Got irq #%.02d '%s'\n", dt_cur_irq, irq_names[dt_cur_irq]);
+
+		/* FIXME: Check if DT string is shorter than sizeof(RSW2_GWCA0_NAME) - 1 */
+		if(prop_str_len > sizeof(RSW2_GWCA0_NAME)) {
+
+			ret = strncmp(irq_names[dt_cur_irq], RSW2_GWCA0_NAME, sizeof(RSW2_GWCA0_NAME) - 1);
+			if(ret == 0) {
+				const char *irq_type_str = irq_names[dt_cur_irq] + sizeof(RSW2_GWCA0_NAME);
+
+				irq = platform_get_irq_byname(pdev, irq_names[dt_cur_irq]);
+				if (irq < 0) {
+					rsw2_err(MSG_GEN, "Failed to get IRQ\n");
+					return -EINVAL; /* FIXME: error handling / memory */
+				}
+				rsw2_dbg(MSG_GEN, "Got irq %d '%s' from DT index %d\n", irq, irq_names[dt_cur_irq], dt_cur_irq);
+
+				ret = strncmp(irq_type_str, "rxtx", 4);
+				if(ret == 0) {
+					rsw2_info(MSG_GEN, "Registering RXTX irq #%.02d '%s'\n", dt_cur_irq, irq_names[dt_cur_irq]);
+					if(rsw2->num_of_rxtx_irqs >=  RSWITCH2_MAX_RXTX_IRQS) {
+						rsw2_err(MSG_GEN, "Too many RXTX interrupts\n");
+						// FIXME: free memory
+						return -EINVAL;
+
+					}
+					rsw2->rxtx_irqs[rsw2->num_of_rxtx_irqs] = irq;
+					rsw2->num_of_rxtx_irqs++;
+
+				}
+				else {
+					rsw2_info(MSG_GEN, "Registering status irq #%.02d '%s'\n", dt_cur_irq, irq_names[dt_cur_irq]);
+					if(rsw2->num_of_status_irqs >=  RSWITCH2_MAX_STATUS_IRQS) {
+						rsw2_err(MSG_GEN, "Too many status interrupts\n");
+
+						// FIXME: free memory
+						return -EINVAL;
+					}
+					rsw2->status_irqs[rsw2->num_of_status_irqs] = irq;
+					rsw2->num_of_status_irqs++;
+
+				}
+			}
+		}
+	}
+
+	devm_kfree(&pdev->dev, irq_names);
+
+	return 0;
+}
+
+static void rswitch2_platf_release_irqs(struct rswitch2_drv *rsw2)
+{
+	// FIXME
+}
+
+#ifdef OLD_STUFF
+static phy_interface_t rswitch2_get_phy_inferface(const char *phy_mode_str) {
+
+	int i;
+	phy_interface_t intf = PHY_INTERFACE_MODE_NA;
+	const int elem = ARRAY_SIZE(rsw2_phy_mode_xlate_tbl);
+
+	for(i = 0; i < elem; i++) {
+		int ret;
+		ret = strcmp(phy_mode_str, rsw2_phy_mode_xlate_tbl[i].dt_str);
+		//printk("Comparing '%s' <-> '%s'\n", phy_mode_str, rsw2_phy_mode_xlate_tbl[i].dt_str);
+		if (ret == 0) {
+			intf = rsw2_phy_mode_xlate_tbl[i].intf;
+			break;
+		}
+	}
+
+	return intf;
+}
+#endif
+
+static int rswitch2_platf_set_port_data(struct rswitch2_drv *rsw2)
+{
+//	int dt_port_num;
+//	unsigned int cur_port_num;
+//	unsigned int total_ports;
+	struct platform_device *pdev;
+//	struct rswitch2_port_data *cur_port_data;
+//	struct device_node *ports, *port;
+
+	int ret = 0;
+
+
+	pdev = container_of(rsw2->dev, struct platform_device, dev);
+	rsw2_dbg(MSG_GEN, "rswitch2_platf_set_port_data(): pdev is at 0x%px\n", pdev);
+
+
+	//total_ports = rsw2->num_of_cpu_ports + rsw2->num_of_tsn_ports;
+#ifdef  RSW2_DEPRECATED
+	rsw2->port_data = kcalloc(total_ports, sizeof(*cur_port_data), GFP_KERNEL);
+	if (!rsw2->port_data)
+		return -ENOMEM;
+
+	memset(rsw2->port_data, 0, sizeof(*cur_port_data));
+
+
+	for (cur_port_num = 0; cur_port_num < total_ports; cur_port_num++) {
+
+		cur_port_data = &rsw2->port_data[cur_port_num];
+		memcpy(cur_port_data->mac_addr, rsw2_default_mac,
+				sizeof(cur_port_data->mac_addr));
+		cur_port_data->mac_addr[ETH_ALEN - 1] = (u8)(cur_port_num);
+
+		if (cur_port_num == 0)
+			cur_port_data->mac_addr[ETH_ALEN - 2] = RSW2_INTERNAL_PORT_MARKER;
+//		else
+//			cur_port_data->phy_iface = PHY_INTERFACE_MODE_RGMII;
+
+		pr_info("Port-%d MAC %.2X:%.2X:%.2X:%.2X:%.2X:%.2X \n", cur_port_num,
+				cur_port_data->mac_addr[0], cur_port_data->mac_addr[1],
+				cur_port_data->mac_addr[2], cur_port_data->mac_addr[3],
+				cur_port_data->mac_addr[4], cur_port_data->mac_addr[5]
+						);
+	}
+#endif /* RSW2_DEPRECATED */
+#ifdef OLD_STUFF
+
+	/* etha0 --> PHY 0x04 */
+	cur_port_data = &rsw2->port_data[1];
+	snprintf(cur_port_data->phy_id, MII_BUS_ID_SIZE + 3, PHY_ID_FMT,
+						"e6800000.ethernet-ffffffff", 0x04);
+	pr_info("Port 0 PHY ID: '%s'\n", cur_port_data->phy_id);
+
+	/* etha1 --> PHY 0x03 */
+	cur_port_data = &rsw2->port_data[2];
+	snprintf(cur_port_data->phy_id, MII_BUS_ID_SIZE + 3, PHY_ID_FMT,
+						"e6800000.ethernet-ffffffff", 0x03);
+	pr_info("Port 1 PHY ID: '%s'\n", cur_port_data->phy_id);
+
+	/* etha2 --> PHY 0x01 */
+	cur_port_data = &rsw2->port_data[3];
+	snprintf(cur_port_data->phy_id, MII_BUS_ID_SIZE + 3, PHY_ID_FMT,
+						"e6800000.ethernet-ffffffff", 0x01);
+	pr_info("Port 2 PHY ID: '%s'\n", cur_port_data->phy_id);
+
+	/* etha3 --> PHY 0x00 */
+	cur_port_data = &rsw2->port_data[4];
+	snprintf(cur_port_data->phy_id, MII_BUS_ID_SIZE + 3, PHY_ID_FMT,
+						"e6800000.ethernet-ffffffff", 0x00);
+	pr_info("Port 3 PHY ID: '%s'\n", cur_port_data->phy_id);
+
+
+
+	ports = of_get_child_by_name(rsw2->dev->of_node, "ports");
+	if (!ports) {
+		dev_err(rsw2->dev, "No ports specified in device tree\n");
+		return -EINVAL;
+	}
+	//else {
+	//	dev_err(rsw2->dev, "Got ports from device tree nodes\n");
+	//}
+
+
+	dt_port_num = of_get_child_count(ports);
+	if(dt_port_num > rsw2->num_of_tsn_ports) {
+		dev_err(rsw2->dev, "%d ports specified in device tree, but maximum %d ports supported.\n",
+				dt_port_num, rsw2->num_of_tsn_ports);
+		return -EINVAL;
+	}
+	else {
+		dev_info(rsw2->dev, "DT specifies %d Ethernet ports\n", dt_port_num);
+	}
+
+
+	cur_port_num = 0;
+
+	for_each_child_of_node(ports, port) {
+		const char *prop_phy_mode_str;
+
+		cur_port_data = &rsw2->port_data[cur_port_num];
+		//cur_port_data->phy_id;
+
+		ret = of_property_read_u32(port, "reg", &cur_port_data->reg);
+		if (ret < 0) {
+			dev_err(rsw2->dev, "Failed to get register property for port %d: %d\n",
+					cur_port_num, ret);
+			break;
+		}
+		//else {
+		//	dev_info(rsw2->dev, "Got reg=%u for port %d\n", cur_port_data->reg, cur_port_num);
+		//}
+
+		ret = of_property_read_string(port, "phy-mode", &prop_phy_mode_str);
+		if (ret < 0) {
+			dev_err(rsw2->dev, "Failed to get phy mode property for port %d: %d\n",
+					cur_port_num, ret);
+			break;
+		}
+		dev_err(rsw2->dev, "DT phy mode: etha%u '%s'\n", cur_port_data->reg, prop_phy_mode_str);
+
+		cur_port_data->phy_iface = rswitch2_get_phy_inferface(prop_phy_mode_str);
+		if (cur_port_data->phy_iface == PHY_INTERFACE_MODE_NA) {
+			dev_err(rsw2->dev, "Invalid phy mode provided for port %d\n", cur_port_num);
+			ret = -EINVAL;
+		}
+		cur_port_num++;
+	}
+
+	of_node_put(ports);
+#endif
+
+// FIXME: Free port data on error
+
+	return ret;
+}
+
+
+static int rswitch2_platf_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct rswitch2_platf_driver_priv *drv_priv;
+	struct rswitch2_drv *rsw2;
+	size_t array_size;
+
+	struct resource *res_rsw2, *res_serdes, *res_sram;
+
+	res_rsw2 = platform_get_resource_byname(pdev, IORESOURCE_MEM, "iobase");
+	res_serdes = platform_get_resource_byname(pdev, IORESOURCE_MEM, "serdes");
+	res_sram = platform_get_resource_byname(pdev, IORESOURCE_MEM, "sram");
+	if (!res_rsw2 || !res_serdes || !res_sram) {
+		dev_err(&pdev->dev, "Invalid resources, check device tree\n");
+		return -EINVAL;
+	}
+
+
+	drv_priv = devm_kzalloc(&pdev->dev, sizeof(*drv_priv), GFP_KERNEL);
+	if (!drv_priv) {
+		return -ENOMEM;
+		// FIXME: Clean up
+	}
+
+	drv_priv->rsw_clk = devm_clk_get(&pdev->dev, "rsw2");
+	if (IS_ERR(drv_priv->rsw_clk)) {
+		dev_err(&pdev->dev, "Failed to get rsw2 clock: %ld\n", PTR_ERR(drv_priv->rsw_clk));
+		return PTR_ERR(drv_priv->rsw_clk);
+	}
+
+	drv_priv->phy_clk = devm_clk_get(&pdev->dev, "eth-phy");
+	if (IS_ERR(drv_priv->phy_clk)) {
+		dev_err(&pdev->dev, "Failed to get eth-phy clock: %ld\n", PTR_ERR(drv_priv->phy_clk));
+		return PTR_ERR(drv_priv->phy_clk);
+	}
+
+	/* Allocate memory for the common rswitch2 data */
+	rsw2 = kzalloc(sizeof(*rsw2), GFP_KERNEL);
+	if (!rsw2) {
+		ret = -ENOMEM;
+		goto out_drv_priv_mem;
+	}
+	drv_priv->rsw2 = rsw2;
+
+	/* Set default logging levels */
+	rsw2->sec_log_lvl[MSG_GEN] = log_level_gen;
+	rsw2->sec_log_lvl[MSG_DESC] = log_level_desc;
+	rsw2->sec_log_lvl[MSG_RXTX] = log_level_rxtx;
+	rsw2->sec_log_lvl[MSG_FWD] = log_level_fwd;
+	rsw2->sec_log_lvl[MSG_SERDES] = log_level_serdes;
+
+	/* Set driver private data */
+	platform_set_drvdata(pdev, drv_priv);
+
+	rsw2->sd_rst = devm_reset_control_get(&pdev->dev, "eth-phy");
+	if (IS_ERR(rsw2->sd_rst)) {
+		dev_err(&pdev->dev, "Failed to get reset control: %ld\n", PTR_ERR(rsw2->sd_rst));
+		return PTR_ERR(rsw2->sd_rst);
+	}
+	//drv_priv->pdev = pdev;
+	rsw2->base_addr = devm_ioremap_resource(&pdev->dev, res_rsw2);
+	if (IS_ERR(rsw2->base_addr))
+		return PTR_ERR(rsw2->base_addr);
+
+	rsw2->serdes_base_addr = devm_ioremap_resource(&pdev->dev, res_serdes);
+	if (IS_ERR(rsw2->serdes_base_addr))
+		return PTR_ERR(rsw2->serdes_base_addr);
+
+	rsw2->sram_base_addr = devm_ioremap_resource(&pdev->dev, res_sram);
+	if (IS_ERR(rsw2->sram_base_addr))
+		return PTR_ERR(rsw2->sram_base_addr);
+
+	/* Allocate base address array for multiple device instances */
+	array_size = sizeof(*rsw2->etha_base_addrs) * RSWITCH2_TSN_PORTS;
+	rsw2->etha_base_addrs = kzalloc(array_size, GFP_KERNEL);
+	if (!rsw2->etha_base_addrs) {
+		ret = -ENOMEM;
+		goto out_rsw2_mem;
+	}
+
+	array_size = sizeof(*rsw2->gwca_base_addrs) * RSWITCH2_CPU_PORTS;
+
+	rsw2->gwca_base_addrs = kzalloc(array_size, GFP_KERNEL);
+	if (!rsw2->gwca_base_addrs) {
+		ret = -ENOMEM;
+		goto out_etha_mem;
+	}
+	rsw2->dev = &pdev->dev;
+
+	rswitch2_platf_set_base_addr(rsw2);
+
+	/* Set number of ports */
+	rsw2->num_of_tsn_ports = RSWITCH2_TSN_PORTS;
+	rsw2->num_of_cpu_ports = RSWITCH2_CPU_PORTS;
+
+	/* TODO: Get data from DT 	 */
+	ret = rswitch2_platf_set_port_data(rsw2);
+	if (ret < 0) {
+		ret = -ENOMEM;
+		goto out_gwca_mem;
+	}
+
+	ret = rswitch2_platf_request_irqs(rsw2);
+	if (ret < 0) {
+		ret = -ENOMEM;
+		goto out_port_mem;
+	}
+
+	rsw2_dbg(MSG_GEN, "pdev->dev.power.disable_depth:: %d\n", pdev->dev.power.disable_depth);
+
+	pm_runtime_enable(&pdev->dev);
+	pm_runtime_get_sync(&pdev->dev);
+	clk_prepare(drv_priv->phy_clk);
+	clk_enable(drv_priv->phy_clk);
+
+	rsw2_dbg(MSG_GEN,"pdev->dev.power.disable_depth:: %d\n", pdev->dev.power.disable_depth);
+
+
+	device_set_wakeup_capable(&pdev->dev, 1);
+
+	/* Init RSwitch2 core */
+	ret = rswitch2_init(rsw2);
+	if (ret < 0) {
+		rsw2_err(MSG_GEN, "Failed to initialized RSwitch2 driver: %d\n", ret);
+		goto out_req_irq;
+	}
+
+	return 0;
+
+out_req_irq:
+	rswitch2_platf_release_irqs(drv_priv->rsw2);
+
+out_port_mem:
+//	kfree(rsw2->port_data);
+
+out_gwca_mem:
+	kfree(rsw2->gwca_base_addrs);
+
+out_etha_mem:
+	kfree(rsw2->etha_base_addrs);
+
+out_rsw2_mem:
+	kfree(rsw2);
+
+out_drv_priv_mem:
+	devm_kfree(&pdev->dev, drv_priv);
+
+	return ret;
+}
+
+/* Clean up */
+//static void rswitch2_pci_remove(struct pci_dev *pdev)
+static int rswitch2_platf_remove(struct platform_device *pdev)
+{
+	struct rswitch2_platf_driver_priv *drv_priv = platform_get_drvdata(pdev);
+
+
+	if (drv_priv) {
+		if (drv_priv->rsw2) {
+			struct rswitch2_drv *rsw2 = drv_priv->rsw2;
+
+
+			rswitch2_exit(drv_priv->rsw2);
+
+			rswitch2_platf_release_irqs(drv_priv->rsw2);
+
+			rsw2_dbg(MSG_GEN, "pdev->dev.power.disable_depth:: %d\n", pdev->dev.power.disable_depth);
+
+			pm_runtime_put(&pdev->dev);
+			pm_runtime_disable(&pdev->dev);
+			clk_disable(drv_priv->phy_clk);
+
+			rsw2_dbg(MSG_GEN, "pdev->dev.power.disable_depth:: %d\n", pdev->dev.power.disable_depth);
+		}
+	}
+
+	if (drv_priv) {
+		if (drv_priv->rsw2) {
+//			iounmap(drv_priv->rsw2->base_addr);
+//			iounmap(drv_priv->rsw2->serdes_base_addr);
+
+			kfree(drv_priv->rsw2->gwca_base_addrs);
+			kfree(drv_priv->rsw2->etha_base_addrs);
+			kfree(drv_priv->rsw2);
+		}
+
+
+		devm_kfree(&pdev->dev, drv_priv);
+		platform_set_drvdata(pdev, NULL);
+	}
+
+	return 0;
+}
+
+#ifdef OLD_STUFF
+static int renesas_eth_sw_probe(struct platform_device *pdev)
+{
+	struct rswitch_private *priv;
+	struct resource *res, *res_serdes;
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "iobase");
+	res_serdes = platform_get_resource_byname(pdev, IORESOURCE_MEM, "serdes");
+	if (!res || !res_serdes) {
+		dev_err(&pdev->dev, "invalid resource\n");
+		return -EINVAL;
+	}
+
+	priv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->rsw_clk = devm_clk_get(&pdev->dev, "rsw2");
+	if (IS_ERR(priv->rsw_clk)) {
+		dev_err(&pdev->dev, "Failed to get rsw2 clock: %ld\n", PTR_ERR(priv->rsw_clk));
+		return PTR_ERR(priv->rsw_clk);
+	}
+
+	priv->phy_clk = devm_clk_get(&pdev->dev, "eth-phy");
+	if (IS_ERR(priv->phy_clk)) {
+		dev_err(&pdev->dev, "Failed to get eth-phy clock: %ld\n", PTR_ERR(priv->phy_clk));
+		return PTR_ERR(priv->phy_clk);
+	}
+
+	platform_set_drvdata(pdev, priv);
+	priv->pdev = pdev;
+	priv->addr = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(priv->addr))
+		return PTR_ERR(priv->addr);
+
+	priv->serdes_addr = devm_ioremap_resource(&pdev->dev, res_serdes);
+	if (IS_ERR(priv->serdes_addr))
+		return PTR_ERR(priv->serdes_addr);
+
+	debug_addr = priv->addr;
+
+	/* Fixed to use GWCA0 */
+	priv->gwca.index = 3;
+	priv->gwca.num_chains = num_ndev * NUM_CHAINS_PER_NDEV;
+	priv->gwca.chains = devm_kcalloc(&pdev->dev, priv->gwca.num_chains,
+					 sizeof(*priv->gwca.chains), GFP_KERNEL);
+	if (!priv->gwca.chains)
+		return -ENOMEM;
+
+	pm_runtime_enable(&pdev->dev);
+	pm_runtime_get_sync(&pdev->dev);
+	clk_prepare(priv->phy_clk);
+	clk_enable(priv->phy_clk);
+
+	rswitch_init(priv);
+
+	device_set_wakeup_capable(&pdev->dev, 1);
+
+	return 0;
+}
+
+static int rswitch2_platf_remove(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct rswitch_device *rdev = netdev_priv(ndev);
+	struct rswitch_private *priv = rdev->priv;
+
+	/* Disable R-Switch clock */
+	rs_write32(RCDC_RCD, rdev->priv->addr + RCDC);
+
+	pm_runtime_put(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+	clk_disable(priv->phy_clk);
+
+	dma_free_coherent(ndev->dev.parent, priv->desc_bat_size, priv->desc_bat,
+			  priv->desc_bat_dma);
+
+	unregister_netdev(ndev);
+	netif_napi_del(&rdev->napi);
+	free_netdev(ndev);
+	platform_set_drvdata(pdev, NULL);
+
+	return 0;
+}
+#endif /* OLD_STUFF */
+
+static const struct of_device_id rswitch2_platf_of_table[] = {
+	{ .compatible = "renesas,rswitch2", },
+	{ }
+};
+MODULE_DEVICE_TABLE(of, rswitch2_platf_of_table);
+
+
+static struct platform_driver rswitch2_platf_drv = {
+	.probe = rswitch2_platf_probe,
+	.remove = rswitch2_platf_remove,
+	.driver = {
+		.owner = THIS_MODULE,
+		.name = "RSwitch2 platform driver",
+		.of_match_table = rswitch2_platf_of_table,
+	}
+};
+module_platform_driver(rswitch2_platf_drv);
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Dennis Ostermann <dennis.ostermann@renesas.com>");
+MODULE_DESCRIPTION("RSwitch2 platform driver");
+MODULE_VERSION("0.20");
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2_platf.h b/drivers/net/ethernet/renesas/rswitch2/rswitch2_platf.h
new file mode 100644
index 000000000000..e2bee84e618b
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2_platf.h
@@ -0,0 +1,99 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Renesas RSwitch PCIe device driver
+ *
+ * Copyright (C) 2021, 2022 Renesas Electronics Corporation
+ */
+
+
+#ifndef _RSWITCH2_PLATF_H
+#define _RSWITCH2_PLATF_H
+
+/* Rswitch2 IP register offsets */
+#define RSWITCH2_MFWD_BASE_ADDR		0x00000000	/* Fabric Register Offset */
+#define RSWITCH2_FAB_BASE_ADDR		0x00008000	/* Forwarding Engine Register Offset */
+#define RSWITCH2_COMA_BASE_ADDR		0x00009000	/* Common Agent Register Offset */
+#define RSWITCH2_TSNA0_BASE_ADDR	0x0000A000	/* TSN Agent Register Offset */
+#define RSWITCH2_TSNA1_BASE_ADDR	0x0000C000	/* TSN Agent Register Offset */
+#define RSWITCH2_TSNA2_BASE_ADDR	0x0000E000	/* TSN Agent Register Offset */
+#define RSWITCH2_GWCA0_BASE_ADDR	0x00010000	/* GWCA Register Offset	*/
+#define RSWITCH2_GWCA1_BASE_ADDR	0x00012000	/* GWCA Register Offset	*/
+#define RSWITCH2_GPTP_BASE_ADDR     0x00018000  /* GPTP Register Offset */
+
+#define RSWITCH2_TSN_PORTS			3
+#define RSWITCH2_CPU_PORTS			1  // FIXME: USe only one internal port for now
+
+/* SerDes definitions */
+/* SerDes */
+enum rswitch_serdes_mode {
+	USXGMII,
+	SGMII,
+	COMBINATION,
+};
+
+#define RSWITCH_SERDES_OFFSET                   0x0400
+#define RSWITCH_SERDES_BANK_SELECT              0x03fc
+#define RSWITCH_SERDES_FUSE_OVERRIDE(n)         (0x2600 - (n) * 0x400)
+
+#define BANK_180                                0x0180
+#define VR_XS_PMA_MP_12G_16G_25G_SRAM           0x026c
+#define VR_XS_PMA_MP_12G_16G_25G_REF_CLK_CTRL   0x0244
+#define VR_XS_PMA_MP_10G_MPLLA_CTRL2            0x01cc
+#define VR_XS_PMA_MP_12G_16G_25G_MPLL_CMN_CTRL  0x01c0
+#define VR_XS_PMA_MP_12G_16G_MPLLA_CTRL0        0x01c4
+#define VR_XS_PMA_MP_12G_MPLLA_CTRL1            0x01c8
+#define VR_XS_PMA_MP_12G_MPLLA_CTRL3            0x01dc
+#define VR_XS_PMA_MP_12G_16G_25G_VCO_CAL_LD0    0x0248
+#define VR_XS_PMA_MP_12G_VCO_CAL_REF0           0x0258
+#define VR_XS_PMA_MP_12G_16G_25G_RX_GENCTRL1    0x0144
+#define VR_XS_PMA_CONSUMER_10G_RX_GENCTRL4      0x01a0
+#define VR_XS_PMA_MP_12G_16G_25G_TX_RATE_CTRL   0x00d0
+#define VR_XS_PMA_MP_12G_16G_25G_RX_RATE_CTRL   0x0150
+#define VR_XS_PMA_MP_12G_16G_TX_GENCTRL2        0x00c8
+#define VR_XS_PMA_MP_12G_16G_RX_GENCTRL2        0x0148
+#define VR_XS_PMA_MP_12G_AFE_DFE_EN_CTRL        0x0174
+#define VR_XS_PMA_MP_12G_RX_EQ_CTRL0            0x0160
+#define VR_XS_PMA_MP_10G_RX_IQ_CTRL0            0x01ac
+#define VR_XS_PMA_MP_12G_16G_25G_TX_GENCTRL1    0x00c4
+#define VR_XS_PMA_MP_12G_16G_TX_GENCTRL2        0x00c8
+#define VR_XS_PMA_MP_12G_16G_RX_GENCTRL2        0x0148
+#define VR_XS_PMA_MP_12G_16G_25G_TX_GENCTRL1    0x00c4
+#define VR_XS_PMA_MP_12G_16G_25G_TX_EQ_CTRL0    0x00d8
+#define VR_XS_PMA_MP_12G_16G_25G_TX_EQ_CTRL1    0x00dc
+#define VR_XS_PMA_MP_12G_16G_MPLLB_CTRL0        0x01d0
+#define VR_XS_PMA_MP_12G_MPLLB_CTRL1            0x01d4
+#define VR_XS_PMA_MP_12G_16G_MPLLB_CTRL2        0x01d8
+#define VR_XS_PMA_MP_12G_MPLLB_CTRL3            0x01e0
+
+#define BANK_300                                0x0300
+#define SR_XS_PCS_CTRL1                         0x0000
+#define SR_XS_PCS_STS1                          0x0004
+#define SR_XS_PCS_CTRL2                         0x001c
+
+#define BANK_380                                0x0380
+#define VR_XS_PCS_DIG_CTRL1                     0x0000
+#define VR_XS_PCS_DEBUG_CTRL                    0x0014
+#define VR_XS_PCS_KR_CTRL                       0x001c
+
+#define BANK_1F00                               0x1f00
+#define SR_MII_CTRL                             0x0000
+
+#define BANK_1F80                               0x1f80
+#define VR_MII_AN_CTRL                          0x0004
+
+
+struct rsw2_phy_mode_xlate {
+	const char *dt_str;
+	phy_interface_t intf;
+};
+
+static const struct rsw2_phy_mode_xlate rsw2_phy_mode_xlate_tbl[] = {
+	{ "mii", PHY_INTERFACE_MODE_MII },
+	{ "gmii", PHY_INTERFACE_MODE_GMII },
+	{ "rgmii", PHY_INTERFACE_MODE_GMII },
+	{ "sgmii", PHY_INTERFACE_MODE_SGMII },
+	{ "usxgmii", PHY_INTERFACE_MODE_USXGMII },
+};
+
+
+#endif /* _RSWITCH2_PLATF_H */
+
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2_rmac.h b/drivers/net/ethernet/renesas/rswitch2/rswitch2_rmac.h
new file mode 100644
index 000000000000..9efecc4ccf0f
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2_rmac.h
@@ -0,0 +1,441 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Renesas RSwitch2 Gateway Common Agent device driver
+ *
+ * Copyright (C) 2021 Renesas Electronics Corporation
+ *
+ */
+#ifndef _RSWITCH2_RMACGWCA_H
+#define _RSWITCH2_GWCA_H
+
+#include <linux/bits.h>
+
+#define RSW2_RMAC_OFFSET	0x1000
+
+#define RSW2_RMAC_MPSM	0x0000 /* MAC PHY station management */
+#define MPSM_PRD	GENMASK(31, 16)
+#define MPSM_POP	GENMASK(14, 13)
+#define MPSM_PRA	GENMASK(12, 8)
+#define MPSM_PDA	GENMASK(7, 3)
+#define MPSM_MFF	BIT_MASK(2)
+#define MPSM_PSME	BIT_MASK(0)
+
+
+#define RSW2_RMAC_MPIC	0x0004 /* MAC PHY interfaces configuration */
+#define MPIC_PSMCT	GENMASK(30, 28)
+#define MPIC_PSMHT	GENMASK(26, 24)
+#define MPIC_PSMDP	BIT_MASK(23)
+#define MPIC_PSMCS	GENMASK(22, 16)
+#define MPIC_RPMT	BIT_MASK(11)
+#define MPIC_PLSPP	BIT_MASK(10)
+#define MPIC_PIPP	BIT_MASK(9)
+#define MPIC_PIP	BIT_MASK(8)
+#define MPIC_LSC	GENMASK(5, 3)
+#define MPIC_PIS	GENMASK(2, 0)
+
+enum rsw2_rmac_link_speed {
+	rsw2_rmac_100mbps	= 1,
+	rsw2_rmac_1000mbps	= 2,
+	rsw2_rmac_2500mbps	= 3,
+};
+
+enum rsw2_rmac_phy_if_type {
+	rsw2_rmac_mii	= 0,
+	rsw2_rmac_gmii	= 2,
+	rsw2_rmac_xgmii	= 4,
+};
+
+#define RSW2_RMAC_MPIM	0x0008 /* MAC PHY interfaces monitoring register */
+#define MPIM_LPIA	BIT_MASK(1)
+#define MPIM_PLS	BIT_MASK(0)
+
+#define RSW2_RMAC_MTFFC	0x0020	/* MAC transmission frame format configuration */
+#define MTFFC_FCM	BIT_MASK(1)
+#define MTFFC_DPAD	BIT_MASK(0)
+
+#define RSW2_RMAC_MTPFC	0x0024	/* MAC transmission pause or PFC frame configuration */
+#define MTPFC_PFRLV	GENMASK(31, 27)
+#define MTPFC_PFM	BIT_MASK(26)
+#define MTPFC_PFRT	GENMASK(23, 16)
+#define MTPFC_PT	GENMASK(15, 0)
+
+#define RSW2_RMAC_MTPFC2	0x0028	/* MAC transmission pause or PFC frame configuration2 */
+#define MTPFC2_MPFR		BIT_MASK(17)
+#define MTPFC2_PFTTZ	BIT_MASK(16)
+#define MTPFC2_MPFCFR	GENMASK(9, 8)
+#define MTPFC2_PFCTTZ	GENMASK(1, 0)
+
+
+#define RSW2_RMAC_MTPFC3(t)	(0x0030 + 4 * (t))	/* MAC transmission pause or PFC frame configuration 3 */
+#define MTPFC3_PFCPG(i)	BIT_MASK(i)
+
+#define RSW2_RMAC_MTATC0	(0x0050 + 4 * (t)) /* MAC transmission automatic timestamp configuration */
+#define MTATC0_TRTL		GEN_MASK(10, 8)
+#define MTATC0_TRTP		GEN_MASK(7, 0)
+
+#define RSW2_RMAC_MRGC	0x0080 /* MAC reception general configuration */
+#define MRGC_PFCRC		GENMASK(23, 16)
+#define MRGC_RFCFE		BIT_MASK(4)
+#define MRGC_MPDE		BIT_MASK(3)
+#define MRGC_PFRTZ		BIT_MASK(2)
+#define MRGC_PFRC		BIT_MASK(1)
+#define MRGC_RCPT		BIT_MASK(0)
+
+
+#define RSW2_RMAC_MRMAC0	0x0084 /* MAC reception MAC address configuration 0 */
+#define MRMAC0_MAUP		GENMASK(15, 0)
+
+#define RSW2_RMAC_MRMAC1	0x0088 /* MAC reception MAC address configuration 1 */
+#define MRMAC1_MADP		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRAFC	0x008C /* MAC reception address filter configuration */
+#define MRAFC_MSAREP	BIT_MASK(26)
+#define MRAFC_NSAREP	BIT_MASK(25)
+#define MRAFC_SDSFREP	BIT_MASK(24)
+#define MRAFC_NDAREP	BIT_MASK(23)
+#define MRAFC_BCACP		BIT_MASK(22)
+#define MRAFC_MCACP		BIT_MASK(21)
+#define MRAFC_BSTENP	BIT_MASK(20)
+#define MRAFC_MSTENP	BIT_MASK(19)
+#define MRAFC_BCENP		BIT_MASK(18)
+#define MRAFC_MCENP		BIT_MASK(17)
+#define MRAFC_UCENP		BIT_MASK(16)
+#define MRAFC_MSAREE	BIT_MASK(10)
+#define MRAFC_NSAREE	BIT_MASK(9)
+#define MRAFC_SDSFREE	BIT_MASK(8)
+#define MRAFC_NDAREE	BIT_MASK(7)
+#define MRAFC_BCACE		BIT_MASK(6)
+#define MRAFC_MCACE		BIT_MASK(5)
+#define MRAFC_BSTENE	BIT_MASK(4)
+#define MRAFC_MSTENE	BIT_MASK(3)
+#define MRAFC_BCENE		BIT_MASK(2)
+#define MRAFC_MCENE		BIT_MASK(1)
+#define MRAFC_UCENE		BIT_MASK(0)
+
+#define RSW2_RMAC_MRSCE	0x0090 /* MAC reception storm configuration for e-frames */
+#define MRSCE_CBFE		GENMASK(31, 16)
+#define MRSCE_CMFE		GENMASK(15, 0)
+
+#define RSW2_RMAC_MRSCP	0x0094 /* MAC reception storm configuration for p-frames */
+#define MRSCP_CBFE		GENMASK(31, 16)
+#define MRSCP_CMFE		GENMASK(15, 0)
+
+#define RSW2_RMAC_MRSCC	0x0098 /* MAC reception storm counter configuration */
+#define MRSCC_BSCCP		BIT_MASK(17)
+#define MRSCC_MSCCP		BIT_MASK(16)
+#define MRSCC_BSCCE		BIT_MASK(1)
+#define MRSCC_MSCCE		BIT_MASK(0)
+
+#define RSW2_RMAC_MRFSCE	0x009C /* MAC reception frame size configuration for p-frames */
+#define MRFSCE_EMNS		GENMASK(31, 16)
+#define MRFSCE_EMXS		GENMASK(15, 0)
+
+#define RSW2_RMAC_MRFSCP	0x00a0 /* MAC reception frame size configuration for e-frames */
+#define MRFSCP_PMNS		GENMASK(31, 16)
+#define MRFSCP_PMXS		GENMASK(15, 0)
+
+#define RSW2_RMAC_MTRC	0x00a4 /* MAC Timestamp reception configuration register */
+#define MTRC_DTN		BIT_MASK(28)
+#define MTRC_TCTSP		BIT_MASK(27)
+#define MTRC_TCTSE		BIT_MASK(26)
+#define MTRC_TRDDP		BIT_MASK(25)
+#define MTRC_TRDDE		BIT_MASK(24)
+#define MTRC_TRHFME		GENMASK(1, 0)
+
+#define RSW2_RMAC_MRIM	0x00a8 /* MAC reception interfaces monitoring */
+
+#define RSW2_RMAC_MRPFM	0x00aC /* MAC PTP filtering register configuration */
+#define MRPFM_PFCTCA	GENMASK(23, 16)
+#define MRPFM_PTCA		BIT_MASK(0)
+
+#define RSW2_RMAC_MPFC0(t)	(0x0100 + 4 * (t)) /* MAC PTP filtering register configuration */
+#define MPFC0_TEF		GENMASK(17, 16)
+#define MPFC0_PFBV		GENMASK(15, 8)
+#define MPFC0_PFBN		GENMASK(7, 0)
+
+#define RSW2_RMAC_MLVC	0x0180 /* MAC link verification configuration */
+#define MLVC_PLV		BIT_MASK(16)
+#define MLVC_PASE		BIT_MASK(8)
+#define MLVC_LVT		GENMASK(6, 0)
+
+#define RSW2_RMAC_MEEEC	0x0184 /* MAC energy efficient ethernet configuration */
+#define MEEEC_LPITR		BIT_MASK(0)
+
+#define RSW2_RMAC_MLBC	0x0188 /* MAC loopback configuration */
+
+#define RSW2_RMAC_MXGMIIC	0x0190 /* XGMII configuration */
+#define MXGMIIC_LFS_TXIDLE	BIT_MASK(1)
+#define MXGMIIC_LFS_TXRFS	BIT_MASK(0)
+
+
+#define RSW2_RMAC_MPCH	0x0194 /* XGMII PCH configuration */
+#define MPCH_RPHCRCD		BIT_MASK(17)
+#define MPCH_RXPCH_TSM		BIT_MASK(16)
+#define MPCH_CTRIOD			BIT_MASK(11)
+#define MPCH_IETRIOD		BIT_MASK(10)
+#define MPCH_CTPTE			BIT_MASK(9)
+#define MPCH_IETPTE			BIT_MASK(8)
+#define MPCH_TXPCH_PID		GENMASK(7, 4)
+#define MPCH_TXPCH_ETYPE	GENMASK(3, 1)
+#define MPCH_TXPCH_M		BIT_MASK(0)
+
+#define RSW2_RMAC_MANC	0x0198 /* Auto-negotiation configuration */
+#define MANC_AN_REQ		BIT_MASK(0)
+
+#define RSW2_RMAC_MANM	0x019C /* Auto-negotiation message */
+#define MANM_TX_AN_MES	GENMASK(31, 15)
+#define MANM_RX_AN_MES	GENMASK(15, 0)
+
+#define RSW2_RMAC_MEIS	0x0200 /* MAC error interrupt status */
+#define MEIS_FOES		BIT_MASK(29)
+#define MEIS_FUES		BIT_MASK(28)
+#define MEIS_FFS		BIT_MASK(27)
+#define MEIS_RPOOMS		BIT_MASK(26)
+#define MEIS_FRCES		BIT_MASK(25)
+#define MEIS_CFCES		BIT_MASK(24)
+#define MEIS_FFMES		BIT_MASK(23)
+#define MEIS_FCMCES		BIT_MASK(22)
+#define MEIS_PNAES		BIT_MASK(21)
+#define MEIS_PDES		BIT_MASK(20)
+#define MEIS_CTLES		GENMASK(13, 12)
+#define MEIS_RPCRES		BIT_MASK(11)
+#define MEIS_RPOES		BIT_MASK(10)
+#define MEIS_REOES		BIT_MASK(9)
+#define MEIS_FCES		BIT_MASK(8)
+#define MEIS_BFES		BIT_MASK(7)
+#define MEIS_TBCIS		BIT_MASK(6)
+#define MEIS_TCES		BIT_MASK(5)
+#define MEIS_FCDS		BIT_MASK(4)
+#define MEIS_PFRROS		BIT_MASK(3)
+#define MEIS_PRES		BIT_MASK(2)
+#define MEIS_TIES		BIT_MASK(1)
+#define MEIS_TSLS		BIT_MASK(0)
+
+#define RSW2_RMAC_MEIE	0x0204 /* MAC error interrupt enable */
+#define MEIE_FOEE		BIT_MASK(29)
+#define MEIE_FUEE		BIT_MASK(28)
+#define MEIE_FFE		BIT_MASK(27)
+#define MEIE_RPOOME		BIT_MASK(26)
+#define MEIE_FRCEE		BIT_MASK(25)
+#define MEIE_CFCEE		BIT_MASK(24)
+#define MEIE_FFMEE		BIT_MASK(23)
+#define MEIE_FCMCEE		BIT_MASK(22)
+#define MEIE_PNAEE		BIT_MASK(21)
+#define MEIE_PDEE		BIT_MASK(20)
+#define MEIE_CTLEE		GENMASK(13, 12)
+#define MEIE_RPCREE		BIT_MASK(11)
+#define MEIE_RPOEE		BIT_MASK(10)
+#define MEIE_REOEE		BIT_MASK(9)
+#define MEIE_FCEE		BIT_MASK(8)
+#define MEIE_BFEE		BIT_MASK(7)
+#define MEIE_TBCIE		BIT_MASK(6)
+#define MEIE_TCEE		BIT_MASK(5)
+#define MEIE_FCDE		BIT_MASK(4)
+#define MEIE_PFRROE		BIT_MASK(3)
+#define MEIE_PREE		BIT_MASK(2)
+#define MEIE_TIEE		BIT_MASK(1)
+#define MEIE_TSLE		BIT_MASK(0)
+
+#define RSW2_RMAC_MEID	0x0208 /* MAC error interrupt disable */
+#define MEID_FOED		BIT_MASK(29)
+#define MEID_FUED		BIT_MASK(28)
+#define MEID_FFD		BIT_MASK(27)
+#define MEID_RPOOMD		BIT_MASK(26)
+#define MEID_FRCED		BIT_MASK(25)
+#define MEID_CFCED		BIT_MASK(24)
+#define MEID_FFMED		BIT_MASK(23)
+#define MEID_FCMCED		BIT_MASK(22)
+#define MEID_PNAED		BIT_MASK(21)
+#define MEID_PDED		BIT_MASK(20)
+#define MEID_CTLED		GENMASK(13, 12)
+#define MEID_RPCRED		BIT_MASK(11)
+#define MEID_RPOED		BIT_MASK(10)
+#define MEID_REOED		BIT_MASK(9)
+#define MEID_FCED		BIT_MASK(8)
+#define MEID_BFED		BIT_MASK(7)
+#define MEID_TBCID		BIT_MASK(6)
+#define MEID_TCED		BIT_MASK(5)
+#define MEID_FCDD		BIT_MASK(4)
+#define MEID_PFRROD		BIT_MASK(3)
+#define MEID_PRED		BIT_MASK(2)
+#define MEID_TIED		BIT_MASK(1)
+#define MEID_TSLD		BIT_MASK(0)
+
+#define RSW2_RMAC_MMIS0	0x0210 /* MAC monitoring interrupt status 0 */
+#define MMIS0_XLISDS	BIT_MASK(12)
+#define MMIS0_XRFSDS	BIT_MASK(11)
+#define MMIS0_XLFSDS	BIT_MASK(10)
+#define MMIS0_XLFES		BIT_MASK(9)
+#define MMIS0_XLFDS		BIT_MASK(8)
+#define MMIS0_ANDETS	BIT_MASK(6)
+#define MMIS0_VFRS		BIT_MASK(4)
+#define MMIS0_LVFS		BIT_MASK(3)
+#define MMIS0_LVSS		BIT_MASK(2)
+#define MMIS0_PIDS		BIT_MASK(1)
+#define MMIS0_PLSCS		BIT_MASK(0)
+
+#define RSW2_RMAC_MMIE0	0x0214 /* MAC monitoring interrupt enable 0 */
+#define MMIE0_XLISDE	BIT_MASK(12)
+#define MMIE0_XRFSDE	BIT_MASK(11)
+#define MMIE0_XLFSDE	BIT_MASK(10)
+#define MMIE0_XLFEE		BIT_MASK(9)
+#define MMIE0_XLFDE		BIT_MASK(8)
+#define MMIE0_ANDETE	BIT_MASK(6)
+#define MMIE0_VFRE		BIT_MASK(4)
+#define MMIE0_LVFE		BIT_MASK(3)
+#define MMIE0_LVSE		BIT_MASK(2)
+#define MMIE0_PIDE		BIT_MASK(1)
+#define MMIE0_PLSCE		BIT_MASK(0)
+
+#define RSW2_RMAC_MMID0	0x0218 /* MAC monitoring interrupt disable 0 */
+#define MMID0_XLISDD	BIT_MASK(12)
+#define MMID0_XRFSDD	BIT_MASK(11)
+#define MMID0_XLFSDD	BIT_MASK(10)
+#define MMID0_XLFED		BIT_MASK(9)
+#define MMID0_XLFDD		BIT_MASK(8)
+#define MMID0_ANDETD	BIT_MASK(6)
+#define MMID0_VFRD		BIT_MASK(4)
+#define MMID0_LVFD		BIT_MASK(3)
+#define MMID0_LVSD		BIT_MASK(2)
+#define MMID0_PIDD		BIT_MASK(1)
+#define MMID0_PLSCD		BIT_MASK(0)
+
+#define RSW2_RMAC_MMIS1	0x0220 /* MAC monitoring interrupt status 1 */
+#define MMIS1_PPRACS	BIT_MASK(3)
+#define MMIS1_PAACS		BIT_MASK(2)
+#define MMIS1_PWACS		BIT_MASK(1)
+#define MMIS1_PRACS		BIT_MASK(0)
+
+#define RSW2_RMAC_MMIE1	0x0224 /* MAC monitoring interrupt enable 1 */
+#define MMIE1_PPRACE	BIT_MASK(3)
+#define MMIE1_PAACE		BIT_MASK(2)
+#define MMIE1_PWACE		BIT_MASK(1)
+#define MMIE1_PRACE		BIT_MASK(0)
+
+#define RSW2_RMAC_MMID1	0x0228 /* MAC monitoring interrupt disable 1 */
+#define MMID1_PPRACD	BIT_MASK(3)
+#define MMID1_PAACD		BIT_MASK(2)
+#define MMID1_PWACD		BIT_MASK(1)
+#define MMID1_PRACD		BIT_MASK(0)
+
+#define RSW2_RMAC_MMIS2	0x0230 /* MAC monitoring interrupt status 2 */
+#define MMIS2_LPIDIS	BIT_MASK(2)
+#define MMIS2_LPIAIS	BIT_MASK(1)
+#define MMIS2_MPDIS		BIT_MASK(0)
+
+#define RSW2_RMAC_MMIE2	0x0234 /* MAC monitoring interrupt enable 2 */
+#define MMIE2_LPIDIE	BIT_MASK(2)
+#define MMIE2_LPIAIE	BIT_MASK(1)
+#define MMIE2_MPDIE		BIT_MASK(0)
+
+#define RSW2_RMAC_MMID2	0x0238 /* MAC monitoring interrupt disable 2 */
+#define MMID2_LPIDID	BIT_MASK(2)
+#define MMID2_LPIAID	BIT_MASK(1)
+#define MMID2_MPDID		BIT_MASK(0)
+
+#define RSW2_RMAC_MMPFTCT	0x0300 /* MAC manual pause frame transmit counter */
+#define MMPFTCT_MPFTC		GENMASK(15, 0)
+
+#define RSW2_RMAC_MAPFTCT	0x0304 /* MAC automatic pause frame transmit counter */
+#define MAPFTCT_APFTC		GENMASK(15, 0)
+
+#define RSW2_RMAC_MPFRCT	0x0308 /* MAC pause frame receive counter */
+#define MPFRCT_PFRC			GENMASK(15, 0)
+
+#define RSW2_RMAC_MFCICT	0x030c /* MAC false carrier indication counter */
+#define MFCICT_FCIC			GENMASK(15, 0)
+
+#define RSW2_RMAC_MEEECT	0x0310 /* MAC energy efficient ethernet counter */
+#define MEEECT_EEERC		GENMASK(15, 0)
+
+#define RSW2_RMAC_MMPCFTCT(t)	(0x0320 + 4 * (t)) /* MAC manual PFC frame transmit counter */
+#define MMPCFTCT_MPCFCTC		GENMASK(15, 0)
+
+#define RSW2_RMAC_MAPCFTCT(t)	(0x0330 + 4 * (t)) /* MAC automatic PFC frame transmit counter */
+#define MAPCFTCT_APCFCTC		GENMASK(15, 0)
+
+#define RSW2_RMAC_MPCFRCT(t)	(0x0340 + 4 * (t)) /* MAC PFC frame receive counter */
+#define MPCFRCT_PCFCRC			GENMASK(15, 0)
+
+#define RSW2_RMAC_MROVFC	0x0360 /* Receive overflow Counter */
+#define MROVFC_ROVFC		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRHCRCEC	0x0364 /* Reception Header-CRC(PCH CRC) error Counter */
+#define MRHCRCEC_RHCRCEC	GENMASK(15, 0)
+
+
+#define RSW2_RMAC_MRGFCE	0x0408 /* RMAC Received good frame counter E-frames */
+#define MRGFCE_RGFNE		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRGFCP	0x040C /* RMAC Received good frame counter P-frames */
+#define MRGFCP_RGFNP		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRBFC	0x0410 /* RMAC Received good broadcast frame counter */
+#define MRBFC_RBFN		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRMFC	0x0414 /* RMAC Received good multicast frame counter */
+#define MRMFC_RMFN		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRUFC	0x0418 /* RMAC Received good unicast frame counter */
+#define MRUFC_RUFN		GENMASK(31, 0)
+
+#define RSW2_RMAC_MREFC	0x041C /* RMAC Received PHY error frame count */
+#define MREFC_RPEFN		GENMASK(15, 0)
+
+#define RSW2_RMAC_MRNEFC	0x0420 /* RMAC Received nibble error frame count */
+#define MRNEFC_RNEFN		GENMASK(15, 0)
+
+#define RSW2_RMAC_MRFMEFC	0x0424 /* RMAC Received FCS/mCRC error frame count */
+#define MRFMEFC_RFMEFN		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRFFMEFC	0x0428 /* RMAC final fragment missing error frame count */
+#define MRFFMEFC_RFFMEFN	GENMASK(15, 0)
+
+#define RSW2_RMAC_MRCFCEFC	0x042C /* RMAC Received C-fragment count error frame count */
+#define MRCFCEFC_RCFCEFN	GENMASK(15, 0)
+
+#define RSW2_RMAC_MRFCEFC	0x0430 /* RMAC Received fragment count error frame count */
+#define MRFCEFC_RFCEFN		GENMASK(15, 0)
+
+#define RSW2_RMAC_MRRCFEFC	0x0434  /* RMAC Received RMAC filter error frame count */
+#define MRRCFEFC_RRCFEFN	GENMASK(15, 0)
+
+#define RSW2_RMAC_MRFC		0x0438 /* RMAC Received frame count */
+#define MRFC_RFN			GENMASK(31, 0)
+
+#define RSW2_RMAC_MRGUEFC	0x043C /* RMAC Received good undersize error frame count */
+#define MRGUEFC_RGUEFN		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRBUEFC	0x0440 /* RMAC Received bad undersize error frame count */
+#define MRBUEFC_RBUEFN		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRGOEFC	0x0444 /* RMAC Received good oversize error frame count */
+#define MRGOEFC_RGOEFN		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRBOEFC	0x0448 /* RMAC Received bad oversize error frame count */
+#define MRBOEFC_RBOEFN		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRXBCEU	0x044C  /* RMAC Received byte counter E-frames upper side */
+#define MRXBCEU_TBNEU		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRXBCEL	0x0450 /* RMAC Received byte counter E-frames lower side */
+#define MRXBCEL_TBNEL		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRXBCPU	0x0454 /* RMAC Received byte counter P-frames upper side */
+#define MRXBCPU_TBNPU		GENMASK(31, 0)
+
+#define RSW2_RMAC_MRXBCPL	0x0458 /* RMAC Received byte counter P-frames lower side */
+#define MRXBCPL_TBNPL		GENMASK(31, 0)
+
+#define RSW2_RMAC_MTGFCE	0x0508 /* Transmitted good frame counter E-frames */
+#define RSW2_RMAC_MTGFCP	0x050C /* Transmitted good frame counter P-frames */
+#define RSW2_RMAC_MTBFC		0x0510 /* Transmitted broadcast frame counter */
+#define RSW2_RMAC_MTMFC		0x0514 /* Transmitted multicast frame counter */
+#define RSW2_RMAC_MTUFC		0x0518 /* Transmitted unicast frame counter */
+#define RSW2_RMAC_MTEFC		0x051C /* Transmitted error frame counter */
+#define RSW2_RMAC_MTXBCEU	0x0520 /* Transmitted byte counter E-frames Upper */
+#define RSW2_RMAC_MTXBCEL	0x0524 /* Transmitted byte counter E-frames Lower */
+#define RSW2_RMAC_MTXBCPU	0x0528 /* Transmitted byte counter P-frames Upper */
+#define RSW2_RMAC_MTXBCPL	0x052C /* Transmitted byte counter P-frames Lower */
+
+#endif /* _RSWITCH2_GWCA_H */
+
diff --git a/drivers/net/ethernet/renesas/rswitch2/rswitch2_serdes.h b/drivers/net/ethernet/renesas/rswitch2/rswitch2_serdes.h
new file mode 100644
index 000000000000..50da50a44b4d
--- /dev/null
+++ b/drivers/net/ethernet/renesas/rswitch2/rswitch2_serdes.h
@@ -0,0 +1,64 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Renesas RSwitch2 serializer/de-serializer definitions
+ *
+ * Copyright (C) 2021, 2022 Renesas Electronics Corporation
+ *
+ */
+
+#ifndef _RSWITCH2_SERDES_H
+#define _RSWITCH2_SERDES_H
+
+#define RSW2_SERDES_CHANNEL_OFFSET              0x0400
+#define RSWITCH_SERDES_BANK_SELECT              0x03fc
+#define RSWITCH_SERDES_FUSE_OVERRIDE(n)         (0x2600 - (n) * 0x400)
+
+#define BANK_180                                0x0180
+#define VR_XS_PMA_MP_12G_16G_25G_SRAM           0x026c
+#define VR_XS_PMA_MP_12G_16G_25G_REF_CLK_CTRL   0x0244
+#define VR_XS_PMA_MP_10G_MPLLA_CTRL2            0x01cc
+#define VR_XS_PMA_MP_12G_16G_25G_MPLL_CMN_CTRL  0x01c0
+#define VR_XS_PMA_MP_12G_16G_MPLLA_CTRL0        0x01c4
+#define VR_XS_PMA_MP_12G_MPLLA_CTRL1            0x01c8
+#define VR_XS_PMA_MP_12G_MPLLA_CTRL3            0x01dc
+#define VR_XS_PMA_MP_12G_16G_25G_VCO_CAL_LD0    0x0248
+#define VR_XS_PMA_MP_12G_VCO_CAL_REF0           0x0258
+#define VR_XS_PMA_MP_12G_16G_25G_RX_GENCTRL1    0x0144
+#define VR_XS_PMA_CONSUMER_10G_RX_GENCTRL4      0x01a0
+#define VR_XS_PMA_MP_12G_16G_25G_TX_RATE_CTRL   0x00d0
+#define VR_XS_PMA_MP_12G_16G_25G_RX_RATE_CTRL   0x0150
+#define VR_XS_PMA_MP_12G_16G_TX_GENCTRL2        0x00c8
+#define VR_XS_PMA_MP_12G_16G_RX_GENCTRL2        0x0148
+#define VR_XS_PMA_MP_12G_AFE_DFE_EN_CTRL        0x0174
+#define VR_XS_PMA_MP_12G_RX_EQ_CTRL0            0x0160
+#define VR_XS_PMA_MP_10G_RX_IQ_CTRL0            0x01ac
+#define VR_XS_PMA_MP_12G_16G_25G_TX_GENCTRL1    0x00c4
+#define VR_XS_PMA_MP_12G_16G_TX_GENCTRL2        0x00c8
+#define VR_XS_PMA_MP_12G_16G_RX_GENCTRL2        0x0148
+#define VR_XS_PMA_MP_12G_16G_25G_TX_GENCTRL1    0x00c4
+#define VR_XS_PMA_MP_12G_16G_25G_TX_EQ_CTRL0    0x00d8
+#define VR_XS_PMA_MP_12G_16G_25G_TX_EQ_CTRL1    0x00dc
+#define VR_XS_PMA_MP_12G_16G_MPLLB_CTRL0        0x01d0
+#define VR_XS_PMA_MP_12G_MPLLB_CTRL1            0x01d4
+#define VR_XS_PMA_MP_12G_16G_MPLLB_CTRL2        0x01d8
+#define VR_XS_PMA_MP_12G_MPLLB_CTRL3            0x01e0
+
+#define BANK_300                                0x0300
+#define SR_XS_PCS_CTRL1                         0x0000
+#define SR_XS_PCS_STS1                          0x0004
+#define SR_XS_PCS_CTRL2                         0x001c
+
+#define BANK_380                                0x0380
+#define VR_XS_PCS_DIG_CTRL1                     0x0000
+#define VR_XS_PCS_DEBUG_CTRL                    0x0014
+#define VR_XS_PCS_KR_CTRL                       0x001c
+#define VR_XS_PCS_SFTY_MR_CTRL                  0x03d4
+
+#define BANK_1F00                               0x1f00
+#define SR_MII_CTRL                             0x0000
+
+#define BANK_1F80                               0x1f80
+#define VR_MII_AN_CTRL                          0x0004
+
+
+
+#endif /* _RSWITCH2_SERDES_H */
-- 
2.39.2

